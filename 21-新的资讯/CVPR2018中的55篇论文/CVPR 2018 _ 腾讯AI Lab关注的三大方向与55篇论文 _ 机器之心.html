<!DOCTYPE html>
<!-- saved from url=(0048)https://www.jiqizhixin.com/articles/2018-07-28-6 -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no" name="viewport"><meta content="ie=edge" http-equiv="X-UA-Compatible"><meta content="telephone=no" name="format-detection"><title>CVPR 2018 | 腾讯AI Lab关注的三大方向与55篇论文 | 机器之心</title>
<meta name="description" content="CVPR 2018上涌现出非常多的优秀论文，腾讯 AI Lab 对其中精华文章归类与摘要，根据受关注程度，对生成对抗网络、视频分析与理解和三维视觉三大类论文进行综述。">
<meta name="keywords" content="cvpr 2018">
<meta property="og:url" content="https://www.jiqizhixin.com/articles/2018-07-28-6">
<meta property="og:title" content="CVPR 2018 | 腾讯AI Lab关注的三大方向与55篇论文">
<meta property="og:type" content="website">
<meta property="og:site_name" content="机器之心">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@SyncedTech">
<meta name="twitter:title" content="CVPR 2018 | 腾讯AI Lab关注的三大方向与55篇论文">
<meta name="twitter:description" content="CVPR 2018上涌现出非常多的优秀论文，腾讯 AI Lab 对其中精华文章归类与摘要，根据受关注程度，对生成对抗网络、视频分析与理解和三维视觉三大类论文进行综述。">
<meta name="twitter:image" content="https://image.jiqizhixin.com/uploads/article/cover_image/28117fd2-0c7b-4a51-aa7e-a9223b74e1c3/888.jpg"><meta name="csrf-param" content="authenticity_token">
<meta name="csrf-token" content="IkdPj3s5K/QO7s0GjylUlLjTTP9y3YgQXEbxk6AzRzvUaCHoGXAn2owWbVcuieoWjJYf2OnRlm5+7ahbe7S+Bw=="><link rel="stylesheet" media="all" href="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/application-5b1eb5d9894f65143e5dc2f37fca9892.css"><style type="text/css">.medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--open .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s}.medium-zoom-image--open{position:relative;z-index:1;cursor:pointer;cursor:zoom-out;will-change:transform}</style></head><body data-controller="articles" id="articles-show"><svg aria-hidden="true" style="position: absolute; width: 0px; height: 0px; overflow: hidden;"><symbol id="coloricon-pdf-" viewBox="0 0 1024 1024"><path d="M897.5 267.2v622.7c0 38.4-31.1 69.5-69.5 69.5H196c-38.4 0-69.5-31.1-69.5-69.5V133.7c0-38.4 31.1-69.5 69.5-69.5h501.5v147.4c0 30.7 24.9 55.6 55.6 55.6h144.4z" fill="#FF3F24"></path><path d="M290.1 357.6h437.6v437.6H290.1z" fill="#FFFFFF"></path><path d="M697.9 66.9v154.7c0 15.2 5.9 29.8 16.6 40.6l183 187.2V268.8L697.9 66.9z" fill="#D12003"></path><path d="M697.5 64.5v164c0 21.4 17.3 38.7 38.7 38.7h161.6L697.5 64.5z" fill="#FFC9C0"></path><path d="M622.9 631.1c-11.1-8-30.2-12.3-56.7-12.8-15.4-19.1-30.1-41.7-40.3-61.7 7.4-13.3 11.9-23.1 13.5-29.2 7.2-28.2 1.7-48.7-5.9-58.4-4.4-5.6-10-8.7-15.9-8.7-7.2 0-24 4.6-25.6 47.2-0.5 12.5 3.9 28.8 13 48.4-14.2 24.3-33.5 53-50.6 74.9-14.9 3.2-29.1 6.8-41.3 10.5-32.8 10.1-35.7 24.8-34.5 32.7 1.6 10.8 12.4 18.3 26.3 18.3 6.2 0 12.6-1.6 18.6-4.5 10.2-5.1 24.1-18.6 41.3-40.2 32.9-6.6 66.3-10.5 92.7-10.8 7.4 8.7 17.1 19.2 26 25.9 14.1 10.6 26.3 15.9 36.2 15.9 9.3 0 16.4-4.8 19-12.8 3.5-11.2-3-25.5-15.8-34.7zM415 671.3c-3.3 1.7-6.9 2.6-10.2 2.6-4.8 0-7.6-1.9-8-2.4 0.2-1.3 4.3-7.1 21.6-12.4 5-1.5 10.3-3 15.9-4.5-10.2 11-16.2 15.1-19.3 16.7z m95.4-163c0.8-21.8 6.2-28.8 6.8-29.4 3.4 1.3 11.1 17 4.2 44-0.3 1.2-1.4 4.7-5.1 12.3-5.1-13.3-6.1-21.8-5.9-26.9z m32.1 110.4c-18.5 1-39.3 3.3-60.4 6.8l0.5-0.6-0.8 0.2c11.4-15.7 22.9-33.1 32.9-49.3l0.2 0.3 0.3-0.5c7.9 14.1 17.5 28.8 27.7 42.5h-0.9l0.5 0.6z m78.7 41.2c-0.1 0.1-0.6 0.2-1.6 0.2-2.6 0-10.3-1.2-25.1-12.2-3.2-2.4-7-5.8-11.1-10 17.1 1.9 25.1 5.7 28.7 8.3 8 5.7 9.4 12.6 9.1 13.7z" fill="#FF3F24"></path></symbol><symbol id="coloricon-translation-line1" viewBox="0 0 1024 1024"><path d="M735.744 560.64H665.6l-99.84 275.968h56.832l20.992-58.368H752.64l20.992 58.368h59.392l-97.28-275.968zM658.432 732.16l40.448-117.76h1.536l37.376 117.76h-79.36z" fill="#EB2835"></path><path d="M325.12 126.976H269.312v73.216H146.944v174.592h50.688v-27.136h73.216V476.16h53.76V347.648H399.36v22.528h55.296V200.192H325.12V126.976zM270.336 307.2H197.632V241.664h73.216L270.336 307.2z m128.512 0H324.096V241.664h74.752V307.2z" fill="#9D9D9D"></path><path d="M374.784 643.072H95.744C43.008 643.072 0.512 600.576 0.512 547.84V96.768C0.512 44.032 43.008 1.536 95.744 1.536h451.072c52.736 0 95.232 42.496 95.232 95.232V378.88h-51.2V96.768c0-24.064-19.968-44.032-44.032-44.032H95.744c-24.064 0-44.032 19.968-44.032 44.032V547.84c0 24.064 19.968 44.032 44.032 44.032h279.04v51.2zM972.8 267.264h-51.2c0-90.624-73.728-164.352-164.352-164.352v-51.2c118.784 0 215.552 96.768 215.552 215.552zM266.752 972.288C147.968 972.288 51.2 875.52 51.2 756.736h51.2c0 90.624 73.728 164.352 164.352 164.352v51.2z" fill="#9D9D9D"></path><path d="M925.184 1019.392H474.112c-52.736 0-95.232-42.496-95.232-95.232V473.088c0-52.736 42.496-95.232 95.232-95.232h451.072c52.736 0 95.232 42.496 95.232 95.232v451.072c0 52.736-42.496 95.232-95.232 95.232zM474.112 429.056c-24.064 0-44.032 19.968-44.032 44.032v451.072c0 24.064 19.968 44.032 44.032 44.032h451.072c24.064 0 44.032-19.968 44.032-44.032V473.088c0-24.064-19.968-44.032-44.032-44.032H474.112z" fill="#EB2835"></path></symbol><symbol id="coloricon-translation-line" viewBox="0 0 1024 1024"><path d="M933.376 1004.544c38.4 0 69.632-31.232 69.632-69.632V483.84c0-38.4-31.232-69.632-69.632-69.632H482.304c-38.4 0-69.632 31.232-69.632 69.632v451.072c0 38.4 31.232 69.632 69.632 69.632h451.072z" fill="#EB2835"></path><path d="M90.624 609.792h274.944V461.824c0-56.832 46.08-102.912 102.912-102.912h142.848V89.088c0-38.4-31.232-69.632-69.632-69.632H90.624c-38.4 0-69.632 31.232-69.632 69.632v451.072c0 38.4 31.232 69.632 69.632 69.632z" fill="#9D9D9D"></path><path d="M751.104 581.12H680.96l-99.84 275.968h56.832l20.992-58.368H768l20.992 58.368h59.392l-97.28-275.968zM673.792 752.64l40.448-117.76h1.536l37.376 117.76h-79.36zM314.88 114.176H259.072v73.216H136.704v174.592h50.688v-27.136h73.216v128.512h53.76V334.848H389.12v22.528h55.296V187.392H314.88V114.176zM260.096 294.4H187.392V228.864h73.216l-0.512 65.536z m128.512 0H313.856V228.864h74.752v65.536z" fill="#FFFFFF"></path><path d="M972.8 267.264h-51.2c0-90.624-73.728-164.352-164.352-164.352v-51.2c118.784 0 215.552 96.768 215.552 215.552zM266.752 972.288C147.968 972.288 51.2 875.52 51.2 756.736h51.2c0 90.624 73.728 164.352 164.352 164.352v51.2z" fill="#9D9D9D"></path></symbol></svg><div class="article"><div class="header__hr is-progress-bar" id="js-progress-bar" style="width: 0%;"></div><div class="header__hr"></div><header class="header has-not-shadow" id="header"><div class="u-container u-flex"><div class="u-flex header__right"><div class="header-logo"><a alt="首页" href="https://www.jiqizhixin.com/"><img alt="机器之心" class="header-logo__black" height="32" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/logo-black-814ff978059dd2570cc09283d01d1e29ebec941b013fc43d3ae6ce3b3f6c2d69.png"></a></div><div class="header-nav__current js-nav-current"><span>知识</span><i class="iconfont icon-arrowdown u-margin-left header-nav__icon is-first"></i><i class="iconfont icon-iconguanbi u-margin-left header-nav__icon is-second"></i></div></div><div id="js-site-search"><div class="u-in-center header-search"><a class="header-search__btn" href="javascript:;" alt="搜索"><i class="iconfont icon-sreach"></i></a><input type="text" class="header-search__input" placeholder="探索机器之心" value=""></div></div><div class="header__btns js-header-btns"><div class="header__btns"><a class="header-other__link header-user__menu" href="javascript:;">登录</a></div></div></div></header><nav class="header-nav__list js-nav-list"><div class="u-container header-nav__items"><a class="u-borbot__item header-nav__item " href="https://www.jiqizhixin.com/">探索</a><a class="u-borbot__item header-nav__item is-active" href="https://www.jiqizhixin.com/categories/basic">知识</a><a class="u-borbot__item header-nav__item " href="https://www.jiqizhixin.com/categories/industry">产业</a><a alt="AI商用搜索" class="u-borbot__item header-nav__item" href="https://handbook.jiqizhixin.com/" rel="noopener noreferrer" target="_blank">AI商用搜索</a></div></nav><div class="u-min-height-container u-container"><div class="u-col-8"><div class="u-flex article__header"><div class="u-flex article-author"><a alt="腾讯AI实验室" class="u-avatar-base article-author__avatar" href="https://www.jiqizhixin.com/users/74c81aae-9594-45c0-93db-987e12802125" rel="noopener noreferrer" target="_blank"><img alt="腾讯AI实验室" class="u-image-center" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/anonymous-1-83f4ab515bbe824688aef66da82d70a493aea5bcfe5c83df85db80a77c1591e9.jpg"></a><div><p class="article-author__attr"><a alt="腾讯AI实验室" class="article-author__name" href="https://www.jiqizhixin.com/users/74c81aae-9594-45c0-93db-987e12802125" rel="noopener noreferrer" target="_blank">腾讯AI实验室</a><span class="article__type">转载</span></p><time class="article__published">2018/07/24  7:17</time></div></div><div class="article-des u-flex"><div></div></div></div><h1 class="article__title">CVPR 2018 | 腾讯AI Lab关注的三大方向与55篇论文</h1></div><div class="u-col-8 article__inline" id="js-article-inline"><p class="article__summary"></p><div class="article__content" id="js-article-content"><section><section><section><section><section><section><section><p>感谢阅读腾讯 AI Lab 微信号第 32 篇文章，CVPR 2018上涌现出非常多的优秀论文，腾讯 AI Lab 对其中精华文章归类与摘要，根据受关注程度，对<mark data-id="d98bc6dc-7ee8-4cc9-9cc0-b8cd2ad5e068" data-type="technologies" class="tooltipstered">生成对抗网络</mark>、视频分析与理解和三维视觉三大类论文进行综述。</p></section></section></section></section></section><strong>第一部分：<mark data-id="d98bc6dc-7ee8-4cc9-9cc0-b8cd2ad5e068" data-type="technologies" class="tooltipstered">生成对抗网络</mark></strong></section></section><section><section><section><section><section><br></section></section></section></section></section><section><section><section><p><strong>1.1 Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic <mark data-id="d98bc6dc-7ee8-4cc9-9cc0-b8cd2ad5e068" data-type="technologies" class="tooltipstered">Generative Adversarial Networks</mark></strong></p><p>这个工作由腾讯 AI Lab 和罗切斯特大学共同完成，主要研究的问题是给定第一帧，如何生成接下来的视频帧，使得生成的画面兼具有真实的内容和生动的运动。为了完成这个任务，作者提出了一个多阶段的动态生成网络。具体地，在第一阶段，生成网络专注于生成真实的内容，判别网络进行“真”和“假”的二分类任务；在第二阶段的生成网络专注于对第一阶段的输出视频进行优化，使之具有生动的运动信息。为了达到这个目的，作者引入了 gram &nbsp;矩阵并且提出了一个排序损失（ranking loss），使得生成的视频的运动信息与真实视频更加接近。该工作在一个提出的 Sky Scene 数据集上面取得了当前的最优性能。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPoyVoFlZOg2RBYicvNRZVgibpHWwAvhrHbnBmiafQsaxBV9scvNKibN5TiaQ/640?wx_fmt=png" data-w="550" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775752426.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.2 PairedCycleGAN: Asymmetric Style Transfer for Applying and Removing Makeup</strong></p><p>这个工作由普林斯顿大学、Adobe Research 和加利福尼亚大学伯克利分校共同完成，主要研究的是给定一幅没有化妆的人物图像 A 和一幅带妆容的参考图像 B，如何给人物图像 A 自动上妆。作者通过一种不对称的风格转换来完成这一目的。具体地，作者同时学习一个上妆函数对 A 进行上妆得到 A' 和一个移除妆容的函数对 B 进行移除妆容的操作得到 B'，然后再次对 B' 进行化妆操作得到 B" 和对 A' 进行移除妆容操作得到 A"，通过约束 A" 与 B" 分别与 A 和 B 接近来学习整个风格转换。作者在不同的人物图像和妆容风格上面取得了不错的结果。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPytlIQHZkL4Cv4icCicUTVujJRWzUf4HWM9XAia1EEjwGZPMGtyGIZg4Rg/640?wx_fmt=png" data-w="555" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775751980.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.3 SeGAN: Segmenting and Generating the Invisible</strong></p><p>这个工作由华盛顿大学和艾伦人工智能研究所（AI2）共同完成，主要研究的是如何对被遮挡的物体完成补全。这个挑战性的问题同时涉及到分割和生成两个任务。研究者提出了 SeGAN 来解决这个问题。具体地，输入一张图像和它的可见部分的分割掩码（mask），该工作通过学习分割网络来得到不可见部分的分割结果，再通过 GAN 来对这些不可见部分进行补全，更为重要的是这两个任务在同一个框架里面进行联合优化，从而得到最终的输出。该工作取得了当前的最佳结果。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPZShahww2nkX8iau580eogB2RYvicDtVISTcgkiaMicibsGHRCtKxNk55trw/640?wx_fmt=png" data-w="562" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775752101.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.4 Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network</strong></p><p>该工作由佛罗里达大学完成，主要研究的是给定文本描述生成高分辨率图像的任务，作者设计了一个单流式（single-stream）生成器，在生成器的不同阶段对生成结果通过判别器进行判别，这种一个生成器加多个判别器的结构被称为级联式对抗网络。实验证明，该方法在 3 个公开数据集上取得当前最佳结果。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPuVVf7hia9OwuicVxZicb4UibqWl0BxnsC20FRDwNtsqnOVENrgaF33e9Lg/640?wx_fmt=png" data-w="564" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775752630.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><br></section></section><section><section><section><p><strong>1.5 TextureGAN: Controlling Deep Image Synthesis with Texture Patches</strong></p><p>该工作由佐治亚理工学院、Adobe Research、加利福尼亚大学伯克利分校和 Argo AI 共同完成。主要研究的是给定一个简笔画和一个带纹理的图块，对简笔画进行纹理生成。作者提出了一个局部纹理损失和内容损失进行生成网络的训练，实验结果证明该方法生成了合理的图像。上图为有基本真值的预训练阶段，下图为微调阶段。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPnQy5HuJwXRDcTNEOjkvuibJvf6soCl3kb6oWiarGeBHLDZVIPKLWcaLw/640?wx_fmt=png" data-w="488" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775753027.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><br></section></section><section><section><section><p><strong>1.6 Generate To Adapt: Aligning Domains using <mark data-id="d98bc6dc-7ee8-4cc9-9cc0-b8cd2ad5e068" data-type="technologies" class="tooltipstered">Generative Adversarial Networks</mark></strong></p><p>该工作由马里兰大学帕克分校完成。主要研究的是如何对<mark data-id="a5deb948-06e3-4875-acdb-35c268734006" data-type="technologies" class="tooltipstered">迁移学习</mark>里面的源域和目标域进行对齐的问题。具体地，作者通过一个两路的 F-C-G-D 网络完成该任务。其中 F 网络进行特征提取，C 网络对源域的样本进行分类，G 网络作为生成网络生成跟源域类似的样本，D 网络同时进行真/假的二分类判别任务和多分类任务。通过实验环节的 3 个任务，作者证明了该方法的有效性。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPPDC5ibfWCTsQUEkzQLCTBLMFYlAoiaNfOZP1GR8YkiaukuKTkIsSmxibgg/640?wx_fmt=png" data-w="500" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775752696.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><br></section></section><section><section><section><p><strong>1.7 Disentangled Person Image Generation</strong></p><p>本文由丰田汽车欧洲公司、苏黎世联邦理工学院、MPII合作完成，是 CVPR 2018 的 spotlight 文章。 在基于 GAN 的人脸图像生成技术趋于成熟后，人们将注意力转向了基于 GAN 的人像生成。本文将人像信息拆解为三个部分：前景、背景和姿态。本文提出了一种两阶段的全身人像重建方法，将前景、背景和姿态三部分的嵌入特征（embedding feature）解耦。在第一阶段，一个多分支的重建网络被用于编码生成上述三部分的嵌入特征，然后将这三部分的嵌入特征被组合到一起用于重建输入图像，相当于一个中间具有多分支的<mark data-id="07645339-ced5-4871-bb96-98062cf4a547" data-type="technologies" class="tooltipstered">自动编码器</mark>（auto-encoder）。第一阶段的自编解码器用真实图像进行训练，可以提取真实图像的嵌入特征。在第二阶段，用对抗的方式训练一个转码器和判别器。转码器学习如何将Gaussian噪声<mark data-id="8ec6a68f-ad96-4b85-ab72-6f8931886922" data-type="technologies" class="tooltipstered">映射</mark>成与真实图像相近的伪嵌入特征。判别器学习如何区分真实嵌入特征和伪嵌入特征。针对前景、背景、姿态三部分，需要分别训练三组转码器和判别器。利用该两阶段方法，可以独立地操纵其中任意一部分的特征，保持其它部分的特征不变，完成更换背景、更换前景、更换姿态等应用。本文在 Market-1501 和 Deepfashion 数据集上进行了训练和测试，能够生成较为真实的人像，且能够独立操纵前景、背景和姿态等因子。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPwrDGw6ibeUJmZQpuPmIHnbnhoHMXpAmicJbCtCrdJ6SXOvvR78Pt1wNQ/640?wx_fmt=png" data-w="344" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775753316.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.8 Super-FAN: Integrated facial landmark localization and super-resolution of real-world low-resolution faces in arbitrary poses with GANs</strong></p><p>本文由诺丁汉大学完成，是 CVPR 2018的 spotlight 文章。本文提出了一种称为 Super-FAN 的方法，能够同时完成人脸关键点定位和超分辨率两个任务。本文通过对抗的方式训练了一个生成器和两个判别器。生成器用于完成人脸超分辨率的任务。一个判别器用于判断生成器的超分辨率结果是否为真实的人脸，另一个判别器用于在生成器的超分辨率结果上完成关键点定位。使用本文提出的方法，能够同时在人脸超分辨率和人脸关键点定位两个任务上都取得提升。实验证明这种联合训练方式优于先训练一个超分辨率网络完成人脸超分辨率再在超分辨率结果上做独立的人脸关键点定位的工作。本文的方法对其它受低分辨率输入困扰的检测任务具有借鉴意义。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPLj3LKpJrcnmSPkbBI7BeubxV3W1kXbNjl8FgzciaHBvpaCfbqf91BJQ/640?wx_fmt=png" data-w="401" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775753466.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.9 Attentive Generative Adversarial Network for Raindrop Removal from A Single Image</strong></p><p>本文由北京大学、新加坡国立大学和耶鲁-新加坡国立大学学院完成，是 CVPR 2018 的 spotlight 文章。 积聚在镜头上的雨滴会严重损害照片的视觉效果。本文提出了一种照片去雨滴的方法，能够有效地将照片中的雨滴去除。为了完成这一任务，本文提出了一种注意力<mark data-id="d92a1cda-8e30-4199-9678-2d8c6c17d736" data-type="technologies" class="tooltipstered">生成模型</mark>——在对抗式训练的过程中，将注意力模块加入到生成器和判别器中。注意力模块学习了如何定位雨滴出现的位置，能够更好地指导生成器和判别器关注雨滴出现的局部区域。本方法的结果较之前方法的结果有较为明显的提升。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDP5XFyQlhxaliaEnh8ej7oWsJPNz5sYPnsOibwaye4pHKJra5Slok4q7mQ/640?wx_fmt=png" data-w="371" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775753608.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.10 Multistage Adversarial Losses for Pose-Based Human Image Synthesis</strong></p><p>本文由中国科学院大学完成，是 CVPR2018 的 spotlight文章。本文关注的是给定一幅人像，如何生成指定的另一个视角的人像。生成新视角的相关工作已经有很多，但主要关注的都是刚性物体，如椅子、楼房、汽车等。相对于刚性物体而言，人体是非刚性的，姿态更为多样，因此生成新视角的人像是一个极具挑战性的问题。本文以多视角姿态估计为辅助，使用了多阶段对抗<mark data-id="4c38563a-2d9b-439e-bfb4-21d209eeff3e" data-type="technologies" class="tooltipstered">损失函数</mark>训练了一个多阶段的转换网络。本文的方法分为三个阶段。第一阶段，先使用已有的方法估计输入图像的 2D 姿态估计结果，然后训练一个网络来生成指定视角的 2D 姿态估计结果。第二阶段，使用已有的方法将人从图像中分割出来，然后以原始 2D 姿态和目标 2D 姿态作为辅助输入，训练一个前景转换网络完成目标新视角前景的生成。第三阶段，输入第二阶段的新视角前景和原始图像，训练一个背景转换网络完成目标新视角背景的生成。本文在 Human3.6M 数据集上进行了测试，能够在保持姿态不变的情况下，生成较为逼真的新视角人像。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDP6SaEm04ZKT0goEibkYdv5QbM1262GEpibFKC1WIj5r9EPPZTuYr9FOzQ/640?wx_fmt=png" data-w="377" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775755469.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.11 Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs<br></strong></p><p>本文由台湾大学完成，是 CVPR 2018 的 spotlight 文章。图像美化一直是<mark data-id="6e614199-9e49-450e-9078-61fb2b122da9" data-type="technologies" class="tooltipstered">计算机视觉</mark>领域的重要主题，在诸多图片编辑软件和拍摄软件中具有广泛应用。本文提出了一种双路的对抗生成网络，与 CycleGAN 类似。为了改善效果，本论文提出了多方面的优化。其一，生成器在传统 UNet 结构的基础之上，加入了全局特征使得网络能够同时捕捉全局和局部信息。其二，使用了一个自适应的<mark data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" data-type="technologies" class="tooltipstered">权重</mark>赋值方案来提升 <mark data-id="ec12cb22-75f7-48da-844f-151a46801564" data-type="technologies" class="tooltipstered">WGAN</mark> 的效果，能够更快更好地<mark data-id="3bf78775-1316-4ac0-bd99-10e2fc88c439" data-type="technologies" class="tooltipstered">收敛</mark>，而且相对 <mark data-id="ec12cb22-75f7-48da-844f-151a46801564" data-type="technologies" class="tooltipstered">WGAN</mark>-GP 而言对<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>更加不敏感。其三，在生成器中使用了个体批规范化（individual batch normalization），使得生成器更适应于输入的分布。这些细节的改动使得本文的方法在图像美化这一应用上取得了不错的效果。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPyVjpseUXxvJuQyqvclRHYDA5Piae6zBMBJAiaOgU2hjX0Nwy0ia2gib67g/640?wx_fmt=png" data-w="516" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775753754.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDP8BawH9micCLkMgOddT3VK9vrfkJQpaMYSjM2B76j3HaJ5TV0Xib0ZiaHA/640?wx_fmt=png" data-w="539" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775753891.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.12 StarGAN: Unified <mark data-id="d98bc6dc-7ee8-4cc9-9cc0-b8cd2ad5e068" data-type="technologies" class="tooltipstered">Generative Adversarial Networks</mark> for Multi-Domain Image-to-Image Translation</strong></p><p>本文由高丽大学、NAVER、新泽西学院、香港科技大学合作完成 ，是 CVPR 2018 的 oral 文章。近期图像间翻译问题已经在两个域的情况下取得了很大的进展。然而，现有方法的可扩展性和鲁棒性都不足以完成两个域以上的图像间翻译问题。针对多个域的问题，最直接的做法是每两个域之间都单独训练一个转换模型，然而这种做法是十分冗余的，而且在数据有限的情况下很难取得出色的结果。为了解决这个问题，本文提出了一种 StarGAN 结构，使用多个域之间的图像翻译训练数据完成对抗训练，能够使用一个模型完成多个域之间的图像间翻译问题。具体地，在训练生成器的过程中，除了给定输入图像还会给定目标域的标签；同时还训练判别器在判别真假的同时判别生成结果所处的域，相当于是一种条件对抗生成网络。训练过程中会混合使用多个域之间的图像对，使得同一个生成器能够完成多个域之间的转换问题。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPLoCrTCuWC2JrMOxPdJDwbXODlaFPQic5hzRsicO2BARgWuXDh8OFIoqQ/640?wx_fmt=png" data-w="560" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775754011.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.13 High-Resolution Image Synthesis and Semantic Manipulation with <mark data-id="4a6522e4-8695-4f4c-835e-a20dca37d758" data-type="technologies" class="tooltipstered">Conditional GAN</mark>s</strong></p><p>本文由英伟达和加利福尼亚大学伯克利分校合作完成，是 CVPR 2018 的 oral 文章。本文提出了一种基于条件<mark data-id="d98bc6dc-7ee8-4cc9-9cc0-b8cd2ad5e068" data-type="technologies" class="tooltipstered">生成对抗网络</mark>（<mark data-id="4a6522e4-8695-4f4c-835e-a20dca37d758" data-type="technologies" class="tooltipstered">Conditional GAN</mark>）的图像生成方法，能够将语义分割标签图转换成高分辨率的真实图像。已有的条件<mark data-id="d98bc6dc-7ee8-4cc9-9cc0-b8cd2ad5e068" data-type="technologies" class="tooltipstered">生成对抗网络</mark>虽然能完成类似的任务，但是只局限在低分辨的图像上。本文生成的结果分辨率高达 2048x1024，而且拥有极高的真实感。为了达到这样的效果，本文提出了一种新的对抗<mark data-id="4c38563a-2d9b-439e-bfb4-21d209eeff3e" data-type="technologies" class="tooltipstered">损失函数</mark>以及多尺度的生成器和判别器结构。具体地，本文将生成器拆分成 G1 和 G2 两个子网络。其中 G1 生成低分辨率的转换结果；G2 在 G1 的基础之上进一步优化结果的精细度，生成更高分辨率的结果。另一方面，本文训练了三个判别器来判断不同尺度的结果是否为真实图像，从而提供三种不同尺度的对抗损失来指导生成器的训练。此外，本文还展示了如何利用条件<mark data-id="d98bc6dc-7ee8-4cc9-9cc0-b8cd2ad5e068" data-type="technologies" class="tooltipstered">生成对抗网络</mark>完成交互式的图像编辑应用，如删除和增加物体、更换物体的外表等。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPwxWsRuwyUUoCWx3By7BjF8n7xKhSCNp4OicpDd4XWDmu2rDoLiaOkxUg/640?wx_fmt=png" data-w="424" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775754126.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>1.14 Synthesizing Images of Humans in Unseen Poses</strong></p><p>本文由麻省理工学院完成，是 CVPR 2018 的 oral 文章。本文研究的是如何生成具有目标姿态的人像照片的问题。本文以对抗的方式训练了一个生成器，该生成器对人体的不同部位分别完成转换，再组合成新的前景。具体地，首先为每个部位生成单独的掩码（mask），然后利用掩码将每个部位单独提取出来，通过空域转换模型（Spatial Transformer）完成变形，再组合成具有新姿态的完整全身人像作为前景。新背景则由原来的背景抠除人之后利用<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>进行填补得来。本文主要在三种动作上进行了测试，包括打高尔夫球、健身和网球。使用本文的方法还能够在给定一幅输入图像和一组目标姿态序列的情况下，生成一个符合目标姿态序列的视频。但目前方法仍然局限于瑜伽、网球、高尔夫三种动作。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPs2LuuzLK86kalb2CHzd4n2Z5E7TNd85C21k4ETJQU6qpPiaBKUQY6Rg/640?wx_fmt=png" data-w="448" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775754213.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><section><section><section><section><p><strong>第二部分：视频分析与理解</strong></p></section></section></section></section></section></section></section><section><section><section><p><strong>2.1 Bidirectional Attentive Fusion With Context Gating for Dense Video Captioning</strong></p><p>本文由腾讯 AI Lab 与华南理工大学合作完成，是今年 CVPR 2018 的 spotlight 文章。密集视频描述是一个时下刚兴起的课题，旨在同时定位并用自然语言描述一个长视频中发生的所有事件或行为。在这个任务中，本文明确并解决了两个挑战，即：（1）如何利用好过去和未来的信息以便更精确地定位出事件，（2）如何给解码器输入有效的视觉信息，以便更准确地生成针对该事件的自然语言描述。第一，过去的工作集中在从正向（视频从开头往结尾的方向）生成事件候选区间，而忽视了同样关键的未来信息。作者引入了一种双向提取事件候选区间的方法，同时利用了过去和未来的信息，从而更有效地进行事件定位。第二，过去的方法无法区分结束时间相近的事件，即给出的描述是相同的。为了解决这个问题，作者通过<mark data-id="60bee267-89dc-4606-ba24-6b3f7d1f6189" data-type="technologies" class="tooltipstered">注意力机制</mark>将事件定位模块中的隐状态与视频原始内容（例如，视频 C3D 特征）结合起来表征当前的事件。进一步地，作者提出一种新颖的上下文门控机制来平衡当前事件内容和它的上下文对生成文字描述的贡献。作者通过大量的实验证明，相比于单独地使用隐状态或视频内容的表征方式，新提出的注意力融合的事件表征方式表现更好。通过将事件定位模块和事件描述模块统一到一个框架中，本文的方法在 ActivityNet Captions 数据库上超过了之前最好的方法，相对性能提升 100%（Meteor 分数从 4.82 到 9.65）。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPwPMQjAlUcGmtwE9mdibwbH1sq4Bm0lJ8pk9XyUDNxLDctXVzX8YOiaeQ/640?wx_fmt=png" data-w="913" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775754920.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.2 End-to-End Learning of Motion Representation for Video Understanding</strong></p><p>本文由腾讯 AI Lab、清华大学、MIT-Watson 实验室、斯坦福大学合作完成，是 CVPR 2018 的 spotlight 文章。尽管端到端的特征学习已经取得了重要的进展，但是人工设计的光流特征仍然被广泛用于各类视频分析任务中。为了弥补这个不足，作者创造性地提出了一个能从数据中学习出类光流特征并且能进行端到端训练的<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>：TVNet。当前，TV-L1 方法通过优化方法来求解光流，是最常用的方法之一。作者发现，把 TV-L1 的每一步迭代通过特定设计翻译成<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>的某一层，就能得到 TVNet 的初始版本。因此，TVNet 能无需训练就能被直接使用。更重要的是，TVNet 能被嫁接到任何分类<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>来构建从数据端到任务端的统一结构，从而避免了传统多阶段方法中需要预计算、预存储光流的需要。最后，TVNet 的某些<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>是可以被通过端到端训练来进一步优化，这有助于 TVNet 学习出更丰富以及与任务更相关的特征而不仅仅是光流。在两个动作识别的标准数据集 HMDB51 和 UCF101 上，该方法取得了比同类方法更好的分类结果。与 TV-L1 相比，TVNet 在节省光流提取时间和存储空间的基础上，明显提高了识别精度。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPAVRpuaGh35fcliaxqD38JCZ7vVZ3ibGXb1ViaVpa6YEfia5YQTkmI02W0g/640?wx_fmt=png" data-w="549" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775755563.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.3 Finding "It": Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos</strong></p><p>本文由斯坦福大学完成，是 CVPR 2018 的 oral 文章。将文本短语对应视觉内容是一项具有挑战性的任务。当我们考虑教学视频中的 grounding 问题，这个问题变得更加的复杂。教学视频的潜在时间结构违背了独立性假设，并且需要上下文理解来解决模糊的视觉语言指代信息。此外，密集的注释和视频数据规模意味着基于监督的方法的成本过高。在这项工作中，作者通过弱监督框架在教学视频中进行参照<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>式 grouding 任务。其中只有文本描述和视频片段之间的对应关系可用于监督信号。文中介绍了视觉上 grounding 的动作图，这是一种捕获视频 grounding 和参照之间潜在依赖关系的结构化表示。文中提出了一种新的参照<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>式多实例学习（RA-MIL）目标，用于对视频中的grounding进行弱监督。在 YouCookII 和 RoboWatch 的视频集合上评估了提出的方法，实验证明提出的方法能提升针对教学视频中的 grounding 问题。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPfrvx3E8HhLYjU1Kia5QG86ske9KQy362hdrwgcP4zyMr1WTxicUY3wNg/640?wx_fmt=png" data-w="629" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775755992.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.4 Actor and Action Video Segmentation From a Sentence</strong></p><p>本文由荷兰阿姆斯特丹大学完成，是 CVPR 2018 的 oral 文章。本文针对人物及其在视频内容中的动作进行像素级分割。 与现有的工作不同，本文的方法从自然语言输入句子中推断出相应的分词。因此，这种方法可以区分同一超类别中的细粒度人物信息，标识人物和动作实例，以及分割词汇表之外的人物与工作的片段对。 本文使用针对视频进行优化的编码器-解码器架构提出了像素级人物和动作分割的全卷积模型。为了展示来自句子的人物和动作视频分割的潜力，本文扩展了两个拥有 7500 多种自然语言描述的人物和动作数据集。 实验进一步展示了句子引导分割的质量，提出的模型的泛化能力，以及与传统人物和动作分割相比的先进的优势。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDP0ibLkSC9p3bm5qHgmHOgWaUXR0388NOiaqTQbhhhwgJhib4BxIm6tdKyQ/640?wx_fmt=png" data-w="424" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775755926.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.5 Compressed Video Action Recognition</strong></p><p>本文由德克萨斯大学奥斯汀分校、卡耐基梅隆大学、南加利福尼亚大学、亚马逊的 A9 和亚马逊合作完成，是 CVPR 2018 的 oral 文章。训练鲁棒的深度视频表示比学习深度图像表示更具挑战性。这部分是由于原始视频流的巨大数据量和高时间冗余; 真正有用的信号经常被淹没在太多不相关的数据中。 由于视频压缩（使用 H.264、HEVC 等），多余的信息可以减少多达两个数量级，因此本文提出直接在压缩视频上训练深层网络。 这种表示具有更高的信息密度，实验证明训练也会更加容易。另外，压缩视频中的信号提供直接的但包含很大噪声的运动信息。 本文提出新的技术来有效地使用它们。 提出的方法比 Res3D 快 4.6 倍，比 ResNet-152 快 2.7 倍。 在动作识别的任务上，本文的方法优于 UCF-101、HMDB-51 和 Charades 数据集上对应的其它方法。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPgGoBN1ibUyHvRiccia4c3N5iaEsxRkQIkfmJURBl9JxZnUrtcBhu42NrDA/640?wx_fmt=png" data-w="486" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775756463.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.6 What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets</strong></p><p>本文由斯坦福大学、Facebook、达特茅斯学院合作完成，是 CVPR 2018 的 spotlight 文章。捕捉时间信息的能力对视频理解模型的发展至关重要。尽管在视频中对运动建模进行了大量尝试，但仍缺少时间信息对视频理解效果的明确分析。本文旨在弥合这一差距，并提出以下问题：视频中的动作对于识别动作有多重要？为此，本文提出了两种新颖的框架：（i）类别不可知的时序产生器和（ii）运动不变帧选择器，以减少/消除运动信息进行分析而不引入其他人为因素。这将视频的运动信息与其他方面的运动隔离开来。与我们的分析中的基线相比，所提出的框架对运动的影响提供了更加精确的估计（UCF101 的 25％ 到 6％，Kinetics 的 15％ 到 5％）。本文的分析提供了有关 C3D 等现有模型的重要见解，以及如何使用更稀疏视频帧实现一致性的结果。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDP9mjoRxb84Jr5EOeAgBYpYDX4nUqVGmTUWVWibRq0G1RzpQlQkZ6c79g/640?wx_fmt=png" data-w="614" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775756524.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.7 Fine-Grained Video Captioning for Sports Narrative</strong></p><p>本文由上海交通大学完成，是 CVPR 2018 的 spotlight 文章。对于视频，如何生成细致的视频描述还远未解决，例如自动体育解说。为此，这项工作做出以下贡献。首先，为了促进对细粒度视频标题的新颖研究，本文收集了一个称为细粒度体育解说数据集（FSN）的新数据集，其中包含来自 YouTube.com 的含有真实性解说的 2K 体育视频。其次，本文开发了一个名为细粒度描述评估（FCE）的新型的评估指标来评估这项新任务。作为被广泛使用的 METEOR 的扩展，它不仅测量语言表现，而且测量动作细节及其时间顺序是否被正确描述。第三，本文提出了一个新的精细体育解说任务的模型框架。该网络具有三个分支：1）时空实体定位和角色发现子网络；2）局部骨骼运动描述的细粒度动作建模子网络；3）群组关系建模子网络以模拟运动员之间的交互。本文进一步融合这些特征，并通过分层递归结构将它们解码为长篇解说。 FSN 数据集上的大量实验证明了所提出的细粒度视频字幕框架的有效性。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPGckLExibuuWNUUtEZdpB8ljQufkMJnGCgDBhEqZMW9lOsMLBK0YXAQw/640?wx_fmt=png" data-w="617" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775756636.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.8 Unsupervised Learning and Segmentation of Complex Activities from Video</strong></p><p>本文由德国波恩大学完成，是 CVPR 2018 的 spotlight 文章。本文提出了一种无监督地将视频复杂活动分成多个步骤或子活动的新方法，没有任何文本输入。 本文提出了一种迭代判别生成方法。该方法交替学习从视频的视觉特征学习子活动的外观，以及通过广义 Mallows 模型学习子活动的时间结构信息。另外，本文引入了背景模型来标注与实际活动无关的视频帧。 本文的方法在具有挑战性的Breakfast Actions和Inria Instructional &nbsp;Videos数据集上得到验证，并且超过了已有的无监督和弱监督模型。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPVZ296xHLoGbL8Uzk9MNDGTBbibW8Cjk3lv5w8NHYpft9fDFv5Opq7ibQ/640?wx_fmt=png" data-w="477" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775756709.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.9 NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning</strong></p><p>本文由德国波恩大学完成，是 CVPR 2018 的 spotlight 文章。视频学习是<mark data-id="6e614199-9e49-450e-9078-61fb2b122da9" data-type="technologies" class="tooltipstered">计算机视觉</mark>领域的一项重要任务，近年来越来越受到关注。 由于即使少量视频也容易包含数百万帧，因此不依赖帧级标注的方法特别重要。 在这项工作中，作者提出了一种基于 Viterbi 损失的新型学习算法，允许在线和<mark data-id="09134d6a-96cc-409b-86ef-18af25abf095" data-type="technologies" class="tooltipstered">增量学习</mark>弱标注视频数据。 此外显式的上下文和长度建模可以帮助视频分割和标签任务方面取得巨大提升。在几个行为分割基准数据集上，与当前最先进的方法相比，作者的方法获得了高达 10％ 的提高。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPFLHTrxTNYaAX3Xw6kFa7tVrg8HQbYEFrDsRX0TRf89UKv6ac0vEejA/640?wx_fmt=png" data-w="368" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757108.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.10 &nbsp;Actor and Observer: Joint Modeling of First and Third-Person Videos<br></strong></p><p>本文由卡耐基梅隆大学、Inria 和艾伦人工智能研究所合作完成，是 CVPR 2018 的 spotlight 文章。认知<mark data-id="306453a1-dae0-4518-9204-c5d533cedbe7" data-type="technologies" class="tooltipstered">神经科学</mark>中的几种理论认为，当人们与世界互动或者模拟互动时，他们从第一人称的自我中心角度出发，并且在第三人（观察者）和第一人称（演员）之间无缝地传递知识。尽管如此，由于缺乏数据，学习这样的模型来识别人类行为并不可行。在本文中，作者创建了一个新的数据集 Charades-Ego。它是一个配对的第一人称视频和第三人视频的大型数据集，涉及 112 人，拥有 4000 对配对视频。这使得学习演员和观察者两者之间的联系。因此，本文解决了自我中心视觉研究的最大瓶颈之一，它提供了从第一人称到第三人称数据的联系。本文利用这些数据来学习了弱监督条件下的第一人称视频和第三人视频的联合表示，并表明了其将知识从第三人转移到第一人称领域的有效性。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPxp2e3NUEJp9qyiaezw7rSQJJzpyiaqowgmGuW5wdjIMeAcAuDFrbBpmw/640?wx_fmt=png" data-w="597" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757178.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>&nbsp;<strong>2.11 Now You Shake Me: Towards Automatic 4D Cinema</strong></p><p>本文由多伦多大学和 Vector Institute 合作完成，是 CVPR 2018 的 spotlight 文章。本文通过自动解析电影中的某些特殊效果来使 4D 影院自动化，这些效果包括溅起的水、光和震动。作者构建了一个新的数据集，称作 Movie4D，这个数据集标注了 63 个电影中的 9000 个特效。为了检测和分类特效，作者提出了建立在<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>之上的 CRF 模型。这个模型利用了电影中的视频、音频、人的轨迹以及特效和角色、电影之间相关性。本文方便了 4D 电影进入家庭。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPtTrUWJznp1bibkz6PRpKc9ib3priczMs4AGQaicdjftG8PVKK0A6UD6Ghg/640?wx_fmt=png" data-w="586" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757257.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.12 &nbsp;Future Person Localization in First-Person Videos</strong></p><p>本文由东京大学与 IIT 合作完成，是 CVPR 2018 的 spotlight 文章。在本文中，作者提出了一项新任务，预测未来在第一人称视频中观看的人的位置。考虑由可佩戴相机连续记录的第一人视频流。给定一个从整个视频流中提取的人的短片，我们的目标是在未来的帧中预测该人的位置。为了促进未来人的定位能力，作者做出以下三个关键观察：a）第一人称视频通常涉及显着的自我运动，这极大地影响了未来帧中目标人的位置; b）目标人的大小作为一个突出提示，可以用于估计第一人称视频中的透视效果; c）第一人称视频经常捕捉人物靠近，使得更容易利用目标姿势（例如，当前位置）来预测他们未来的位置。作者将这三种观察结合到具有多流卷积–反卷积结构的预测框架中。实验结果表明作者提出的方法在新数据集以及公开的社交互动数据集上是有效的。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPJlQicEAIZO9lVDNsbFtX57icF3JsYPQOKx8RNgyqm8CDWedRuY8nKW5g/640?wx_fmt=png" data-w="597" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757316.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.13 &nbsp;MovieGraphs: Towards Understanding Human-Centric Situations From Videos</strong></p><p>本文由多伦多大学、Vector Institute 和 Montreal Institute of Learning Algorithms 合作完成，是 CVPR 2018 的 spotlight 文章。通过人工智能构建社会智能<mark data-id="6c5b1955-9e01-4b6e-b24c-4b6c87d44ed0" data-type="technologies" class="tooltipstered">机器人</mark>要求机器有能力“阅读”人们的情绪、动机和其它影响行为的因素。为了实现这一目标，作者引入了一个名为 MovieGraphs 的新数据集，它提供了详细的基于图的电影剪辑描述的社交情景注释。每张图包含几种类型的节点，可以捕捉剪辑中出现的人、他们的情绪和身体属性、他们的关系（即父母/子女）以及他们之间的交互。大多数互动与提供额外细节的主题以及为动作提供动机的原因相关。另外，大多数互动和许多属性都基于视频。作者对新数据集进行了仔细的分析，显示了场景的不同方面之间的相关性方面以及跨越时间的相关性。作者提出了一种用图来<mark data-id="bf740558-f0f7-41a8-87a0-e695a97563b3" data-type="technologies" class="tooltipstered">查询</mark>视频和文本的方法，并且显示：1）图包含丰富和充足的信息来总结和定位每个场景; 2）子图可以描述抽象层次的情境并检索多个语义相关的情况。作者还通过排序和推理理解提出了交互理解的方法。 MovieGraphs 是第一个关注以人为中心的推断属性的基准数据集，为社交智能 AI 打开了一扇新的大门。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPZbqRgiayibj2LtbEkz4Mnib2n3VojibhuFmZOFXZtZlic7JNr4o9FxFfcibw/640?wx_fmt=png" data-w="542" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757397.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.14 Action Sets: Weakly Supervised Action Segmentation Without Ordering Constraints</strong></p><p>本文由德国波恩大学完成，是 CVPR 2018 的 spotlight 文章。视频中的动作检测和时序分割是研究人员越来越感兴趣的话题。虽然完全有监督系统近来倍受关注，但对视频内的每个操作进行全面标注对于大量视频数据来说代价昂贵且不切合实际。因此，弱监督行为检测和时序分割方法是非常重要的。尽管大多数人在这方面的工作都是假设给定的是有序的行为序列，但作者的方法只使用行为的集合。这样的行为集提供的监督信息少得多，因为行为顺序和行为次数都是未知的。但是它们可以很容易地从元标签中获得，而有序序列仍然需要人工标注。作者引入了一个自动学习的系统，可以在视频中学习时序分段和标注动作，其中唯一使用的监督信息是动作集。在三个数据集上的评估表明，尽管使用的监督信息明显少于其他相关方法，但作者的方法仍然取得了良好的结果。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPEcUoTkbHdwAF36ic6eSGu5DMfiaDNqCibOoHVXnEYlU6788oiaDk5YibbKQ/640?wx_fmt=png" data-w="571" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757459.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>2.15 HAS-RNN: Hierarchical Structure-Adaptive RNN for Video Summarization</strong></p><p>本文由西北工业大学和中国科学院西安光学精密机械研究所联合完成，是 CVPR 2018 的 spotlight 文章。其主要出发点在于，视频是由多个镜头（slot）按照序列组成的。在以往的视频缩略任务中，往往需要先进行镜头的预切割。人工进行镜头预分割十分耗时，但自动分割往往效果不佳。这样就限制了视频缩略方法的直接运用。为解决这一难题，该工作提出将镜头分割和视频缩略融入到一个模型中。如下图所示，该工作提出了一个两层双向 LSTM 模型，第一层负责镜头边缘的检测（也即镜头分割），然后将分割后的镜头输入到第二层模型，进行视频的缩略。该工作实现了从视频到缩略的一站式解决方案，省去了预处理过程，具有很高的现实价值。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPe4LjiafsEDf3YRibGDbRUvW0WLGy7Taia6O7Rw2Lg7OAqlJzIX7oIlAfw/640?wx_fmt=png" data-w="312" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757537.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>&nbsp;2.16 &nbsp;Viewpoint-aware Video Summarization</strong></p><p>&nbsp;本文由东京大学、RIKEN 研究所、苏黎世联邦理工学院和荷语天主教鲁汶大学联合完成，是 CVPR 2018 的 spotlight 文章。该工作提出同一段视频可以有多种不同的缩略结果，取决于用户的需求或者关注点。作者将这种关注点称为视角（viewpoint）。如下图所示，作者展示了两种视角，视频拍摄的地点和视频拍摄的事件。两种视角下产生的缩略显然不同。为了定义视角，作者提出了视频组缩略，而不是对单个视频进行独立的缩略。同组中的视频具有同样的视角，而不同组视频的视角则有所不同，但是具有一定的相关性。该工作借鉴 Fisher 判别准则，提出以下缩略准则：单视频的缩略要与该视频内容相关，同组视频的缩略要相近（体现出相同的视角），不同组视频的缩略要不同（体现出不同的视角）。为了展现视角在视频缩略中的合理性，该工作还给出了一个新的数据集。该工作的主要意义在于，拓展了视频缩略的涵义，从单个视频拓展到视频组的缩略，从单一的缩略拓展到不同视角的缩略。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPrOKs8BygHYicRExiblibiaEKmFYqM9X374PK507rYiaHPJP8ljBpUicGjXLQ/640?wx_fmt=png" data-w="355" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757740.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><section><section><section><section><p><strong>第三部分：三维视觉</strong></p></section></section></section></section></section></section></section><section><section><section><p><strong>3.1 Self-Supervised Multi-level Face Model Learning for Monocular Reconstruction at Over 250 Hz</strong></p><p>本文由马克斯-普朗克研究所和斯坦福大学等机构合作完成，是 CVPR 2018 的 oral 文章。为了提升单张图片重建 3D 脸部模型的效果，该论文采用了多层次的脸部结构重建方法，作者把传统的基于<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>化 3D 可变形模型（3DMM）作为基础模型，在此之上引入纠正模型来增加模型的表达能力。实验表明纠正模型使得 3D 脸部重建效果更接近原图，而且能重建出更多细节。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPVz2TiaeibLTKicq2Kx8aGfHnTdhvcwjOdhzhOaR3tBIDQ4mESs5eIcnVA/640?wx_fmt=png" data-w="897" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757972.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p>基础模型与纠正模型均为线性模型，其中基础模型的基向量通过对训练样本做 PCA 得到（即 3DMM 模型），而纠正模型的基向量由<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>直接学习得到。脸部的形状与纹理通过基础模型加上纠正模型来拟合。算法使用编码器来学习基础模型和纠正模型的组合<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>。随后整合两个模型的结果，通过解码器得到渲染的 3D 脸部模型。然后，算法把 3D 模型成像，对比成像结果与输入图片的差异，目标是使差异变小，因此该方法是自监督的方式进行训练。此外，算法还限制了成像结果与输入图的脸部特征点要对齐。注意该方法中只有编码器是可学习的，而解码器和渲染器都是手工设计的可导层，不是可学习的。为了让模型更加鲁棒和训练过程更加稳定，作者在<mark data-id="4c38563a-2d9b-439e-bfb4-21d209eeff3e" data-type="technologies" class="tooltipstered">损失函数</mark>上加入额外调节项，用于提升纠正模型的平滑性、纹理的稀疏性和整体一致性。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPHxJDx8h5HpYy0RJN4ws8ibjmetjdgDEbuNEGTdyaBul8OibFVa6oRrUA/640?wx_fmt=png" data-w="906" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775757888.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p><strong>3.2 Extreme 3D Face Reconstruction: Seeing Through Occlusions</strong></p><p>本文由美国南加州大学和以色列公开大学合作完成，是 CVPR 2018 的 spotlight 文章。现有的基于单视角图像的 3D 人脸重建算法大多以接近正脸且没有遮挡的图片为输入。本论文提出了一种基于凹凸<mark data-id="8ec6a68f-ad96-4b85-ab72-6f8931886922" data-type="technologies" class="tooltipstered">映射</mark>（bump mapping）的新算法，可用于解决被遮挡图像的 3D 人脸重建问题。该算法把脸部的重建分成两部分，一部分是基于 3DMM 的人脸基础形状和表情的重建，另一部分是局部细节纹理的重建。算法首先利用 BFM 的线性模型对全局形状和表情进行重建，对视角的估计采用了作者之前的工作 FasePoseNet。对于脸部细节的描述，作者使用了 CNN 来学习图片到凹凸图的转换，训练数据采用传统的 Shape from Shading 的方法计算得到。为了复现被遮挡住的细节，作者把非脸部区域看作丢失的信息并采用图像修复算法来填补。而成像角度而形成的自遮挡问题则通过软对称的机制完成。经过上述一系列步骤之后，算法可以获得细节逼真的结果且可以处理遮挡情况下的重建。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPNqiaL9UkwDJTs9XIWhVohSv81xON50rGkGN1XkgXcJuUicdQx8fFMNKQ/640?wx_fmt=png" data-w="780" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775758581.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p>&nbsp;<strong>3.3 Unsupervised Training for 3D Morphable Model Regression</strong></p><p>本文由普林斯顿大学、谷歌和麻省理工学院合作完成，是 CVPR 2018 的 spotlight 文章。使用无监督训练的方法基于 3DMM 进行人脸三维重建。论文基于编码器和解码器模型，创新性地将<mark data-id="3ff3ab31-7595-48d8-905a-baddfecc22be" data-type="technologies" class="tooltipstered">人脸识别</mark>网络引入训练的<mark data-id="4c38563a-2d9b-439e-bfb4-21d209eeff3e" data-type="technologies" class="tooltipstered">损失函数</mark>，使得生成的 3D 人脸能很好地保留了输入图片的人脸个体特征。该模型旨在拟合形状和纹理，并没有学习姿态表情和光照。算法的编码器接受图像作为输入，输出用于 3DMM 模型的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>。解码器接受<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>后合成 3D 人脸。为了使网络不仅能保持个体信息，还能生成自然真实的人脸，作者提出了 3 个新的<mark data-id="4c38563a-2d9b-439e-bfb4-21d209eeff3e" data-type="technologies" class="tooltipstered">损失函数</mark>，即批分布损失（batch distribution loss）、回环损失（loopback loss）和多视角身份损失（multi-view identity loss）。批分布损失可使每个批的统计量与 3DMM 的统计量一致。回环损失可保证生成的 3D 人脸模型的2D成像图片重新进入编码器得到的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>和原图的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>尽量一致。多视角身份损失能使得模型学习到独立于观察角度的个体特征。实验结果说明，模型不仅仅可以生成与输入图像高度相似的 3D 人脸，而且生成的人脸独立于输入的表情和姿态，甚至被遮挡的人脸也可以达到不错的生成效果。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPH5icWIDR5cics2fGicr1ia6477LEV5tgKD6BC8lbKt5ZiabxTcrfAyEQ2jA/640?wx_fmt=png" data-w="582" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775758682.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>&nbsp;<strong>3.4 Mesoscopic Facial Geometry Inference Using Deep <mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">Neural Network</mark>s</strong></p><p>本文由南加州大学、谷歌和 Pinscreen 联合完成，是 CVPR 2018 的 spotlight 文章。本文提出了一个由散射人脸纹理图（diffusely-lit facial texture maps）合成 3D 人脸的算法。该算法结合了图像到图像的转换网络和超分辨率网络。其中图像到图像的转换网络分成两个子网络，分别学习高频和中频信息，使得模型可以捕捉更多细节。基于一系列不同角度拍摄的图像，算法首先计算出基础 mesh 和 1k 分辨率的纹理图。随后通过条件对抗生成网络把输入的纹理图转换成高频和中频两种位移图。高频的位移图通过超分辨率网络提升到 4k 分辨率，而中频的位移图通过升采样提升到 4k 分辨率。这两种频率的位移图整合结束后，把信息重新加到 mesh 上，得到最后的输出。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPRKCpUiaBY8UrpgibyXs419yIPayxd6m4EFk1PHjQQMLwUgsribT8B7Z7A/640?wx_fmt=png" data-w="545" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775758787.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>&nbsp;<strong>3.5 Modeling Facial Geometry Using Compositional VAEs</strong></p><p>本文由瑞士洛桑联邦理工学院与 Facebook 联合完成，是 CVPR 2018 的 spotlight 文章。对人脸建模而言，保证鲁棒性是一个难点，抓住表情是另一个难点。为了解决这两个难点，该论文提出了基于多层次<mark data-id="2970d5b8-08e6-4be6-8bd2-712a3435549b" data-type="technologies" class="tooltipstered">变分自编码器</mark>（compositional VAE）的深度<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>模型。这个算法只需要少量样本就可以训练出一个可以推广到新个体和任意表情的模型。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPEmaTecrrW6bII81j8t4V4jxjNDAgPbiakGCyzdQyA29CAc3ic96Cnj8w/640?wx_fmt=png" data-w="304" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775758843.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPsozg2I7edrHYusZP7fJ1fK5ef86Zj4as0MhftzFuRORXySAPJ7NXAQ/640?wx_fmt=png" data-w="453" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775760674.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>现存的人脸建模算法大部分基于线性模型，而线性的假设限制了模型的表达能力。为了增加模型的表达能力，该算法使用<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>对人脸进行非线性建模。它充分利用卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>对人脸进行整体和局部的多层次建模，其中高层网络抓住整体和低频信息，底层网络抓住局部和高频信息。模型采用了编码器和解码器结合的结构，并将 VAE 的思想融入 U-net 的跳转连接，使模型更具有鲁棒性。编码器的每层输出分成两部分，一部分作为下一层的输入，另一部分为该层隐变量后验分布的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>。解码器的每层输入包括了上一层的输出，以及由该层先验分布抽样得到的隐变量；它的输出包括了下一层的输入和下一册隐变量的先验分布<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>。此外，为了更好地使用这个框架，论文提出了一种新的 mesh 表示方法，使二维图片上的近邻像素和三维拓扑的近邻保持一致。作者表示这个框架可以应用于很多具体任务，包括3D mesh 的对应、2D 的标志性特征拟合、深度图的重建等等。下图为算法对带噪声的深度图进行重建的结果，而训练数据只包括 16 个人。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPjPgRLeRmSia0RLMeRg8JCdI4Z6Tia4TriaBBxIGUZ1RRuerKSagvogJSA/640?wx_fmt=png" data-w="387" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775758967.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>&nbsp;3.6 Nonlinear 3D Face Morphable Model</strong></p><p>本文由密西根州立大学完成，是 CVPR 2018 的 spotlight 文章。现有的基于 3DMM 的人脸重建方法大多是线性模型，线性模型的基向量通过对训练数据做 PCA 得到。由于计算基向量的样本量少，且线性模型的表达能力有限，所以生成效果提升会遇到瓶颈。本文提出了一个非线性人脸可变形模型，不需要采用事先已知的 3D Mesh 基向量，而是通过<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>来将 3DMM <mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>解码出 3D Mesh。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPtQuiczxrG8ecKL55w0d9x8pgaiczlxsoOicorDjyvZdWRjBkcSS5c4EEg/640?wx_fmt=png" data-w="533" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775759046.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>该工作基于编码器-解码器模型。其中编码器通过输入图片学习投影<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>以及形状和纹理<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>。解码器通过形状和纹理<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>直接学习 3D 的形状和纹理，因此可以看做是一种 3D 人脸的非线性可变形模型。随后，基于 z-buffer 算法，渲染层使用投影<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>以及 3D 形状和纹理把 3D 模型渲染成一张 2D 图片。模型的目标是最小化 3D 人脸的 2D 投影与输入图片的像素级差异。为了让生成的人脸更加真实，作者引入了 patchGAN 来学习高质量的纹理和局部特征，还利用与特征标志对齐相关的<mark data-id="4c38563a-2d9b-439e-bfb4-21d209eeff3e" data-type="technologies" class="tooltipstered">损失函数</mark>来调节编码器。实验结果表示，解码器作为一种非线性变形模型有更强大的表示能力，可以重建出更多的人脸细节。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDP8j5IKzj82sFicow6FSacvpqSznx9yB0kd0mTdypNDGkwNzic3hH03d3g/640?wx_fmt=png" data-w="305" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775759132.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>&nbsp;<strong>3.7 Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies</strong></p><p>本文由卡内基梅隆大学和 Facebook 联合完成，获得了 CVPR 2018 的最佳学生论文奖（Best Student Paper Award）。本文提出了一个统一的变形模型，可在无标志物（Marker）的情况下捕获多个尺度的人体运动，包括大尺度的躯体动作以及微妙的脸部和手部动作。在没有标志物的情况下，此前还没有系统可以同时捕获躯体、面部、手部的运动，主要挑战在于人体不同部分的尺度差距较大，比如在一张含有多个人的图像中，面部和手指所占的分辨率通常很小，造成难以捕获其运动。为了解决此问题，作者提出了一种统一的变形模型，可以表达人体每个主要部分的运动情况。具体而言，通过将人体各个部分模型缝合到一个骨架层次结构中，形成初始的 Frank 模型，此单一模型可以完整地表达人体各个部位的运动，包括面部和手部。通过使用多摄像头的捕捉系统大规模采集穿着日常服装的人，进一步优化初始模型得到 Adam 模型。Adam 是一个经过校准的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>化模型，与初始模型享有相同的骨架层次结构，同样包括躯体、面部、手部的模型，另外还可以表达头发和服装的几何形状。Adam 模型可以像其它 3D 可变形模型（例如 &nbsp;SMPL 人体模型、BFM 人脸模型等）一样用于基于单摄像头的整个人体的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>化 3D 重建。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPOunG5JM7JHoOZxeERicZvoxIrrxIo1rEsGEeFHMFFUkTmf9NKKtdZCA/640?wx_fmt=png" data-w="582" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775759203.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p>&nbsp;<strong>3.8 DensePose: Dense Human Pose Estimation in the Wild</strong></p><p>本文由法国国家信息与自动化研究所（INRIA）和Facebook AI 研究中心（FAIR）合作完成，是 CVPR 2018 的 oral 文章。本文提出了一个新的任务，提出密集人体姿态估计 ，并发布了一个人工标注的由 RGB 图像到 3D 模型密集对应的数据集 DensePose-COCO。传统的人体姿态估计主要基于少量的人体关节点，比如手腕等。本文通过将人体表面切分成多个部分，然后在每个部分标注区域关键点的方法，将人体表面关键点扩展到 100-150 个。基于该数据集，本文尝试了基于全卷积网络和基于 Mask-RCNN 的方法，发现基于 Mask-RCNN 的方法表现最优。该方法是在 Mask-RCNN 的基础上，在最后连接一个分类器和回归函数。分类器用于将每一个点分类到属于人体哪个部位，回归函数输出每一个点在各部位表面的 UV 坐标。本文展示的实验结果表明，提出的基于 Mask-RCNN 的方法可以成功学习到由 RGB 图像到 3D 模型的密集对应。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPcEgpicq3icQQkkyILl7jY7Ct7QndFkOAjBsfyQttQG24Ipt8Xd8YCfLA/640?wx_fmt=png" data-w="586" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775759356.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p><strong>3.9 DoubleFusion: Real-Time Capture of Human Performances with Inner Body Shapes From a Single Depth Sensor</strong></p><p>本文由清华大学、北京航空航天大学等机构合作完成，是 CVPR 2018 的 oral 文章。本文研究的问题是单个深度摄像头实时三维人体重建。此前最新的工作 BodyFusion 已经证明利用人体的骨架可以更好地重建三维人体模型。但是，BodyFusion 在跟踪环节只利用了体表信息，并且骨架信息在初始化后是固定的。一旦骨架信息的初始化没有做好，BodyFusion 的效果就会受很大影响。本文提出了一种动态利用骨架信息来约束三维人体模型重建的方法。将传统的 DynamicFusion 的方法作为外部层（outer surface layer），利用 SMPL 来建模内部骨架层（inner body layer），内部层也是通过融合的方法来不断迭代更新，因此该方法称作 DoubleFusion。本文还提出一种利用现有的信息来更新初始化 SMPL <mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>的优化方法。实验结果表明：1）本文提出的内部骨架层可以有效改善快速移动下的三维人体模型重建；2）更新初始化 SMPL <mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>的优化方法可以有效提高三维重建的精度</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPgB9IiaQyoC9htys6tF1gicuLpzv9z3ZGq4YMbxicogdYRib2cKXdZ1ZfMg/640?wx_fmt=png" data-w="487" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775760627.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p><strong>3.10 Video Based Reconstruction of 3D People Models</strong></p><p>本文由马克斯-普朗克研究所和德国布伦瑞克工业大学合作完成，是 CVPR 2018 的 spotlight 文章。给定单个含有一个运动人体（此人从各个角度可见）的单目视频，本文提出的方法首次实现了从中获得精确的三维人体模型，包括头发、衣服及表面纹理。标准的视觉外形方法（visual hull methods）从多个视图捕获静态形状，但是视频中的人体姿态是变化的，本文的核心是将视觉外形方法推广到含有运动人体的单目视频中，即将视频中动态的身体姿势转化为规范的参照系。作者通过对动态人体轮廓对应的轮廓锥进行变换以“消除”人体动态姿势，从而获得公共参照系中的视觉外形，这使得能够有效使用大量帧估计一个共同的人体三维形状。为了对衣服和其他细节进行建模，本文在估计 SMPL 模型<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>的基础上增加一项三维位移的优化<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>。在多个三维数据集上的实验结果表明，此方法可以达到 4.5mm 的三维人体重建精度。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPzWdHwWib35nJc5jKZCrCYguP1GPH8GbbQdPwWmJicE7KnianrPvJy8nqA/640?wx_fmt=png" data-w="536" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775760729.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p><strong>3.11 End-to-End Recovery of Human Shape and Pose</strong></p><p>本文由加州大学伯克利分校、马克斯-普朗克研究所和马里兰大学合作完成，提出了一种根据单张彩色图像端到端恢复人体三维模型的方法。目前大多数方法使用多步骤的方式估计三维人体姿态，首先估计二维关节点位置，然后基于此估计三维关节或者三维模型<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>。本文提出的框架如下图所示，不依赖中间二维关键点检测，直接从单张图片编码特征并回归三维模型<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>，借助 SMPL 模型输出人体三维网格。本文根据关键点投影构造<mark data-id="4c38563a-2d9b-439e-bfb4-21d209eeff3e" data-type="technologies" class="tooltipstered">损失函数</mark>，使得三维关键点投影后与真实二维关键点距离尽可能小。由于同一个二维投影面可以由多个三维模型经过投影得到，为了解决这种不确定性，作者还使用了一个判别器联合监督训练，判断生成的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>是否是真实的人体。通过引入这种条件 GAN 的方式，使得可以在没有任何成对的二维和三维数据的情况下进行训练，为从大规模二维数据学习三维信息提供了可能。此方法在估计三维关节精度方面超过了以前方法，并且在给定图片中人体边框的情况下可以实时运行。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPWq7uCCNLfAHDP2nDbCicibwM0bWIfktpUCibnlibVsECVKgUZtvjwxUtVA/640?wx_fmt=png" data-w="549" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775760851.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>&nbsp;3.12 Learning to Estimate 3D Human Pose and Shape from a Single Color Image</strong></p><p>本文由宾夕法尼亚大学、北京大学和浙江大学共同合作完成。本文同样提出了一种基于 SMPL 模型的端到端<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>模型，可从单张彩色图像恢复人体三维模型。该模型首先由 RGB 图片得到 2D 关键点和轮廓，然后利用 2D 关键点和轮廓信息来分别估计 SMPL 中的 3D 姿态（pose）<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>和形态（shape）<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>。不同于上述前一篇文章（End-to-End Recovery of Human Shape and Pose）中由图像来直接估计 SMPL 中的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>，本文是通过 2D 关键点和轮廓来估计。对于 2D 关键点和轮廓，已有大量公开的数据集和成熟的模型；对于 SMPL <mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>的估计，可以通过公开的运动捕捉数据和人体扫描数据来获取三维模型数据，并且<mark data-id="8ec6a68f-ad96-4b85-ab72-6f8931886922" data-type="technologies" class="tooltipstered">映射</mark>到 2D 图像来获取 2D 关键点和轮廓。实验结果表明该方法在三个公开数据集上取得很好的结果。根据论文中在 Human3.6M 数据集上的性能，之前的方法SMPLify的重建误差是 82.3，本文的结果是 75.9，而前一篇文章能达到 56.8（注意重建误差的数字是越小越好）。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPntFR9KicHwllSy18mbAJmW6ZxhM8zKyOo9kp12xIGtj9SMoyIWZA2Pw/640?wx_fmt=png" data-w="492" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775760909.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>&nbsp;<strong>3.13 CodeSLAM — Learning a Compact, Optimisable Representation for Dense Visual SLAM</strong></p><p>本文由帝国理工学院 Andrew Davison 组完成，是CVPR 2018 荣誉提名奖（Honorable Mention Award）论文，主要关注场景几何信息的紧凑表示方法。场景几何信息的表示方式一直是三维视觉实时系统至关重要的研究问题。传统的密集地图的表示方式虽然可以描述完整的场景几何信息并加上语义标志，但其缺点是密集地图计算复杂高和存储代价大，不利于进行严格的概率推理。相反，稀疏地图的表示方式没有这样的问题，但其仅仅描述了稀疏的场景几何信息，这些信息一般主要用于相机的定位。为了获得密集地图的优点和稀疏地图的优点，本文提出了一种新的紧凑的密集地图表示方法。作者从图像中学习深度图和自编解码器这些方法中获得启发，提出了一种场景几何信息的紧凑表示方法，这种方法可以通过将紧凑编码与原始图像一起送进一个 CNN 解码器中得到场景的密集深度图。在基于关键帧的 SLAM 系统中，可以通过一起优化该编码和相机的位姿来恢复场景的几何信息和相机的运动信息。作者在文章中解释了如何学习得到这个编码，以及这个编码在单目 SLAM 系统中的优点。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPlOoMlbdM6aibUgN6iac7IENSymRNVEyJEr8WblOCAP45Y76D8Ey8hgPw/640?wx_fmt=png" data-w="470" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775760998.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>3.14 Left-Right Comparative Recurrent Model for Stereo Matching</strong></p><p>本文由腾讯 AI Lab 和新加坡国立大学等机构合作完成，是 CVPR 2018 的 oral 文章。在立体视觉匹配问题中，充分利用左右双目的视差信息对于视差估计问题非常关键。左右一致性检测是通过参考对侧信息来提高视差估计质量的有效方法。然而，传统的左右一致性检测是孤立的后处理过程，而且重度依赖人工设计。作者提出了一种全新的左右双目对比的递归<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>模型，同时实现左右一致性检测和视差估计。在每个递归步上，模型同时为双目预测视差结果，然后进行在线左右双目对比并识别出很可能预测错误的左右不匹配区域。作者提出了一种“软<mark data-id="60bee267-89dc-4606-ba24-6b3f7d1f6189" data-type="technologies" class="tooltipstered">注意力机制</mark>”更好地利用学习到的误差图来指导模型在下一步预测中有针对性地修正更新低置信度的区域。通过这种左右对比的递归模型，生成的视差图质量能够不断提高。在 KITTI 2015、Scene Flow 和 Middlebury 标准库上的实验验证了本方法的有效性，并显示本方法能取得很高的立体匹配视差估计性能。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPWq7uCCNLfAHDP2nDbCicibwM0bWIfktpUCibnlibVsECVKgUZtvjwxUtVA/640?wx_fmt=png" data-w="549" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775760792.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPdIzbtYmCRSKiag5rW4WEu4UicYuV0L1zgVACMvm1Gq4GDRIUpichVibX0w/640?wx_fmt=png" data-w="434" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775761071.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>3.15 Learning Depth from Monocular Videos Using Direct Methods</strong></p><p>本文主要由卡内基·梅隆大学完成，提出了一种利用直接法进行无监督训练的单目图像预测深度图方法。作者受到了最近的直接法视觉里程计（DVO）的启发，认为深度图 CNN 可以在没有相机姿态 CNN 的情况下学习得到。具体来讲，由于深度估计和相机姿态估计是紧密联系在一起的问题，本文提出利用 CNN 预测深度图和利用一种可求导的 DVO 计算相机姿态的方法来计算训练损失，从而达到无监督式训练。借助一种有效的深度归一化策略，本文的方法可以使得在单目数据训练中的性能有很大的提升。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDP2MTK5cu7VDZGkfibzibBjDRAYqICh8Kv5ozVicqNOQ72WrHfmscYIV1fQ/640?wx_fmt=png" data-w="367" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775764911.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>3.16 Geometry-Aware Learning of Maps for Camera Localization</strong></p><p>本文由美国佐治亚理工学院和英伟达合作完成，是 &nbsp;CVPR 2018 的 spotlight 文章。该论文提出了一种叫 MapNet 的方法来进行单目 SLAM 的相机 6DOF 定位。 MapNet 跟之前的工作 PoseNet 类似，也是通过卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>直接预测相机的 6DOF 位姿，不同之处是在训练的时候，除了像 PoseNet 一样用相机位姿的基本真值计算损失之外，引入了两帧图像之间的相对位姿计算损失。在此基础上的 MapNet+ 方法可以利用无基本真值的数据进行训练，方法是利用已有的视觉里程计例如 SVO 和 DSO 方法计算得到相对位姿作为训练的基本真值位姿来计算损失。同时，IMU 或 GPS 等数据也可以作为额外的数据来加入训练。另外，本文还提出使用 SLAM 常用的位姿图优化（PGO/Pose Graph Optimization）技术作为后处理来进一步提高位姿估计的精度。本文的另一个创新点是提出用对数域的单位四元数来做位姿中旋转的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>化，作者指出这种<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>化方法更有利于<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPHuZjR3GF0c4SOak8KAgRbV9LsgZCPYzooOELV1hC2v0RX7DdiaAxMOw/640?wx_fmt=png" data-w="511" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775764407.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>3.17 Learning Less Is More - 6D Camera Localization via 3D Surface Regression</strong></p><p>本文由德国海德堡大学完成，跟上面介绍的 MapNet 一样，也是研究从单张图像恢复相机 6DOF 位姿的问题。在目前主流的基于<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>进行相机 6DOF 定位的方法中，一般都采用端到端地学习整个相机定位的过程或者用学习相机定位的大部分流程。这篇文章与其它方法不太一样，作者认为，其实只需要学习相机定位中的一个模块便足够做到精准的定位。基于作者之前的 CVPR 2017 的可导 RANSAC 方法，作者提出了一种用全连接<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>来稠密拟合“场景坐标”的方法，建立输入彩色图像和三维场景空间的联系。“场景坐标”是指将图像中的局部块<mark data-id="8ec6a68f-ad96-4b85-ab72-6f8931886922" data-type="technologies" class="tooltipstered">映射</mark>到三维空间中三维点，从而得到局部块的坐标。由于局部块具有相对比较稳定的外观，即对视角变化不太敏感，使得对齐图像和三维模型比较容易。作者提出的这种方法具有高效、精确、训练鲁棒、泛化能力强等优点，其在室内和室外的数据集上都一致地比当前领先的技术要好。最后值得一提的是，这种方法在训练的时候不需要利用一个已知的三维场景模型，因为训练的时候可以自动地从单目视觉约束中学习到这个三维场景的几何信息。从论文中描述的在 7Scenes 数据集上的性能来看，本篇论文的方法的精度比上面介绍的 MapNet 方法的精度要高出不少。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPDzXIxrBWXJbSxkGX1TXiaHMZSN6Xe9zR5mgtMljyx2e4vVxxQd8eCibA/640?wx_fmt=png" data-w="272" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775764485.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>&nbsp;<strong>3.18 &nbsp;Semantic Visual Localization</strong></p><p>本文由苏黎世联邦理工学院、MPI Tubingen 和微软合作完成，主要研究在大范围的视角变化下的相机定位问题。问题的设定是，假设系统中有一些已知图像及其深度图和相机位姿，给定一个未见过的<mark data-id="bf740558-f0f7-41a8-87a0-e695a97563b3" data-type="technologies" class="tooltipstered">查询</mark>图像，求出其相机的 6DOF 位姿。处理这个困难的问题不仅有挑战性，而且非常实用。例如，<mark data-id="04b05578-4fe7-446c-b7bd-893604f85d04" data-type="technologies" class="tooltipstered">增强现实</mark>和<mark data-id="05e15688-59c9-43b0-9dd5-cd627d4e8d08" data-type="technologies" class="tooltipstered">自动驾驶</mark>的长期定位都需要面对这个问题。作者提出了一种新的基于三维几何和语义的定位方法，这种方法可以克服过去方法失败的情况。作者利用了一个新的<mark data-id="d92a1cda-8e30-4199-9678-2d8c6c17d736" data-type="technologies" class="tooltipstered">生成模型</mark>来进行描述子学习。训练描述子时，用一个语义场景补全任务作为辅助训练任务。训练得到的三维描述子对观察不全的情况很鲁棒，因为训练的时候把高维的三维几何和语义信息编码进去了。作者在几个有挑战性的大型定位数据集上表明了其方法在超大视角变化、光照变化和几何变化的情况下的可靠定位性能。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDP3Vic0lnqiaNwlr5G8TpiaQQOfdd3k4Scf4WrtRiavVicfXu85Fk0mGCJmtg/640?wx_fmt=png" data-w="468" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775764565.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>&nbsp;<strong>3.19 &nbsp;Neural 3D Mesh Renderer</strong></p><p>本文由东京大学完成，是 CVPR 2018 的 spotlight 论文。该论文提出了一种将三维 Mesh 渲染成二维图像的近似可求导模块，可以用于<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>的反向传递中。如果用传统方法直接将三维 Mesh 渲染成二维图像，会涉及一个光栅化的离散操作。这个操作是不能反向传播的，也就是说，不能用于常用的<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>中。为了解决这个问题，这篇文章提出了一种近似的光栅化梯度，使其可用于<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>的渲染模块。作者使用这个方法展示了由单幅图像带轮廓图像监督的三维 Mesh 重建，其性能优于目前的基于体素（Voxel）的方法。除此之外，作者在二维图像的监督下实现了基于梯度的三维网格编辑操作，例如二维到三维的风格转换，三维的 DeepDream 等。这些应用表明了作者提出的这个网络渲染器在<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>的潜在应用和有效性。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPrNunziasDn4KdQDHcDNyjk8ibdmVzBVeRSRw8soeoOzmrUiahPkkJxPFg/640?wx_fmt=png" data-w="337" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775764617.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>3.20 Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene</strong></p><p>本文由加州伯克利大学完成。文章旨在提供一种从单张 2D 图像，通过预测立体场景的三个要素来重建场景 3D 结构的方法。其中三个要素包括：场景布局（layout， 墙面和地板）、物体的形状（shape）以及姿态（pose）。文章首先提出基于三要素的场景 3D 结构表示方法，并展示了对比与两种主要场景三维表示方法（深度图像和体素）的优势。随后作者给出了基于<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>的场景重建网络结构（如下图）。网络输入为 2D 场景图片和物体边框，输出为作者定义的三要素。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPs6hOthBgdjjONGUz626KiaxZUB44Bib8zyYvyg6xiaPhItibUQJqnVcicog/640?wx_fmt=png" data-w="437" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775764694.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>作者在合成数据集上进行了训练，并证实了模型在合成数据和真实数据上都具有良好的表现。部分真实图片的预测结果如下所示。本文方法的预测结果为场景 3D 结构理解提供了更好的解释性。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPq6M4YqHZXjXz7P1V37tpWxiaUcIXh9vAK6N5ZHoNXvc2bGDsC667HeA/640?wx_fmt=png" data-w="444" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775765879.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>3.21 Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net</strong></p><p>本文由 Uber 公司与多伦多大学合作完成，是 CVPR 2018 的 oral 文章。作者首先指出<mark data-id="05e15688-59c9-43b0-9dd5-cd627d4e8d08" data-type="technologies" class="tooltipstered">自动驾驶</mark>任务通常被分割成四个步骤： 物体检测、物体追踪、运动预测和路径规划。四个步骤通常独立学习。这样造成了后续步骤中的错误信息无法反馈给之前的步骤并且运行时间长。 在本文中， 作者提出了一种名为 FaF（Fast and Furious，速度与激情）的端到端全卷积网络方法。该方法利用 3D 传感器提供的时空数据，可同时完成 3D 检测，追踪和运动预测。该方法不仅能在作者采集的大数据上取得最优结果，还可以实时（30ms 内）完成所有任务。值得注意的是，在对单帧点云数据体素化之后，作者并未采用复杂的 3D 卷积操作，而是把高度量化后作为通道信息，在宽度和深度上使用 2D 卷积。这样既保证了网络的高效， 又可以让网络学习准确的高度信息。对于时间信息，作者提供了早期融合（early fusion）和延迟融合（later fusion）两种模型（如下图）。其中早期融合模型运行速度更快，延迟融合<mark data-id="8be77eae-12da-4e9e-9a88-b7f5bae98c2e" data-type="technologies" class="tooltipstered">准确率</mark>更高。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPydrgoqdia3MRktOxJSwj0ibNCIrTAuIOYbIyGfsJrJMDY1vice3Sbu0gQ/640?wx_fmt=png" data-w="499" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775764982.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>在作者采集激光雷达数据集上，该方法在三个任务上表现优异。作者通过消融研究证实了三个任务同时学习的必要性。</p><p><strong>3.22 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks&nbsp;</strong></p><p>本文由 FAIR 与牛津大学联合完成，是 CVPR 2018 的 spotlight 文章。该论文提出的方法及其上一版本（A spatially-sparse convolutional neural network）已经在多个视觉任务比赛中名列前茅，如 2017 年 shapeNet 3D 物体语义分割比赛第一名和 2015 年 Kaggle 举办的糖尿病视网膜病变检测第一名等。 本文在利用三维稀疏结构的基础上强调卷积操作不应减少原有稀疏特性（图1）。为不改变 3D 数据稀疏特性（图2），作者提出了子流形稀疏卷积操作子（SSC，Submanifold Sparse Convolution operator）。具体来说，首先对输入的边缘进行 0 值扩增，以保持操作后输出与输入同尺寸。然后只在以输入样本的激活像素为中心点的区域（图 2，绿色像素）进行卷积操作。这样就保证了卷积操作不改变特征的稀疏结构。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPF6ZCcN0dBRWpp5VS9hpG9TcJU0dCCZXb5t3fIZEHmROAXL8ZPs2NcA/640?wx_fmt=png" data-w="439" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775767664.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p>图 1 左图为原始输入。中图为一次卷积操作后结果。右图为两次卷积后结果。常规卷积运算减少了特征的稀疏特性。特征发生“膨胀（dilation）”</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPLZ3n9cgavnfneffXEZBel3vSmuMwnJk9y3d2wKzzUR5RPOk1TiaEpMQ/640?wx_fmt=png" data-w="427" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775767704.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p>图 2 SSC感受视野在不同位置示意图。绿色为激活像素，红色为忽略像素。输出激活区域的模式与输入保持不变。</p><p>在SSC的基础上，作者又定义了对应的<mark data-id="1697e627-30e7-48a6-b799-39e2338ffab5" data-type="technologies" class="tooltipstered">激活函数</mark>，批归一化和<mark data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" data-type="technologies" class="tooltipstered">池化</mark>函数用以高效的搭建不同的<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>网络。实验表明，该方法计算高效且<mark data-id="8be77eae-12da-4e9e-9a88-b7f5bae98c2e" data-type="technologies" class="tooltipstered">准确率</mark>高。</p><p><strong>3.23 SPLATNet: Sparse Lattice Networks for Point Cloud Processing</strong></p><p>本文由马塞诸塞州大学、加州大学美熹德分校和英伟达公司共同完成，获得本届 CVPR 最佳论文荣誉提名奖（Best Paper Honorable Mention Award）。<mark data-id="05e15688-59c9-43b0-9dd5-cd627d4e8d08" data-type="technologies" class="tooltipstered">自动驾驶</mark>和<mark data-id="6c5b1955-9e01-4b6e-b24c-4b6c87d44ed0" data-type="technologies" class="tooltipstered">机器人</mark>应用中， 通常需要对激光雷达等 3D 传感器获得的不规则数据（如点云（point cloud）和表面（mesh）），进行处理和分析。点云数据的稀疏和无序特性给直接运用卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>带来了困难。为解决上述问题，本文通过引入双边卷积层（BCL/Bilateral Convolution layer）， 构建了一种适用于点云数据的通用灵活的<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>结构——稀疏晶格网络（SPLATNet, SParse LAttice Network）。该网络的核心是双边卷积层，由以下三个步骤完成：</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPDb3v9c0Ob3BGNwccJpgRPzP9ETNFVfDmqAibPp70R5BibEuMNqc7wetA/640?wx_fmt=png" data-w="435" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775767767.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p>a）Splat</p><p>该步骤首先把特征从原空间（欧式空间）投影到Permutohedral Lattice 空间中。然后通过重心插值法（barycentric interpolation）将点特征整合到晶格顶点。这样就将不规则的点云信息表示在了规则的晶格顶点上。</p><p>b）Convolve</p><p>一旦将点云投影到规则的晶格上，卷积操作就和平坦空间卷积类似了。</p><p>c）Slice</p><p>该操作可视为 Splat 的反操作。它通过质心插值法，将特征<mark data-id="8ec6a68f-ad96-4b85-ab72-6f8931886922" data-type="technologies" class="tooltipstered">映射</mark>回原空间。此时的输出点云可以和输入点云相同，也可以不相同。</p><p>在 BCL 的基础， 为解决语义分割问题，作者提供了 3D-3D 和 2D-3D 两种网络结构，如下图所示：</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPnYxaenqcgKUibHyuXBDjJFCuIpfcVHK3Mq12SpibHs2x6UiadJibBna2jQ/640?wx_fmt=png" data-w="439" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775768847.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p>其实验结果如下：</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPHkN6F3gVzxXhGicwBicTdHQhzsDpibsVGb9kyib0RsWcbSRibMd0MPNGlEQ/640?wx_fmt=png" data-w="441" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775770949.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></section></section></section><section><section><section><p><strong>3.24 &nbsp;Tangent Convolutions for Dense Prediction in 3D</strong></p><p>本文由弗莱堡大学和英特尔实验室（Intel lab）共同完成，是 CVPR 2018 的 spotlight 文章。为在 3D 语义场景分析任务中使用深度卷积网络结构，作者提出一种新的卷积操作——切面卷积（Tangent Convolution）。作者首先提出了 3D 数据都是来源于局部欧式表面（locally Euclidean ）采样的假设。 在此基础上，每个点的局部表面几何结构可以投影到该点周围的切平面上。 每个切面图像可以认为是支持平面卷积的二维规则图像。 所以对于 3D 数据，作者推导出如下切面卷积公式：</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPD048RBeg25Ndfyqlyib3ebG3WWhk8h8J2mufm0DlkHt0R01NX0uCmlA/640?wx_fmt=png" data-w="447" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775770847.png" class="fr-fic fr-dib medium-zoom-image" style="width: 43.31%;"></section></section></section><section><section><section><p>其中 P 为 3D &nbsp;输入数据，u为该点在切平面的 K 近邻点，c 为卷积核，w 为 u 的插值函数，g 为近邻选择函数。 因为切平面图像只与输入几何结构有关，作者认为预计算切平面图像后，该方法可以适用于大规模点云数据。</p><p>在切面卷积基础上，作者搭建了如下全卷积 U 形网络 来进行 3D 场景理解。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPaIjr2kesqL920nEmsZDUkhYGPq7dqlPMsWtdEiacnGLw346NtiaJb7KA/640?wx_fmt=png" data-w="389" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775771029.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>部分室内、室外对比实验结果如下所示：</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPGRsCd8gRGa2QjiaKFDA4y008gAyiajKUiauicyOXzv8x7RLqnAs2v0u7pw/640?wx_fmt=png" data-w="458" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775771164.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p><strong>3.25 3D-RCNN: Instance-Level 3D Object Reconstruction via Render-and-Compare</strong></p><p>本文由佐治亚理工和卡耐基梅隆大学合作完成，是 CVPR 2018 的 oral 文章。该论文提出了一种用深度卷积网络从 2D 图像中重建 3D 物体实例的方法。文中，作者首先解决了 3D 形状（shape）和姿态（pose）如何表示的问题。 对于 3D 形状，作者通过对每一类物体的 CAD 模型进行 PCA 建模后，得到了该类物体的一组基向量。之后实例的 3D 形状就可以用一组<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>表示为对应基向量的线性组合。对于姿态，作者认为物体为中心的表示方式（allocentric orientation）相比较于相机为中心的表示方式（egocentric oritation）更加适合学习，因为这种表示方式与物体为中心的 2D 图像感兴趣区域（RoI）的表示方式更加一致。在此基础上，作者给出了名为 3D-RCNN 的网络结构：</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPYyN0J0jNPoXJ6OcHkMa0kdEicU49hwUsFVskiadU8fn5iaP5ZBAehsBeg/640?wx_fmt=png" data-w="469" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775771222.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section><section><section><section><p>网络结构的解渲染部分（De-render）从 2D 图像中预测出上述 3D 表示。为利用更易获取的 2D 图像进行监督训练，作者设计了渲染与对比<mark data-id="4c38563a-2d9b-439e-bfb4-21d209eeff3e" data-type="technologies" class="tooltipstered">损失函数</mark>。 该部分将重建后的 3D 实例进行渲染得到对应 RoI 区域的分割掩膜和深度图像。在与真实结果对比后，利用有限差分法计算近似梯度进行训练。作者首先在合成数据集上进行了训练，然后在真实数据集上进行微调。该网络在真实图像上部分测试结果如下所示。</p></section></section></section><section><section><section><img data-src="https://mmbiz.qpic.cn/mmbiz_png/LWHwEYpIANIicsOdNRzoyYzbg3WYEGPDPERFK9Kt0yMWrVSxkX32lTSyqzFTdPUjImKLWKfQ6OvD1UNmWibRrJvg/640?wx_fmt=png" data-w="429" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/1532775772238.png" class="fr-fic fr-dib medium-zoom-image" style="width: 720px;"></section></section></section></div><div class="article-sidebar js-article-sidebar is-show"><a alt="评论" class="u-btn--circle article-sidebar__item is-comment js-go-comment" data-tip="评论" href="javascript:;"><i class="iconfont icon-message-default"></i></a><a alt="收藏文章" class="u-btn--circle article-sidebar__item js-like-action" data-id="28117fd2-0c7b-4a51-aa7e-a9223b74e1c3" data-path="/articles/2018-07-28-6" data-tip="收藏文章" href="javascript:;"><i class="iconfont icon-like-default"></i></a><a alt="分享文章" class="u-btn--circle article-sidebar__item article-share__box" data-tip="分享文章" href="javascript:;"><i class="iconfont icon-share-default"></i><div class="article-share__list "><button alt="分享到微信" class="u-btn--circle article-share__item article-share__webtn" href="javascript:;"><i class="iconfont icon-wechat"></i><div class="article-share__wechat js-qrcode"><canvas width="120" height="120"></canvas></div></button><button alt="分享到微博" class="u-btn--circle article-share__item js-share-btn" data-title="" data-type="weibo" href="javascript:;"><i class="iconfont icon-weibo"></i></button><button alt="分享到推特" class="u-btn--circle article-share__item js-share-btn" data-title="" data-type="twitter" href="javascript:;"><i class="iconfont icon-twitter"></i></button></div></a></div><div class="u-flex article__other js-article-other"><div class="article__other--tags"><a alt="basic" class="u-btn--gray category__link article__other--tag" href="https://www.jiqizhixin.com/categories/basic">入门</a><a alt="CVPR 2018" class="u-btn--gray category__link article__other--tag" href="javascript:;">CVPR 2018</a></div><div class="article-action js-article-action"><a alt="收藏文章" class="u-like-icon article-action__item js-like-action" data-id="28117fd2-0c7b-4a51-aa7e-a9223b74e1c3" data-path="/articles/2018-07-28-6" href="javascript:;"><i class="iconfont icon-like-default"></i><span class="article-action__count"></span></a><a class="u-like-icon article-action__item js-go-comment js-switch-comment" href="javascript:;"><i class="iconfont icon-message-default"></i><span class="article-action__count"></span></a><a alt="分享" class="article-action__item u-like-icon article-share__box" href="javascript:;"><i class="iconfont icon-share-default"></i><div class="article-share__list is-vertical"><button alt="分享到微信" class="u-btn--circle article-share__item article-share__webtn" href="javascript:;"><i class="iconfont icon-wechat"></i><div class="article-share__wechat js-qrcode"><canvas width="120" height="120"></canvas></div></button><button alt="分享到微博" class="u-btn--circle article-share__item js-share-btn" data-title="CVPR 2018 | 腾讯AI Lab关注的三大方向与55篇论文" data-type="weibo" href="javascript:;"><i class="iconfont icon-weibo"></i></button><button alt="分享到推特" class="u-btn--circle article-share__item js-share-btn" data-title="CVPR 2018 | 腾讯AI Lab关注的三大方向与55篇论文" data-type="twitter" href="javascript:;"><i class="iconfont icon-twitter"></i></button></div></a></div></div></div><div class="u-col-8"><div class="article__hr"></div><div class="article-graph__container"><div class="article-similar__tip">相关数据</div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">激活函数</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Activation function</div></div><p class="graph__content">在 计算网络中， 一个节点的激活函数定义了该节点在给定的输入或输入的集合下的输出。标准的计算机芯片电路可以看作是根据输入得到"开"(1)或"关"(0)输出的数字网络激活函数。这与神经网络中的线性感知机的行为类似。
一种函数（例如 ReLU 或 S 型函数），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">维基百科</a><a class="graph__origin" href="https://developers.google.com/machine-learning/crash-course/glossary">Google ML glossary</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">注意力机制</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Attention mechanism</div></div><p class="graph__content">我们可以粗略地把神经注意机制类比成一个可以专注于输入内容的某一子集（或特征）的神经网络. 注意力机制最早是由 DeepMind 为图像分类提出的，这让「神经网络在执行预测任务时可以更多关注输入中的相关部分，更少关注不相关的部分」。当解码器生成一个用于构成目标句子的词时，源句子中仅有少部分是相关的；因此，可以应用一个基于内容的注意力机制来根据源句子动态地生成一个（加权的）语境向量（context vector）, 然后网络会根据这个语境向量而不是某个固定长度的向量来预测词。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-08-22-6">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">神经网络</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Neural Network</div></div><p class="graph__content">（人工）神经网络是一种起源于 20 世纪 50 年代的监督式机器学习模型，那时候研究者构想了「感知器（perceptron）」的想法。这一领域的研究者通常被称为「联结主义者（Connectionist）」，因为这种模型模拟了人脑的功能。神经网络模型通常是通过反向传播算法应用梯度下降训练的。目前神经网络有两大主要类型，它们都是前馈神经网络：卷积神经网络（CNN）和循环神经网络（RNN），其中 RNN 又包含长短期记忆（LSTM）、门控循环单元（GRU）等等。深度学习是一种主要应用于神经网络帮助其取得更好结果的技术。尽管神经网络主要用于监督学习，但也有一些为无监督学习设计的变体，比如自动编码器和生成对抗网络（GAN）。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">增强现实</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Augmented reality </div></div><p class="graph__content">增强现实，是指透过摄影机影像的位置及角度精算并加上图像分析技术，让屏幕上的虚拟世界能够与现实世界场景进行结合与互动的技术。这种技术于1990年提出。随着随身电子产品运算能力的提升，增强现实的用途也越来越广。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E6%93%B4%E5%A2%9E%E5%AF%A6%E5%A2%83">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">自动编码器</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Autoencoder</div></div><p class="graph__content">自动编码器是用于无监督学习高效编码的人工神经网络。 自动编码器的目的是学习一组数据的表示（编码），通常用于降维。 最近，自动编码器已经越来越广泛地用于生成模型的训练。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Autoencoder">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">深度神经网络</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Deep neural network</div></div><p class="graph__content">深度神经网络（DNN）是深度学习的一种框架，它是一种具备至少一个隐层的神经网络。与浅层神经网络类似，深度神经网络也能够为复杂非线性系统提供建模，但多出的层次为模型提供了更高的抽象层次，因而提高了模型的能力。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-03-30-4">机器之心</a><a class="graph__origin" href="https://www.techopedia.com/definition/32902/deep-neural-network">Techopedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">收敛</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Convergence</div></div><p class="graph__content">在数学，计算机科学和逻辑学中，收敛指的是不同的变换序列在有限的时间内达到一个结论（变换终止），并且得出的结论是独立于达到它的路径（他们是融合的）。
通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练损失和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Convergence_(logic)">Wikipedia</a><a class="graph__origin" href="https://developers.google.com/machine-learning/crash-course/glossary?hl=zh-cn">Google ML glossary</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">条件生成式对抗网络</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Conditional GAN</div></div><p class="graph__content">生成对抗网络是一个训练产生式模型的框架。原始的GAN能够训练一个非条件的产生式模型，它对产生数据的模式没有加以控制。给原始的GAN模型中加入条件约束，使得引导数据产生的过程成为可能，这样的GAN网络称为条件生成式对抗网络。其中，添加的条件可以是类别标签，或者是其他模态的数据等。下图所示为条件生成对抗网络的基本结构，从中可以看出，该模型的关键是在生成器和判别器中分别加入条件作为输入。一个简单的应用例子是以数字类别标签作为条件，训练产生式模型使之能够根据给定的标签产生特定的数字。条件生成式对抗网络能够应用于跨模态问题，例如图像自动标注。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://arxiv.org/abs/1411.1784">Conditional Generative Adversarial Nets</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">计算机视觉</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Computer Vision</div></div><p class="graph__content">计算机视觉（CV）是指机器感知环境的能力。这一技术类别中的经典任务有图像形成、图像处理、图像提取和图像的三维推理。目标识别和面部识别也是很重要的研究领域。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">卷积神经网络</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Convolutional neural network</div></div><p class="graph__content">卷积神经网路（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网路由一个或多个卷积层和顶端的全连通层（对应经典的神经网路）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网路能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网路在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网路，卷积神经网路需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。
卷积网络是一种专门用于处理具有已知的、网格状拓扑的数据的神经网络。例如时间序列数据，它可以被认为是以一定时间间隔采样的一维网格，又如图像数据，其可以被认为是二维像素网格。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2018-07-28-6">Goodfellow, I.; Bengio Y.; Courville A. (2016). Deep Learning.  MIT Press.</a><a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">人脸识别</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Facial recognition</div></div><p class="graph__content">广义的人脸识别实际包括构建人脸识别系统的一系列相关技术，包括人脸图像采集、人脸定位、人脸识别预处理、身份确认以及身份查找等；而狭义的人脸识别特指通过人脸进行身份确认或者身份查找的技术或系统。 人脸识别是一项热门的计算机技术研究领域，它属于生物特征识别技术，是对生物体（一般特指人）本身的生物特征来区分生物体个体。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">生成对抗网络</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Generative Adversarial Networks</div></div><p class="graph__content">生成对抗网络是一种无监督学习方法，是一种通过用对抗网络来训练生成模型的架构。它由两个网络组成：用来拟合数据分布的生成网络G，和用来判断输入是否“真实”的判别网络D。在训练过程中，生成网络-G通过接受一个随机的噪声来尽量模仿训练集中的真实图片去“欺骗”D，而D则尽可能的分辨真实数据和生成网络的输出，从而形成两个网络的博弈过程。理想的情况下，博弈的结果会得到一个可以“以假乱真”的生成模型。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">增量学习</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Incremental learning</div></div><p class="graph__content">增量学习作为机器学习的一种方法，现阶段得到广泛的关注。对于满足以下条件的学习方法可以定义为增量学习方法：
* 可以学习新的信息中的有用信息
* 不需要访问已经用于训练分类器的原始数据
* 对已经学习的知识具有记忆功能
* 在面对新数据中包含的新类别时，可以有效地进行处理</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/%7B%22J%22=%3E%7B%22.%20IEEE%20transactions%20on%20systems,%20man,%20and%20cybernetics,%20part%20C%20(applications%20and%20reviews),%202001,%2031(4):%20497-508.%22=%3E%22%22%7D%7D">Polikar R, Upda L, Upda S S, et al. Learn++: An incremental learning algorithm for supervised neural networks</a><a class="graph__origin" href="https://en.wikipedia.org/wiki/Incremental_learning">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">生成模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Generative Model</div></div><p class="graph__content">在概率统计理论中, 生成模型是指能够随机生成观测数据的模型，尤其是在给定某些隐含参数的条件下。 它给观测值和标注数据序列指定一个联合概率分布。 在机器学习中，生成模型可以用来直接对数据建模（例如根据某个变量的概率密度函数进行数据采样），也可以用来建立变量间的条件概率分布。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/zh-hans/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">映射</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Mapping</div></div><p class="graph__content">映射指的是具有某种特殊结构的函数，或泛指类函数思想的范畴论中的态射。 逻辑和图论中也有一些不太常规的用法。其数学定义为：两个非空集合A与B间存在着对应关系f，而且对于A中的每一个元素x，B中总有有唯一的一个元素y与它对应，就这种对应为从A到B的映射，记作f：A→B。其中，y称为元素x在映射f下的象，记作：y=f(x)。x称为y关于映射f的原象*。*集合A中所有元素的象的集合称为映射f的值域，记作f(A)。同样的，在机器学习中，映射就是输入与输出之间的对应关系。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Mapping">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">损失函数</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Loss function</div></div><p class="graph__content">在数学优化，统计学，计量经济学，决策理论，机器学习和计算神经科学等领域，损失函数或成本函数是将一或多个变量的一个事件或值映射为可以直观地表示某种与之相关“成本”的实数的函数。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Loss_function">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">神经科学</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">neuroscience</div></div><p class="graph__content">神经科学，又称神经生物学，是专门研究神经系统的结构、功能、发育、演化、遗传学、生物化学、生理学、药理学及病理学的一门科学。对行为及学习的研究都是神经科学的分支。 对人脑研究是个跨领域的范畴，当中涉及分子层面、细胞层面、神经小组、大型神经系统，如视觉神经系统、脑干、脑皮层。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E7%A5%9E%E7%BB%8F%E7%A7%91%E5%AD%A6">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">池化</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Pooling</div></div><p class="graph__content">池化（Pooling）是卷积神经网络中的一个重要的概念，它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效的原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的卷积层之间都会周期性地插入池化层。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="http://cs231n.github.io/convolutional-networks/">cs231n</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">参数</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">parameter</div></div><p class="graph__content">在数学和统计学裡，参数（英语：parameter）是使用通用变量来建立函数和变量之间关系（当这种关系很难用方程来阐述时）的一个数量。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E5%8F%82%E6%95%B0">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">查询</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Query</div></div><p class="graph__content">一般来说，查询是询问的一种形式。它在不同的学科里涵义有所不同。在信息检索领域，查询指的是数据库和信息系统对信息检索的精确要求</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Query">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">自动驾驶</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">self-driving </div></div><p class="graph__content">从 20 世纪 80 年代首次成功演示以来（Dickmanns &amp; Mysliwetz (1992); Dickmanns &amp; Graefe (1988); Thorpe et al. (1988)），自动驾驶汽车领域已经取得了巨大进展。尽管有了这些进展，但在任意复杂环境中实现完全自动驾驶导航仍被认为还需要数十年的发展。原因有两个：首先，在复杂的动态环境中运行的自动驾驶系统需要人工智能归纳不可预测的情境，从而进行实时推论。第二，信息性决策需要准确的感知，目前大部分已有的计算机视觉系统有一定的错误率，这是自动驾驶导航所无法接受的。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650725803&amp;idx=1&amp;sn=0805515d0edd5cf01d2be07b435eb312&amp;chksm=871b19d5b06c90c366c2a873ca1156ae61cef284c52c6bb7127f758f7fbb865748658f678a0f&amp;scene=21#wechat_redirect">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">感知</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">perception</div></div><p class="graph__content">知觉或感知是外界刺激作用于感官时，脑对外界的整体的看法和理解，为我们对外界的感官信息进行组织和解释。在认知科学中，也可看作一组程序，包括获取信息、理解信息、筛选信息、组织信息。与感觉不同，知觉反映的是由对象的各样属性及关系构成的整体。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E7%9F%A5%E8%A7%89">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">机器人</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Robotics</div></div><p class="graph__content">机器人学（Robotics）研究的是「机器人的设计、制造、运作和应用，以及控制它们的计算机系统、传感反馈和信息处理」 [25] 。 机器人可以分成两大类：固定机器人和移动机器人。固定机器人通常被用于工业生产（比如用于装配线）。常见的移动机器人应用有货运机器人、空中机器人和自动载具。机器人需要不同部件和系统的协作才能实现最优的作业。其中在硬件上包含传感器、反应器和控制器；另外还有能够实现感知能力的软件，比如定位、地图测绘和目标识别。之前章节中提及的技术都可以在机器人上得到应用和集成，这也是人工智能领域最早的终极目标之一。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">迁移学习</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Transfer learning</div></div><p class="graph__content">迁移学习是一种机器学习方法，就是把为任务 A 开发的模型作为初始点，重新使用在为任务 B 开发模型的过程中。迁移学习是通过从已学习的相关任务中转移知识来改进学习的新任务，虽然大多数机器学习算法都是为了解决单个任务而设计的，但是促进迁移学习的算法的开发是机器学习社区持续关注的话题。 迁移学习对人类来说很常见，例如，我们可能会发现学习识别苹果可能有助于识别梨，或者学习弹奏电子琴可能有助于学习钢琴。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2018-01-04-7">机器之心</a><a class="graph__origin" href="https://www.jiqizhixin.com/articles/2018-07-28-6">Pan, S. J., &amp; Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345–1359.</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">变分自编码器</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Variational autoencoder</div></div><p class="graph__content">变分自编码器可用于对先验数据分布进行建模。从名字上就可以看出，它包括两部分：编码器和解码器。编码器将数据分布的高级特征映射到数据的低级表征，低级表征叫作本征向量（latent vector）。解码器吸收数据的低级表征，然后输出同样数据的高级表征。变分编码器是自动编码器的升级版本，其结构跟自动编码器是类似的，也由编码器和解码器构成。在自动编码器中，需要输入一张图片，然后将一张图片编码之后得到一个隐含向量，这比原始方法的随机取一个随机噪声更好，因为这包含着原图片的信息，然后隐含向量解码得到与原图片对应的照片。但是这样其实并不能任意生成图片，因为没有办法自己去构造隐藏向量，所以它需要通过一张图片输入编码才知道得到的隐含向量是什么，这时就可以通过变分自动编码器来解决这个问题。解决办法就是在编码过程给它增加一些限制，迫使其生成的隐含向量能够粗略的遵循一个标准正态分布，这就是其与一般的自动编码器最大的不同。这样生成一张新图片就比较容易，只需要给它一个标准正态分布的随机隐含向量，这样通过解码器就能够生成想要的图片，而不需要给它一张原始图片先编码。
</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-10-23-3%20https://arxiv.org/abs/1606.05908">机器之心</a><a class="graph__origin" href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">权重</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Weight</div></div><p class="graph__content">线性模型中特征的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/Wikipedia">Google AI Glossary</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">WGAN</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Wasserstein GAN</div></div><p class="graph__content">就其本质而言，任何生成模型的目标都是让模型（习得地）的分布与真实数据之间的差异达到最小。然而，传统 GAN 中的判别器 D 并不会当模型与真实的分布重叠度不够时去提供足够的信息来估计这个差异度——这导致生成器得不到一个强有力的反馈信息（特别是在训练之初），此外生成器的稳定性也普遍不足。
Wasserstein GAN 在原来的基础之上添加了一些新的方法，让判别器 D 去拟合模型与真实分布之间的 Wasserstein 距离。Wassersterin 距离会大致估计出「调整一个分布去匹配另一个分布还需要多少工作」。此外，其定义的方式十分值得注意，它甚至可以适用于非重叠的分布。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-10-05">机器之心</a><a class="graph__origin" href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">深度学习</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Deep learning</div></div><p class="graph__content">深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。
深度学习是机器学习中一种基于对数据进行表征学习的算法，至今已有数种深度学习框架，如卷积神经网络和深度置信网络和递归神经网络等已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。
</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.nature.com/articles/nature14539">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. nature, 521(7553), 436.</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">准确率</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Accuracy</div></div><p class="graph__content">分类模型的正确预测所占的比例。在多类别分类中，准确率的定义为：正确的预测数/样本总数。
在二元分类中，准确率的定义为：(真正例数+真负例数)/样本总数
</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://developers.google.com/machine-learning/crash-course/glossary?hl=zh-cn">Google ML Glossary</a></div></div></div></div><div class="article__hr"></div><div class="article-from u-clearfix"><div class="article-from__author"><div class="u-flex article-from__inline"><a alt="腾讯AI实验室" class="u-avatar-base article-from__avatar" href="https://www.jiqizhixin.com/users/74c81aae-9594-45c0-93db-987e12802125" rel="noopener noreferrer" target="_blank"><img alt="腾讯AI实验室" class="u-image-center" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/anonymous-1-83f4ab515bbe824688aef66da82d70a493aea5bcfe5c83df85db80a77c1591e9.jpg"></a><div><a alt="腾讯AI实验室" class="u-text-limit--one article-from__name" href="https://www.jiqizhixin.com/users/74c81aae-9594-45c0-93db-987e12802125" rel="noopener noreferrer" target="_blank">腾讯AI实验室</a><p class="u-text-limit--two article-from__bio">机器之心编辑</p></div></div></div><div class="article-from__author"><div class="u-flex article-from__inline"><a alt="腾讯AI实验室" class="u-image-base article-from__avatar" href="https://www.jiqizhixin.com/columns/txAlsys" rel="noopener noreferrer" target="_blank"><img alt="腾讯AI实验室" class="u-image-center" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/WechatIMG43.jpeg"></a><div><a alt="腾讯AI实验室" class="u-text-limit--one article-from__name" href="https://www.jiqizhixin.com/columns/txAlsys" rel="noopener noreferrer" target="_blank">腾讯AI实验室</a><p class="u-text-limit--two article-from__bio"></p></div></div></div></div></div></div><div class="article__comment"><div class="u-container"><div class="u-col-8" id="comment-container"><div data-react-class="common/Comment" data-react-props="{&quot;currentUser&quot;:null,&quot;url&quot;:&quot;/articles/2018-07-28-6&quot;}"><div class="comment" id="comment"><div class="comment-inline comment-editor js-comment-area"><a class="is-disabled js-no-user-comment" href="javascript:;"><b>登录</b>后评论</a></div><div class="comment__container"><div class="comment-none__container"><img class="comment-none__cover" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/comment_none-80cb4ed688611aa4f23190a8338f8bab.png" alt="暂无评论"><div class="comment-none__title">暂无评论~</div></div></div></div></div></div></div></div></div><div data-user="null" id="js-login-functionality"></div><footer class="footer u-flex"><div class="u-flex footer__wrapper"><div class="footer__left"><div class="footer__logo"><img height="50" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/logo-c617614d41c836153141ce68ff2b8be19e15cd9c16b2ef1936bc4ad734397392.png"></div><div class="footer-link"><a alt="关于我们" class="footer-link__item" href="https://www.jiqizhixin.com/about" target="_blank">关于我们</a><a alt="寻求报道" class="footer-link__item" href="https://www.jiqizhixin.com/report" target="_blank">寻求报道</a><a alt="商务合作" class="footer-link__item" href="https://www.jiqizhixin.com/business" target="_blank">商务合作</a><a alt="加入我们" class="footer-link__item" href="https://www.jiqizhixin.com/join" target="_blank">加入我们</a><a alt="服务条款" class="footer-link__item" href="https://www.jiqizhixin.com/terms" target="_blank">服务条款</a></div><p class="footer__other">©2017 机器之心（北京）科技有限公司</p><p class="footer__other">京 ICP 备 12027496</p></div><div class="footer__middle"><div class="footer__title">全球人工智能信息服务</div><h5 class="footer__sub-title">友情链接</h5><div class="footer-link"><a alt="Synced Global" class="footer-link__item" href="https://syncedreview.com/" target="_blank">Synced Global</a><a alt="机器之心 Medium 博客" class="footer-link__item" href="https://medium.com/@Synced" target="_blank">机器之心 Medium 博客</a><a alt="PaperWeekly" class="footer-link__item" href="http://paperweek.ly/" target="_blank">PaperWeekly</a><a alt="网易智能" class="footer-link__item" href="http://tech.163.com/smart" target="_blank">网易智能</a><a alt="动脉网" class="footer-link__item" href="http://www.vcbeat.net/" target="_blank">动脉网</a><a alt="硬蛋网" class="footer-link__item" href="http://www.ingdan.com/" target="_blank">硬蛋网</a><a alt="达观数据" class="footer-link__item" href="http://www.datagrand.com/" target="_blank">达观数据</a><a alt="品途商业评论" class="footer-link__item" href="https://www.pintu360.com/" target="_blank">品途商业评论</a></div></div></div><div class="footer-right"><div class="footer__social"><a class="iconfont icon-qq" href="http://wpa.qq.com/msgrd?v=1&amp;uin=2378836078&amp;site=jiqizhxin.com&amp;menu=yes" rel="noopener noreferrer" target="_blank"></a><div class="footer__tooltip"><a class="iconfont icon-wechat" href="javascript:;"></a><div class="footer__tooltip-box"><img alt="机器之心微信公众平台" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/weixinQR-2e2eb9e01d350c1ce6bd7cfe4cf8aa2209a1d72b50f1a6147a371e85548fcdf4.jpg"></div></div><a class="iconfont icon-weibo" href="http://weibo.com/synced" rel="noopener noreferrer" target="_blank"></a><a class="iconfont icon-rss" href="https://jiqizhixin.com/rss" rel="noopener noreferrer" target="_blank"></a></div><p>联系电话：+86 010-57150141</p><p>联系邮箱：contact@jiqizhixin.com</p></div></footer><div class="backtop" id="js-backtop"><a class="backtop__link" href="javascript:;"><i class="iconfont icon-xiangshangjiantou"></i></a><a class="backtop__link is-second">返回顶部</a></div><div data-react-class="common/Modal" data-react-props="{}"><div class="modal-layer"><span></span><span></span></div></div><div data-react-class="common/Alert" data-react-props="{}"><div class="s-alert-wrapper"></div></div><div class="notice__container" id="js-notice-container"></div><script async="" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/analytics.js"></script><script src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/plupload_2.1.6_plupload.full.min.js"></script><script data="208" defsi="361" id="ParadigmSDK" src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/ParadigmSDK_v2_define_itemid.js"></script><script>ParadigmSDK.init("0c2abb5a135747ca9d0f5103e5e95cc3");
ParadigmSDK.trackDetailPageShow("28117fd2-0c7b-4a51-aa7e-a9223b74e1c3")</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-96130205-2', 'auto');
ga('send', 'pageview');</script><script src="./CVPR 2018 _ 腾讯AI Lab关注的三大方向与55篇论文 _ 机器之心_files/application-519a083400a58281ee16.js"></script></body></html>