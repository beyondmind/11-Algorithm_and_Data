<!DOCTYPE html>
<!-- saved from url=(0042)https://www.jiqizhixin.com/articles/091203 -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no" name="viewport"><meta content="ie=edge" http-equiv="X-UA-Compatible"><meta content="telephone=no" name="format-detection"><title>超越像素平面：聚焦3D深度学习的现在和未来 | 机器之心</title>
<meta name="description" content="在过去的 5 年中，3D 深度学习方法已经从使用三维数据的派生表示（二维投影）转变为直接使用原始数据（点云）。">
<meta name="keywords" content="3d, 深度学习, 计算机视觉">
<meta property="og:url" content="https://www.jiqizhixin.com/articles/091203">
<meta property="og:title" content="超越像素平面：聚焦3D深度学习的现在和未来">
<meta property="og:type" content="website">
<meta property="og:site_name" content="机器之心">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@SyncedTech">
<meta name="twitter:title" content="超越像素平面：聚焦3D深度学习的现在和未来">
<meta name="twitter:description" content="在过去的 5 年中，3D 深度学习方法已经从使用三维数据的派生表示（二维投影）转变为直接使用原始数据（点云）。">
<meta name="twitter:image" content="https://image.jiqizhixin.com/uploads/article/cover_image/8d3b1584-e945-4a8c-94ed-33bb162aa79b/image__27_.png"><meta name="csrf-param" content="authenticity_token">
<meta name="csrf-token" content="HgRFCbDI7PPtkdOgTQ1VzdjHpl+ZORzz2GkGrVGW9k2iHuGug1W3Cyx5ppu0uY+RoiWnHXzqGOreyPVSGpuFww=="><link rel="stylesheet" media="all" href="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/application-0aa1744c2fe3f29c13f376789c2b96ff.css"><style type="text/css">.medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--open .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s}.medium-zoom-image--open{position:relative;z-index:1;cursor:pointer;cursor:zoom-out;will-change:transform}</style></head><body data-controller="articles" id="articles-show"><svg aria-hidden="true" style="position: absolute; width: 0px; height: 0px; overflow: hidden;"><symbol id="coloricon-pdf-" viewBox="0 0 1024 1024"><path d="M897.5 267.2v622.7c0 38.4-31.1 69.5-69.5 69.5H196c-38.4 0-69.5-31.1-69.5-69.5V133.7c0-38.4 31.1-69.5 69.5-69.5h501.5v147.4c0 30.7 24.9 55.6 55.6 55.6h144.4z" fill="#FF3F24"></path><path d="M290.1 357.6h437.6v437.6H290.1z" fill="#FFFFFF"></path><path d="M697.9 66.9v154.7c0 15.2 5.9 29.8 16.6 40.6l183 187.2V268.8L697.9 66.9z" fill="#D12003"></path><path d="M697.5 64.5v164c0 21.4 17.3 38.7 38.7 38.7h161.6L697.5 64.5z" fill="#FFC9C0"></path><path d="M622.9 631.1c-11.1-8-30.2-12.3-56.7-12.8-15.4-19.1-30.1-41.7-40.3-61.7 7.4-13.3 11.9-23.1 13.5-29.2 7.2-28.2 1.7-48.7-5.9-58.4-4.4-5.6-10-8.7-15.9-8.7-7.2 0-24 4.6-25.6 47.2-0.5 12.5 3.9 28.8 13 48.4-14.2 24.3-33.5 53-50.6 74.9-14.9 3.2-29.1 6.8-41.3 10.5-32.8 10.1-35.7 24.8-34.5 32.7 1.6 10.8 12.4 18.3 26.3 18.3 6.2 0 12.6-1.6 18.6-4.5 10.2-5.1 24.1-18.6 41.3-40.2 32.9-6.6 66.3-10.5 92.7-10.8 7.4 8.7 17.1 19.2 26 25.9 14.1 10.6 26.3 15.9 36.2 15.9 9.3 0 16.4-4.8 19-12.8 3.5-11.2-3-25.5-15.8-34.7zM415 671.3c-3.3 1.7-6.9 2.6-10.2 2.6-4.8 0-7.6-1.9-8-2.4 0.2-1.3 4.3-7.1 21.6-12.4 5-1.5 10.3-3 15.9-4.5-10.2 11-16.2 15.1-19.3 16.7z m95.4-163c0.8-21.8 6.2-28.8 6.8-29.4 3.4 1.3 11.1 17 4.2 44-0.3 1.2-1.4 4.7-5.1 12.3-5.1-13.3-6.1-21.8-5.9-26.9z m32.1 110.4c-18.5 1-39.3 3.3-60.4 6.8l0.5-0.6-0.8 0.2c11.4-15.7 22.9-33.1 32.9-49.3l0.2 0.3 0.3-0.5c7.9 14.1 17.5 28.8 27.7 42.5h-0.9l0.5 0.6z m78.7 41.2c-0.1 0.1-0.6 0.2-1.6 0.2-2.6 0-10.3-1.2-25.1-12.2-3.2-2.4-7-5.8-11.1-10 17.1 1.9 25.1 5.7 28.7 8.3 8 5.7 9.4 12.6 9.1 13.7z" fill="#FF3F24"></path></symbol><symbol id="coloricon-translation-line1" viewBox="0 0 1024 1024"><path d="M735.744 560.64H665.6l-99.84 275.968h56.832l20.992-58.368H752.64l20.992 58.368h59.392l-97.28-275.968zM658.432 732.16l40.448-117.76h1.536l37.376 117.76h-79.36z" fill="#EB2835"></path><path d="M325.12 126.976H269.312v73.216H146.944v174.592h50.688v-27.136h73.216V476.16h53.76V347.648H399.36v22.528h55.296V200.192H325.12V126.976zM270.336 307.2H197.632V241.664h73.216L270.336 307.2z m128.512 0H324.096V241.664h74.752V307.2z" fill="#9D9D9D"></path><path d="M374.784 643.072H95.744C43.008 643.072 0.512 600.576 0.512 547.84V96.768C0.512 44.032 43.008 1.536 95.744 1.536h451.072c52.736 0 95.232 42.496 95.232 95.232V378.88h-51.2V96.768c0-24.064-19.968-44.032-44.032-44.032H95.744c-24.064 0-44.032 19.968-44.032 44.032V547.84c0 24.064 19.968 44.032 44.032 44.032h279.04v51.2zM972.8 267.264h-51.2c0-90.624-73.728-164.352-164.352-164.352v-51.2c118.784 0 215.552 96.768 215.552 215.552zM266.752 972.288C147.968 972.288 51.2 875.52 51.2 756.736h51.2c0 90.624 73.728 164.352 164.352 164.352v51.2z" fill="#9D9D9D"></path><path d="M925.184 1019.392H474.112c-52.736 0-95.232-42.496-95.232-95.232V473.088c0-52.736 42.496-95.232 95.232-95.232h451.072c52.736 0 95.232 42.496 95.232 95.232v451.072c0 52.736-42.496 95.232-95.232 95.232zM474.112 429.056c-24.064 0-44.032 19.968-44.032 44.032v451.072c0 24.064 19.968 44.032 44.032 44.032h451.072c24.064 0 44.032-19.968 44.032-44.032V473.088c0-24.064-19.968-44.032-44.032-44.032H474.112z" fill="#EB2835"></path></symbol><symbol id="coloricon-translation-line" viewBox="0 0 1024 1024"><path d="M933.376 1004.544c38.4 0 69.632-31.232 69.632-69.632V483.84c0-38.4-31.232-69.632-69.632-69.632H482.304c-38.4 0-69.632 31.232-69.632 69.632v451.072c0 38.4 31.232 69.632 69.632 69.632h451.072z" fill="#EB2835"></path><path d="M90.624 609.792h274.944V461.824c0-56.832 46.08-102.912 102.912-102.912h142.848V89.088c0-38.4-31.232-69.632-69.632-69.632H90.624c-38.4 0-69.632 31.232-69.632 69.632v451.072c0 38.4 31.232 69.632 69.632 69.632z" fill="#9D9D9D"></path><path d="M751.104 581.12H680.96l-99.84 275.968h56.832l20.992-58.368H768l20.992 58.368h59.392l-97.28-275.968zM673.792 752.64l40.448-117.76h1.536l37.376 117.76h-79.36zM314.88 114.176H259.072v73.216H136.704v174.592h50.688v-27.136h73.216v128.512h53.76V334.848H389.12v22.528h55.296V187.392H314.88V114.176zM260.096 294.4H187.392V228.864h73.216l-0.512 65.536z m128.512 0H313.856V228.864h74.752v65.536z" fill="#FFFFFF"></path><path d="M972.8 267.264h-51.2c0-90.624-73.728-164.352-164.352-164.352v-51.2c118.784 0 215.552 96.768 215.552 215.552zM266.752 972.288C147.968 972.288 51.2 875.52 51.2 756.736h51.2c0 90.624 73.728 164.352 164.352 164.352v51.2z" fill="#9D9D9D"></path></symbol></svg><div class="article"><div class="header__hr is-progress-bar" id="js-progress-bar" style="width: 93.95%;"></div><div class="header__hr"></div><header class="header" id="header"><div class="u-container u-flex"><div class="header__left"><a alt="首页" class="header__logo" href="https://www.jiqizhixin.com/"><img alt="机器之心" height="32" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/logo-black-814ff978059dd2570cc09283d01d1e29ebec941b013fc43d3ae6ce3b3f6c2d69.png"></a><div class="header-nav__current js-nav-current"><span>知识</span><i class="iconfont icon-arrowdown u-margin-left header-nav__icon is-first"></i><i class="iconfont icon-iconguanbi u-margin-left header-nav__icon is-second"></i></div></div><div id="js-site-search"><div class="u-in-center header-search "><a class="header-search__btn u-icon-right t-left" href="javascript:;" alt="搜索"><i class="iconfont icon-sreach"></i></a><input type="text" class="header-search__input" placeholder="探索机器之心" value=""><a class="header-search__btn u-icon-left t-right" href="javascript:;" alt="清空搜索框"><i class="iconfont icon-iconguanbi"></i></a></div></div><div class="header__btns js-header-btns"><div class="header__btns"><a class="header-other__link header-user__menu" href="javascript:;">登录</a></div></div></div></header><nav class="header-nav__list js-nav-list"><div class="u-container header-nav__items"><a class="u-borbot__item header-nav__item " href="https://www.jiqizhixin.com/">探索</a><a class="u-borbot__item header-nav__item is-active" href="https://www.jiqizhixin.com/categories/basic">知识</a><a class="u-borbot__item header-nav__item " href="https://www.jiqizhixin.com/categories/industry">产业</a><a alt="AI商用搜索" class="u-borbot__item header-nav__item" href="https://handbook.jiqizhixin.com/" rel="noopener noreferrer" target="_blank">AI商用搜索</a></div></nav><div class="u-min-height-container u-container"><div class="u-col-8"><div class="u-flex article__header"><div class="u-flex article-author"><a alt="机器之心" class="u-avatar-base article-author__avatar" href="https://www.jiqizhixin.com/users/7f316f0c-8f72-4231-bb30-0eb1dd5a5660" rel="noopener noreferrer" target="_blank"><img alt="机器之心" class="u-image-center" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/58d0f028b9fe3.png"></a><div><p class="article-author__attr"><a alt="机器之心" class="article-author__name" href="https://www.jiqizhixin.com/users/7f316f0c-8f72-4231-bb30-0eb1dd5a5660" rel="noopener noreferrer" target="_blank">机器之心</a><span class="article__type">翻译</span></p><time class="article__published">2018/09/12  2:42</time></div></div><div class="article-des u-flex"><div><span class="article-des__inline"><span class="article-des__top">Mihir Garimella、Prathik Naidu</span><span class="article-des__bottom">作者</span></span><span class="article-des__inline"><span class="article-des__top">Geek ai、晓坤</span><span class="article-des__bottom">参与</span></span></div></div></div><h1 class="article__title">超越像素平面：聚焦3D深度学习的现在和未来</h1></div><div class="u-col-8 article__inline" id="js-article-inline"><p class="article__summary">在过去的 5 年中，3D 深度学习方法已经从使用三维数据的派生表示（二维投影）转变为直接使用原始数据（点云）。而在方法上也从将二维卷积神经网络应用到三维数据上转变为专门为三维场景设计的方法，这大大提高了物体分类和语义分割等任务的性能。这些结果非常有前景，它们证明了通过三维技术观察和表示这个世界是有价值的。然而，这个领域才刚刚步入发展的快车道。</p><div class="article__content" id="js-article-content"><p>想象一下，如果你正在建造一辆<mark data-id="05e15688-59c9-43b0-9dd5-cd627d4e8d08" data-type="technologies" class="tooltipstered">自动驾驶</mark>汽车，它需要了解周围的环境。为了安全行驶，你的汽车该如何<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>行人、骑车的人以及周围其它的车辆呢？你可能会想到用一个摄像头来满足这些需求，但实际上，这种做法似乎效果并不好：你面对的是一个三维的环境，相机拍摄会使你把它「压缩」成二维的图像，但最后你需要将二维图像恢复成真正关心的三维图像（比如你前方的行人或车辆与你的距离）。在相机将周围的三维场景压缩成二维图像的过程中，你会丢掉很多最重要的信息。试图恢复这些信息是很困难的，即使我们使用最先进的算法也很容易出错。</p><p>相反，用三维数据来增强对世界的二维视图是非常好的做法。与其试着从一张二维图像中估计你和行人或其它车辆的距离，你不如通过传感器直接对这些物体进行定位。但是，这样做又会使<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>的工作变得十分困难。如何在三维数据中识别人、骑车者和汽车这样的目标呢？传统的像<mark data-id="85c4b79b-6428-4184-b9bc-5beb6e2b1f3f" data-type="technologies" class="tooltipstered">卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark></mark>（CNN）这样的<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>技术，能够使在二维图像中识别这些目标变得简单而直接，但是它们也需要进行一些调整从而适应在三维环境下的工作。幸运的是，三维<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>问题在过去的几年中已经被人们广泛研究，我们在本文中的任务就是对这项工作进行一个简要的概述。</p><p>具体来说，我们将关注最近的三维目标分类和语义分割的<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>技术。我们将首先回顾一些获取和表示三维数据的常用方法的背景。接着，我们将介绍三种不同的表示三维数据的基本<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>方法，最后，我们将介绍未来有前景的新研究方向，并且从我们的视角总结该领域未来的走向。</p><p><strong>我们如何获取并表示三维数据？</strong></p><p>显然，我们需要能够直接在三维空间进行操作的<mark data-id="6e614199-9e49-450e-9078-61fb2b122da9" data-type="technologies" class="tooltipstered">计算机视觉</mark>方法，但是这向我们提出了三个明确的挑战：<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>、表示和理解三维数据。</p><p><strong><mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark></strong></p><p>获取三维数据的过程是复杂的。虽然二维摄像头既便宜又随处可见，但是三维<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>通常需要专用的硬件设备。</p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="0.6983546617915904" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibUFAR2g2bwoRgJ7GLqmna6tZkVLqxaB5ibBicJ4VQPKGDaBAdZoYJBib6nw/640?wx_fmt=png" data-w="547" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734241994.png" class="medium-zoom-image"><span class="fr-inner">Stereo vision 需要使用多个摄像头，测量<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>目标的位置变化，从而计算深度信息。（来源：爱丁堡大学）</span></span></span></p><p>1：立体视觉系统 Stero 需要在相对于待测物体特定的一些位置固定两个或多个摄像头，并且利用这样设定的结构获取不同场景下的图像，匹配相应的像素点，计算每个像素点对应的位置在不同的图像间的差异，从而计算出该像素点在三维空间中的位置。人类大致上就是像这样<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>这个世界的。我们的眼睛会捕捉到两个独立的对于关于我们眼前世界的「图像」，接着大脑会从左眼和右眼的视角来看某个物体的位置有何不同，从而确定其三维位置。Stero 涉及到的硬件系统十分简单，这正是它吸引人的地方，它仅仅需要两个或多个普通的摄像头。然而，这种方法在对<mark data-id="8be77eae-12da-4e9e-9a88-b7f5bae98c2e" data-type="technologies" class="tooltipstered">准确率</mark>和运行速度有要求的应用程序中表现并不好，这是因为使用视觉细节来匹配不同的摄像头得到的图像之间对应的像素点不仅具有很高的计算复杂度，而且在缺乏纹理特征或视觉重复的环境中也很容易出错。</p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="0.5522875816993464" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibUVzXORrAqNribX17uRaTJ6ARGZ1wO4ZdgW7B6MkX1wXo6xq0CAzYN10w/640?wx_fmt=png" data-w="918" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734242095.png" class="medium-zoom-image"><span class="fr-inner">RGB-D 摄像头输出一个四通道图像，该图像包含颜色信息和每个像素的深度（来源：九州大学）</span></span></span></p><p>2：RGB-D 涉及到对一种特殊摄像头的使用，这种摄像头除了颜色信息（「RGB」），还可以获取深度信息（「D」）。具体而言，它能够获取通过普通的二维摄像头得到的彩色图像，而且还能告诉你这些图像中像素点的某些子集代表的物体离摄像头有多远。在系统内部，RGB-D 传感器要么通过「结构光」技术工作，要么通过「飞行时间法」（TOF）工作。「结构光」技术能够将红外线图案投射到一个场景上，并且<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>这种图案是如何在几何表面上变形的；「飞行时间法」会观察其发出的红外线返回摄像头所需要的时间。举例来说，微软的 Kinect 以及 Iphone X 的 FaceID 传感器都是 RGB-D 摄像头。由于这些传感器在尺寸相对较小、成本较低的同时，也能在对视觉匹配错误具有免疫性的同时很快地运行，所以 RGB-D 是一种很好的<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>手段。然而，RGB-D 摄像头的深度输出常常有许多的「孔洞」，这是由于遮挡（前景中的物体遮挡住了其后方物体的投影），模式<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>失败以及<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>范围的问题（在离摄像头更远的地方，投影和<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>会变得很困难）。</p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="0.7345454545454545" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibUtvvibBpibqBgJrkU2Z8bEc3DNafs4rED96UHQs5gqnI5NgI9mXRtwDUg/640?wx_fmt=png" data-w="1100" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734242264.png" class="medium-zoom-image"><span class="fr-inner">激光雷达（LiDAR）使用多个激光束（同心圆模式）直接<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>环境的集合结构（Giphy）。</span></span></span></p><p>3：激光雷达会向物体发出快速激光脉冲，测量它们返回传感器所需要的时间，这类似于我们在上面描述的「飞行时间法」，但是激光雷达的<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>范围更远，能够捕获更多像点，并且抗其它光源干扰的能力更强。目前，大多数的三维激光雷达传感器都会发出一些（多达 64 个）垂直排列的光束，会在传感器周围的各个方向上快速旋转。出于对准确性、<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>范围以及鲁棒性的要求，目前大多数的<mark data-id="05e15688-59c9-43b0-9dd5-cd627d4e8d08" data-type="technologies" class="tooltipstered">自动驾驶</mark>汽车都会采用这种系统。但是激光雷达传感器存在的问题是，它们通常都很大很重并且极其昂贵（大多数<mark data-id="05e15688-59c9-43b0-9dd5-cd627d4e8d08" data-type="technologies" class="tooltipstered">自动驾驶</mark>汽车使用的 64 束传感器需要花费 7.5 万美元）！因此，许多公司目前正在尝试开发更加廉价的「固态激光雷达」系统，它无需旋转就可以在三维场景下进行<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>。</p><p><strong>三维表示</strong></p><p>一旦你获取了三维数据，你就需要以一种形式来表示它，这种形式将作为你正在构建的处理流程的输入。以下是四种你应该已经知道的表示方式：</p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="0.3007456503728252" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibU3n6aZAIyApkyfFUBTAJjvibl1QR25n3TTpEmtYUrfsxgiaiaaibDCWVvCw/640?wx_fmt=png" data-w="2414" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734242387.png" class="medium-zoom-image"><span class="fr-inner">三维数据的不同表示方式。（a）点云（来源：加州理工学院）；（b）体素网格（来源：印度理工学院）；（c）三角网格（来源：华盛顿大学）；（d）多视角表示（来源：斯坦福大学）。</span></span></span></p><p>a. 点云即三维空间中点的集合；每一点都是由某个（xyz）位置决定的，我们同时可以为其指定其它的属性（如 RGB 颜色）。它们是激光雷达数据被获取时的原始形式，立体视觉系统和 RGB-D 数据（包含标有每个像素点深度值的图像）通常在进行进一步处理之前会被转换成点云。</p><p>b. 体素网格是从点云发展而来的。「体素（Voxel）」就好比三维空间中的像素点，我们可以把体素网格看作量化的、大小固定的点云。然而，点云在空间中的任何地方能够以浮点像素坐标的形式涵盖无数个点；体素网格则是一种三维网格，其中的每个单元（或称「体素」）都有固定的大小和离散的坐标。</p><p>c. 多边形网格由一组带公共顶点的凸多边形表面组成，可近似一个几何表面。我们可以将点云看作是从基础的连续集合表面采样得到的三维点集；多边形网格则希望通过一种易于渲染的方式来表示这些基础表面。尽管多边形网格最初是为计算机图形学设计的，但它对于三维视觉也十分有用。我们可以通过几种不同的方法从点云中得到多边形网格，其中包括 Kazhdan 等人于 2006 年提出的「泊松表面重建法」。</p><p>d. 多视图表示是从不同的模拟视角（「虚拟摄像头」）获取到的渲染后的多边形网格二维图像集合，从而通过一种简单的方式表现三维几何结构。简单地从多个摄像头（如立体视觉系统 stereo）捕捉图像和构建多视图表示之间的区别在于，多视图实际上需要构建一个完整的 3D 模型，并从多个任意视点渲染它，以充分表达底层几何结构。与上面的其他三种表示不同，多视图表示通常只用于将 3D 数据转换为易于处理或可视化的格式。</p><p><strong>理解</strong></p><p>现在，你已经将你的三维数据转化成了易于理解的形式，你需要做的是真正构建一个<mark data-id="6e614199-9e49-450e-9078-61fb2b122da9" data-type="technologies" class="tooltipstered">计算机视觉</mark>处理流程来理解它。这里的问题在于，传统的在二维图像上性能良好的<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>技术（如 CNN）在处理三维数据时可能会很困难，具体情况取决于数据的表示。这使得传统的如目标检测或图像分割等任务变得具有挑战性。</p><p><strong>通过多视图输入学习</strong></p><p>使用三维数据的多视图表示是将二维<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>技术应用到三维场景下的最简单的方式。这是一种将三维<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>问题转化为二维<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>问题的明智做法，但它仍然在某种程度上允许你对物体的三维几何结构进行推理。早期的利用这种思想的基于<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>的工作是 Su 等人在 2015 年提出的 multi-view CNN。</p><p>这是一种简单却十分有效的网络架构，它可以从三维物体的多个二维视图中学到特征描述符。实现这种方法与对目标分类任务使用单个二维图像相比，提高了模型的性能。这种方法通过将一张张图像输入在 <mark data-id="f3400606-ef61-441c-8bc3-dd5663313fb9" data-type="technologies" class="tooltipstered">ImageNet</mark> 上预训练好的 VGG 网络，从而提取出显著的特征、将这些结果向量组合在一起，并且将这些信息传递给其余的卷积层进行进一步的特征学习得以实现。</p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="0.38545454545454544" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibU0PE9peosW2Ik2FicUibsPPY3zFnfaGTsMA0cUwjwhOAiaWslYR3bTS28Q/640?wx_fmt=png" data-w="825" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734242471.png" class="medium-zoom-image"><span class="fr-inner">multi-view CNN 架构（来源：multi-view CNN）</span></span></span></p><p>然而，multi-view 图像表示仍然有很多的局限性。主要的问题是你并没有真正在三维环境下学习，固定数量的二维视图仍然只是一个对于底层的三维结构的不完美的近似。因此，像语义分割这样的任务，特别是在跨越更复杂的物体和场景时，就会因为从二维图像中获得的特征信息有限而变得具有挑战性。此外，这种可视化三维数据对于像<mark data-id="05e15688-59c9-43b0-9dd5-cd627d4e8d08" data-type="technologies" class="tooltipstered">自动驾驶</mark>和<mark data-id="459a31cf-f695-4550-8519-19e87f16defb" data-type="technologies" class="tooltipstered">虚拟现实</mark>这样计算开销非常大的任务来说是不可扩展的。请记住，生成多视图表示需要渲染一个完整的三维模型并且模拟一些任意的视点。多视图学习仍然有许多缺陷，这些缺陷促使人们研究直接利用三维数据进行学习的方法。</p><p><strong>通过体积式表示学习</strong></p><p><span data-ratio="1.7777777777777777"><iframe frameborder="0" width="677" height="380.8125" allowfullscreen="true" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/player.html"></iframe></span></p><p>通过体素网格进行学习可以解决多视图表示的主要缺点。体素网格缩小了二维和三维之间的差距，它们是最接近图像的三维表示形式，这使得二维<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>的概念（比如卷积操作）能够容易地应用于三维情景。</p><p>Maturana 和 Scherer 在 2015 年提出的 VoxNet（2015）是最早在给定体素网格输入的情况下在物体分类任务上取得优异表现的<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>方法。VoxNet 使用的是概率占用网格，其中的每个体素都包含了该体素在空间中被占用的概率。这样做的一个好处就是，它允许网络区分已知是自由的体素（例如，激光雷达光束经过的体素）和占用情况未知的体素（例如，激光雷达击中位置后方的体素）。</p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="1.2997542997542997" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibUw2oqz6bPzwqnqGLL5y0nAvMQ21LbzCwotVB5Inib5XCftpgyfvqNtUQ/640?wx_fmt=png" data-w="407" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734242564.png" class="medium-zoom-image"><span class="fr-inner">VoxNet 架构（来源：VoxNet）</span></span></span></p><p>VoxNet 的架构本身非常简单，它由两个卷积层、一个最大<mark data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" data-type="technologies" class="tooltipstered">池化</mark>层和两个用于计算输出的类别得分向量的全连接层组成。这个网络比最先进的图像分类网络要浅得多，<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>也少得多，但是它是从上百种可能的<mark data-id="85c4b79b-6428-4184-b9bc-5beb6e2b1f3f" data-type="technologies" class="tooltipstered">卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark></mark>架构中挑选出来的。由于体素网格与图像十分相似，它们实际上使用的带步长的卷积和<mark data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" data-type="technologies" class="tooltipstered">池化</mark>运算都是从二维像素的工作方式进行调整迁移到三维体素上来的。卷积算子使用的是 d×d×d×c 的卷积核而不是二维<mark data-id="85c4b79b-6428-4184-b9bc-5beb6e2b1f3f" data-type="technologies" class="tooltipstered">卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark></mark>中使用的 d×d×c，<mark data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" data-type="technologies" class="tooltipstered">池化</mark>运算考虑的是不重叠的三维体素块而不是二维像素块。</p><p>VoxNet 存在的一个问题是，该架构本质上并没有旋转不变性，尽管作者合理地假设传感器保持直立，使体素网格的 z 轴和重力方向保持一致，但是并没有假设物体会绕着 z 轴旋转：一个物体从背后看仍然对应相同的物体，即使在体素网格中的几何形状与我们所学的卷积核有很大的不同。为了解决这个问题，他们使用了一个简单的数据增强策略。在训练中，他们多次对每个体素网格进行旋转，并且在所得到的副本上进行训练；接着在测试时，他们将最后的全连接层在输入的不同方向上得到的输出进行<mark data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" data-type="technologies" class="tooltipstered">池化</mark>。他们指出，这种方法比 Su 等人使用的多视图 CNN 在「视图<mark data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" data-type="technologies" class="tooltipstered">池化</mark>」步骤中所做的对中间卷积层输出进行<mark data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" data-type="technologies" class="tooltipstered">池化</mark>的方法能更快<mark data-id="3bf78775-1316-4ac0-bd99-10e2fc88c439" data-type="technologies" class="tooltipstered">收敛</mark>。通过这种方式，VoxNet 通过在输入的体素网格的不同旋转方向上共享相同的学到的卷积核权值来学习这种旋转不变性。</p><p>VoxNet 代表着我们向真正的三维学习迈进了一大步，但是体素网格仍然具有一些缺点。首先，与点云相比，它们丢失了分辨率。因为如果代表复杂结构的不同点距离很近，它们会被被绑定在同一个体素中。与此同时，与稀疏环境中的点云相比，体素网格可能导致不必要的高内存使用率。这是因为它们主动消耗内存来表示自由和未知的空间，而点云只包含已知点。</p><p><strong>通过点云学习</strong></p><p><strong>PointNet</strong></p><p>考虑到这些使用基于体素的方法存在的问题，近期的工作将重点放在了直接在原始点云数据上进行操作的架构上。最值得人们注意的是 Qi 等人于 2016 年提出的 PointNet 是最早的处理这种不规则三维数据的方法。然而，正如作者所指出的，点云仅仅是一组通过 xyz 坐标表示位置的点。更具体地说，当我们给定点云中的 N 个点，网络需要学到相对于这 N 个输入点的全排列不变的唯一特征，因为这些输入给<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark>的点的顺序并不会影响底层的几何形状。此外，网络应该对点云的旋转、平移等转换有很强的鲁棒性，而放缩操作也不应该影响预测结果。</p><p>为了保证对于不同的输入顺序的不变性，PointNet 背后的解决方案的关键思路是使用一个简单的对称函数，该函数为任意顺序排列的输入生成一致的输出（加法和乘法就属于这类函数）。在这种直觉的引导下，PointNet 背后的基本架构（称 PointNet Vanilla）可以定义为：</p><p><img data-ratio="0.12" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibU4zxa8sMfVLApYaHM88VfqBujYfIkcQ449Ln7A9tCicqOledwSGgpxWw/640?wx_fmt=png" data-w="700" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734242661.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><p>其中 f 是将输入点转换为 k 维向量的转换函数（用于物体分类）。该函数 f 可以近似表示为另一个存在的对称函数 g。在方程中，h 是一个多层<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>机（MLP），它将单个输入点（以及它们相应的特征，如 xyz 位置、颜色、表面法线等）<mark data-id="8ec6a68f-ad96-4b85-ab72-6f8931886922" data-type="technologies" class="tooltipstered">映射</mark>到更高维度的潜在空间。最大<mark data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" data-type="technologies" class="tooltipstered">池化</mark>操作则会作为对称函数 g 起作用，它将学到的特征聚合为点云的全局描述符。这个单一特征向量会被传递给 另一个输出物体预测结果的多层<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>机 γ。</p><p>为了应对学习对于点云的几何变换具有不变性的表示方式的挑战，PointNet 使用了一个称为 T-Net 的小型网络，它将仿射变换应用于输入点云。这个概念类似于 Jaderberg 等人于 2016 年提出的空间变换网络（https://arxiv.org/pdf/1506.02025.pdf），但是比它要简单得多，因为在这里不需要定义新的类型的层。T-Net 由可学习的<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>组成，这些<mark data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" data-type="technologies" class="tooltipstered">参数</mark>使 PointNet 能够将输入点云变换为一个固定的、规范的空间，从而确保整个网络对于即使是最细微的变化都具有很强的鲁棒性。</p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="0.38753799392097266" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibUIICMzov3snwSZbCv00Hn2BXZQ82cYBfUOGQiaQ6Ebh6e9XFooicOwKfQ/640?wx_fmt=png" data-w="658" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734242790.png" class="medium-zoom-image"><span class="fr-inner">PointNet 的架构（来源：PointNet）</span></span></span></p><p>整体的 PointNet 架构继承了最基本的方法和 T-Net 以及多层<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>机层，它们为点云创建特征表示。然而，除了物体分类之外，PointNet 还支持对物体和场景进行语义分割。为了实现这一点，该架构将来自最大<mark data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" data-type="technologies" class="tooltipstered">池化</mark>对称函数的全局特征和将输入数据传递给一些多层<mark data-id="1ccbfcc7-1b56-44d6-b64b-729d855abcb1" data-type="technologies" class="tooltipstered">感知</mark>机后学到的点特征相结合。通过将这两个向量连接起来，每个点都知道它的全局语义和局部特征，这使网络能够学习额外的、有助于分割的更有意义的特征。</p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="0.5949367088607594" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibUric9hX7uKhFUWe6Ojiap4yKcBRK4SbVrSnibIUfXSbBqmVlib8ou1j4fHw/640?wx_fmt=png" data-w="553" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734242918.png" class="medium-zoom-image"><span class="fr-inner">使用 PointNet 得到的室内场景的语义分割结果（来源：PointNet）</span></span></span></p><p><strong>PointNet++</strong></p><p>尽管 PointNet 取得了令人满意的结果，其最主要的缺点是该架构不能获取点附近的底层局部结构，这一思想类似于使用 CNN 从图像中不断增大的感受野中提取特征，为了解决这个问题，Qi 等人于 2017 年开发了 PointNet ++。&nbsp;</p><p>它是一个从 PointNet 派生出来的架构，但是也能从点云的局部区域中学习到特征。这种方法的基本结构是一个有层次的特征学习层，它包含三个关键步骤：（1）为局部区域采样一些点作为质心，（2）根据到质心的距离对这些局部区域中的相邻点进行分组，（3）使用一个 mini-PointNet 对这些区域进行特征编码。</p><p>这些步骤将被不断重复，从而在点云中不同大小的点组中学习特征。这样做可以使网络更好地理解整个点云的局部点集中的底层关系，最终有助于提升泛化性能。这项工作的结果表明，PointNet++ 相对于包括 PointNet 在内的现有方法的性能有显著提升，并且在三维点云分析<mark data-id="308c3a45-0fee-4ec6-858e-85b15f440fc0" data-type="technologies" class="tooltipstered">基准</mark>测试（ModelNet40 和 ShapeNet）中取得了目前最好的模型性能。</p><p><strong>有前景的新研究领域</strong></p><p><strong>Graph CNNs</strong></p><p>目前关于处理三维数据的<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>架构的研究主要关注点云表示，而最近许多的工作扩展了从 PointNet/PointNet++ 发展而来的思想，从其它领域获得灵感，进一步提高了模型性能。举例而言，Wang 等人于 2018 年提出的动态图<mark data-id="85c4b79b-6428-4184-b9bc-5beb6e2b1f3f" data-type="technologies" class="tooltipstered">卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark></mark>（https://arxiv.org/pdf/1801.07829.pdf），使用了基于图形的<mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>方法来改进点云中的特征提取工作。其思想是，PointNet 和 PointNet++ 不能捕获各个点之间的几何关系，因为这些方法需要对不同的输入顺序的排列组合保持不变性。然而，通过将一个点和它周围最近的邻居作为一个有向图来考虑，Wang 等人构建了 EdgeConv 算子，它能够生成数据中各点独特的特征。如果你有兴趣学习更多关于图结构上的<mark data-id="1a0e9c5e-6502-4cd7-8683-6b5ca6c48be2" data-type="technologies" class="tooltipstered">机器学习</mark>的知识，可以参阅这篇概述：https://thegradient.pub/structure-learning/</p><p><strong>SPLATNet</strong></p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="0.39975093399750933" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibUgxvY1ArO4cSlzyzgTOXMaHa0ibzkOjiadI1oDLic4qUD7E8wfxY5qt5ibQ/640?wx_fmt=png" data-w="803" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734243036.png" class="medium-zoom-image"><span class="fr-inner">SPLATNet 架构（来源：SPLATNet）</span></span></span></p><p>另一方面，一些研究不同于 PointNet/PointNet++ 中提出的经典特征提取方法，选择设计一种新的点云处理方法。Su 等人于 2018 年提出的 SPLATNet 架构是点云研究领域这个新的研究热点的很好例子。SPLATNet 的作者设计了一个新的架构和卷积算子，它可以对点云直接进行操作。这篇论文背后的关键思想是将「感受野」的概念转换为了不规则点云，这使得空间信息即使在稀疏区域（PointNet/PointNet++ 的一个主要缺陷）中也可以保持。特别吸引人的一点是，SPLATNet 可以将从多视图图像中提取的特征投影到三维空间中，将二维数据与原始点云以一种端到端的可学习的架构进行融合。SPLATNet 通过使用这种二维-三维联合学习取得了目前最好的语义分割模型性能。</p><p><strong>基于截椎体的 PointNet</strong></p><p><span class="fr-img-caption fr-fic fr-dib" style="width: 700%; width: 673px;"><span class="fr-img-wrap"><img data-ratio="0.36084142394822005" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibEI84MHZNUmZexClsgYkibUXPa9GhIH2gFJrLVo0ibKfE7ySkPruVgQmSuJFxdlic7ujFEAZYxcIqfw/640?wx_fmt=png" data-w="2472" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/1536734243183.png" class="medium-zoom-image"><span class="fr-inner">利用二维边界框估计生成的三维截椎体可视化结果（来源：Frustum PointNets）</span></span></span></p><p>基于同时使用二维数据和三维数据的思路，Qi 等人的 Frustrum PointNet 于 2017 年提出了一种将 RGB 图像和点云融合从而提高在大型三维场景中定位物体的新方法。传统的解决该任务的方法是通过直接在整个点云上的滑动窗口上执行分类来确定物体可能的三维边界框，这样做的计算开销非常大，并且很难进行实时预测。Qi 等人的工作有两个主要的贡献。首先，他们建议首先使用标准的<mark data-id="85c4b79b-6428-4184-b9bc-5beb6e2b1f3f" data-type="technologies" class="tooltipstered">卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark></mark>在二维图像上进行物体检测，提取出一个与待检测的物体可能从属点云区域相对应的三维边界框，然后在点云的这个「切片」上进行搜索。这大大缩小了边界框估计的搜索空间，减少了检测错误的可能性，在很大程度上简化了处理流程，这对<mark data-id="05e15688-59c9-43b0-9dd5-cd627d4e8d08" data-type="technologies" class="tooltipstered">自动驾驶</mark>应用十分关键。其次，Qi 等人设计了一种新颖的基于 PointNet 的架构，它可以直接对实例进行分割（将点云分割为一个个独立的物体），并一次性地在整个三维边界框中进行边界框估计，而不是在边界框搜索过程中执行经典的滑动窗口分类工作。这使得他们的方法对遮挡和稀疏等情况即迅速又鲁棒。最终，由于这些改进，这项工作发布时，它在 KITTI 以及 SUN RGB-D 检测等对比<mark data-id="308c3a45-0fee-4ec6-858e-85b15f440fc0" data-type="technologies" class="tooltipstered">基准</mark>上比之前所有的方法都取得了更好的性能。</p><p><strong>结语</strong></p><p>在过去的 5 年中，3D <mark data-id="01946acc-d031-4c0e-909c-f062643b7273" data-type="technologies" class="tooltipstered">深度学习</mark>方法已经从使用三维数据的派生表示（多视图表示）转变为使用原始数据（点云）。在这个过程中，我们采用的方法已经从简单地将二维<mark data-id="85c4b79b-6428-4184-b9bc-5beb6e2b1f3f" data-type="technologies" class="tooltipstered">卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark></mark>应用到三维数据上（多视图<mark data-id="85c4b79b-6428-4184-b9bc-5beb6e2b1f3f" data-type="technologies" class="tooltipstered">卷积<mark data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" data-type="technologies" class="tooltipstered">神经网络</mark></mark>、体素网络 VoxNet）转变为专门为三维场景设计的方法（PointNet 以及其它基于点云的方法），这大大提高了物体分类和语义分割等任务的性能。这些结果非常有前景，因为它们证明了通过三维技术观察和表示这个世界是有价值的。然而，这个领域才刚刚步入发展的快车道。当前的工作不仅要着眼于提高这些算法的<mark data-id="8be77eae-12da-4e9e-9a88-b7f5bae98c2e" data-type="technologies" class="tooltipstered">准确率</mark>和性能，还要确保鲁棒性和可扩展性。尽管目前大多数研究是由无人驾驶应用驱动的，但直接在点云上运行的新方法在三维医学影像、<mark data-id="459a31cf-f695-4550-8519-19e87f16defb" data-type="technologies" class="tooltipstered">虚拟现实</mark>和室内地图中也将发挥重要的作用。</p><p><br></p><p>原文链接：https://thegradient.pub/beyond-the-pixel-plane-sensing-and-learning-in-3d/</p></div><div class="article-sidebar js-article-sidebar is-show"><a alt="评论" class="u-btn--circle article-sidebar__item is-comment js-go-comment" data-tip="评论" href="javascript:;"><i class="iconfont icon-message-default"></i></a><a alt="收藏文章" class="u-btn--circle article-sidebar__item js-like-action" data-path="/articles/091203" data-tip="收藏文章" href="javascript:;"><i class="iconfont icon-like-default"></i></a><a alt="分享文章" class="u-btn--circle article-sidebar__item u-share__box" data-tip="分享文章" href="javascript:;"><i class="iconfont icon-share-default"></i><div class="u-share__list "><button alt="分享到微信" class="u-btn--circle u-share__item u-share__webtn" href="javascript:;"><i class="iconfont icon-wechat"></i><div class="u-share__wechat js-qrcode"><canvas width="120" height="120"></canvas></div></button><button alt="分享到微博" class="u-btn--circle u-share__item js-share-btn" data-title="" data-type="weibo" href="javascript:;"><i class="iconfont icon-weibo"></i></button><button alt="分享到推特" class="u-btn--circle u-share__item js-share-btn" data-title="" data-type="twitter" href="javascript:;"><i class="iconfont icon-twitter"></i></button></div></a></div><div class="u-flex article__other js-article-other"><div class="article__other--tags"><a alt="theory" class="u-btn--gray category__link article__other--tag" href="https://www.jiqizhixin.com/categories/theory">理论</a><span class="u-btn--gray category__link article__other--tag">3D</span><span class="u-btn--gray category__link article__other--tag">深度学习</span><span class="u-btn--gray category__link article__other--tag">计算机视觉</span></div><div class="article-action js-article-action"><a alt="收藏文章" class="u-like-icon article-action__item js-like-action" data-path="/articles/091203" href="javascript:;"><i class="iconfont icon-like-default"></i><span class="article-action__count">2</span></a><a class="u-like-icon article-action__item js-go-comment js-switch-comment" href="javascript:;"><i class="iconfont icon-message-default"></i><span class="article-action__count"></span></a><a alt="分享" class="article-action__item u-like-icon u-share__box" href="javascript:;"><i class="iconfont icon-share-default"></i><div class="u-share__list t-vertical"><button alt="分享到微信" class="u-btn--circle u-share__item u-share__webtn" href="javascript:;"><i class="iconfont icon-wechat"></i><div class="u-share__wechat js-qrcode"><canvas width="120" height="120"></canvas></div></button><button alt="分享到微博" class="u-btn--circle u-share__item js-share-btn" data-title="超越像素平面：聚焦3D深度学习的现在和未来" data-type="weibo" href="javascript:;"><i class="iconfont icon-weibo"></i></button><button alt="分享到推特" class="u-btn--circle u-share__item js-share-btn" data-title="超越像素平面：聚焦3D深度学习的现在和未来" data-type="twitter" href="javascript:;"><i class="iconfont icon-twitter"></i></button></div></a></div></div></div><div class="u-col-8"><div class="article__hr"></div><div class="article-graph__container"><div class="article-similar__tip">相关数据</div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">自动驾驶汽车</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Autonomous cars</div></div><p class="graph__content">自动驾驶汽车，又称为无人驾驶汽车、电脑驾驶汽车或轮式移动机器人，是自动化载具的一种，具有传统汽车的运输能力。作为自动化载具，自动驾驶汽车不需要人为操作即能感测其环境及导航。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%87%AA%E5%8B%95%E9%A7%95%E9%A7%9B%E6%B1%BD%E8%BB%8A?oldformat=true" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/72b0bcc0-d8f9-4edd-919f-fa7c2560388c" target="_blank">神经网络</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Neural Network</div></div><p class="graph__content">（人工）神经网络是一种起源于 20 世纪 50 年代的监督式机器学习模型，那时候研究者构想了「感知器（perceptron）」的想法。这一领域的研究者通常被称为「联结主义者（Connectionist）」，因为这种模型模拟了人脑的功能。神经网络模型通常是通过反向传播算法应用梯度下降训练的。目前神经网络有两大主要类型，它们都是前馈神经网络：卷积神经网络（CNN）和循环神经网络（RNN），其中 RNN 又包含长短期记忆（LSTM）、门控循环单元（GRU）等等。深度学习是一种主要应用于神经网络帮助其取得更好结果的技术。尽管神经网络主要用于监督学习，但也有一些为无监督学习设计的变体，比如自动编码器和生成对抗网络（GAN）。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5" target="_blank">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">收敛</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Convergence</div></div><p class="graph__content">在数学，计算机科学和逻辑学中，收敛指的是不同的变换序列在有限的时间内达到一个结论（变换终止），并且得出的结论是独立于达到它的路径（他们是融合的）。
通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练损失和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Convergence_(logic)" target="_blank">Wikipedia</a><a class="graph__origin" href="https://developers.google.com/machine-learning/crash-course/glossary?hl=zh-cn" target="_blank">Google ML glossary</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/6e614199-9e49-450e-9078-61fb2b122da9" target="_blank">计算机视觉</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Computer Vision</div></div><p class="graph__content">计算机视觉（CV）是指机器感知环境的能力。这一技术类别中的经典任务有图像形成、图像处理、图像提取和图像的三维推理。目标识别和面部识别也是很重要的研究领域。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5" target="_blank">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/f3400606-ef61-441c-8bc3-dd5663313fb9" target="_blank">图网</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">ImageNet</div></div><p class="graph__content"></p></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/85c4b79b-6428-4184-b9bc-5beb6e2b1f3f" target="_blank">卷积神经网络</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Convolutional neural network</div></div><p class="graph__content">卷积神经网路（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网路由一个或多个卷积层和顶端的全连通层（对应经典的神经网路）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网路能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网路在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网路，卷积神经网路需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。
卷积网络是一种专门用于处理具有已知的、网格状拓扑的数据的神经网络。例如时间序列数据，它可以被认为是以一定时间间隔采样的一维网格，又如图像数据，其可以被认为是二维像素网格。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/091203" target="_blank">Goodfellow, I.; Bengio Y.; Courville A. (2016). Deep Learning.  MIT Press.</a><a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">基准</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">baseline</div></div><p class="graph__content">一种简单的模型或启发法，用作比较模型效果时的参考点。基准有助于模型开发者针对特定问题量化最低预期效果。


</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://developers.google.com/machine-learning/crash-course/glossary" target="_blank">Google ML Glossary</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/1a0e9c5e-6502-4cd7-8683-6b5ca6c48be2" target="_blank">机器学习</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Machine Learning</div></div><p class="graph__content">机器学习是人工智能的一个分支，是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/091203" target="_blank">Mitchell, T. (1997). Machine Learning. McGraw Hill.</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/8ec6a68f-ad96-4b85-ab72-6f8931886922" target="_blank">映射</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Mapping</div></div><p class="graph__content">映射指的是具有某种特殊结构的函数，或泛指类函数思想的范畴论中的态射。 逻辑和图论中也有一些不太常规的用法。其数学定义为：两个非空集合A与B间存在着对应关系f，而且对于A中的每一个元素x，B中总有有唯一的一个元素y与它对应，就这种对应为从A到B的映射，记作f：A→B。其中，y称为元素x在映射f下的象，记作：y=f(x)。x称为y关于映射f的原象*。*集合A中所有元素的象的集合称为映射f的值域，记作f(A)。同样的，在机器学习中，映射就是输入与输出之间的对应关系。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Mapping" target="_blank">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/0a4cedf0-0ee0-4406-946e-2877950da91d" target="_blank">池化</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Pooling</div></div><p class="graph__content">池化（Pooling）是卷积神经网络中的一个重要的概念，它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效的原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的卷积层之间都会周期性地插入池化层。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="http://cs231n.github.io/convolutional-networks/" target="_blank">cs231n</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">参数</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">parameter</div></div><p class="graph__content">在数学和统计学裡，参数（英语：parameter）是使用通用变量来建立函数和变量之间关系（当这种关系很难用方程来阐述时）的一个数量。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E5%8F%82%E6%95%B0" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">自动驾驶</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">self-driving </div></div><p class="graph__content">从 20 世纪 80 年代首次成功演示以来（Dickmanns &amp; Mysliwetz (1992); Dickmanns &amp; Graefe (1988); Thorpe et al. (1988)），自动驾驶汽车领域已经取得了巨大进展。尽管有了这些进展，但在任意复杂环境中实现完全自动驾驶导航仍被认为还需要数十年的发展。原因有两个：首先，在复杂的动态环境中运行的自动驾驶系统需要人工智能归纳不可预测的情境，从而进行实时推论。第二，信息性决策需要准确的感知，目前大部分已有的计算机视觉系统有一定的错误率，这是自动驾驶导航所无法接受的。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650725803&amp;idx=1&amp;sn=0805515d0edd5cf01d2be07b435eb312&amp;chksm=871b19d5b06c90c366c2a873ca1156ae61cef284c52c6bb7127f758f7fbb865748658f678a0f&amp;scene=21#wechat_redirect" target="_blank">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">感知</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">perception</div></div><p class="graph__content">知觉或感知是外界刺激作用于感官时，脑对外界的整体的看法和理解，为我们对外界的感官信息进行组织和解释。在认知科学中，也可看作一组程序，包括获取信息、理解信息、筛选信息、组织信息。与感觉不同，知觉反映的是由对象的各样属性及关系构成的整体。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E7%9F%A5%E8%A7%89" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/459a31cf-f695-4550-8519-19e87f16defb" target="_blank">虚拟现实</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Virtual reality </div></div><p class="graph__content">虚拟现实，简称虚拟技术，也称虚拟环境，是利用电脑模拟产生一个三维空间的虚拟世界，提供用户关于视觉等感官的模拟，让用户感觉仿佛身历其境，可以及时、没有限制地观察三维空间内的事物。用户进行位置移动时，电脑可以立即进行复杂的运算，将精确的三维世界视频传回产生临场感。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%99%9A%E6%8B%9F%E7%8E%B0%E5%AE%9E" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/01946acc-d031-4c0e-909c-f062643b7273" target="_blank">深度学习</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Deep learning</div></div><p class="graph__content">深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。
深度学习是机器学习中一种基于对数据进行表征学习的算法，至今已有数种深度学习框架，如卷积神经网络和深度置信网络和递归神经网络等已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。
</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.nature.com/articles/nature14539" target="_blank">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. nature, 521(7553), 436.</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">准确率</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Accuracy</div></div><p class="graph__content">分类模型的正确预测所占的比例。在多类别分类中，准确率的定义为：正确的预测数/样本总数。
在二元分类中，准确率的定义为：(真正例数+真负例数)/样本总数
</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://developers.google.com/machine-learning/crash-course/glossary?hl=zh-cn" target="_blank">Google ML Glossary</a></div></div></div></div><div class="article__hr"></div><div class="article-from u-clearfix"><div class="article-from__author"><div class="u-flex article-from__inline"><a alt="机器之心" class="u-avatar-base article-from__avatar" href="https://www.jiqizhixin.com/users/7f316f0c-8f72-4231-bb30-0eb1dd5a5660" rel="noopener noreferrer" target="_blank"><img alt="机器之心" class="u-image-center" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/58d0f028b9fe3.png"></a><div><a alt="机器之心" class="u-text-limit--one article-from__name" href="https://www.jiqizhixin.com/users/7f316f0c-8f72-4231-bb30-0eb1dd5a5660" rel="noopener noreferrer" target="_blank">机器之心</a><p class="u-text-limit--two article-from__bio">机器之心编辑</p></div></div></div></div></div></div><div class="article__comment"><div class="u-container"><div class="u-col-8" id="comment-container"><div data-react-class="common/Comment" data-react-props="{&quot;currentUser&quot;:null,&quot;url&quot;:&quot;/articles/091203&quot;}"><div class="comment " id="comment"><div class="comment-inline comment-editor js-comment-area"><a class="is-disabled js-no-user-comment" href="javascript:;"><b>登录</b>后评论</a></div><div class="comment__container"><div class="comment-none__container"><img class="comment-none__cover" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/comment_none-80cb4ed688611aa4f23190a8338f8bab.png" alt="暂无评论"><div class="comment-none__title">暂无评论~</div></div></div></div></div></div></div></div></div><div data-user="null" id="js-login-functionality"></div><footer class="footer u-flex"><div class="u-flex footer__wrapper"><div class="footer__left"><div class="footer__logo"><img height="50" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/logo-c617614d41c836153141ce68ff2b8be19e15cd9c16b2ef1936bc4ad734397392.png"></div><div class="footer-link"><a alt="关于我们" class="footer-link__item" href="https://www.jiqizhixin.com/about" target="_blank">关于我们</a><a alt="寻求报道" class="footer-link__item" href="https://www.jiqizhixin.com/report" target="_blank">寻求报道</a><a alt="商务合作" class="footer-link__item" href="https://www.jiqizhixin.com/business" target="_blank">商务合作</a><a alt="加入我们" class="footer-link__item" href="https://www.jiqizhixin.com/join" target="_blank">加入我们</a><a alt="服务条款" class="footer-link__item" href="https://www.jiqizhixin.com/terms" target="_blank">服务条款</a></div><p class="footer__other">©2018 机器之心（北京）科技有限公司</p><p class="footer__other">京 ICP 备 12027496</p></div><div class="footer__middle"><div class="footer__title">全球人工智能信息服务</div><h5 class="footer__sub-title">友情链接</h5><div class="footer-link"><a alt="Synced Global" class="footer-link__item" href="https://syncedreview.com/" target="_blank">Synced Global</a><a alt="机器之心 Medium 博客" class="footer-link__item" href="https://medium.com/@Synced" target="_blank">机器之心 Medium 博客</a><a alt="PaperWeekly" class="footer-link__item" href="http://paperweek.ly/" target="_blank">PaperWeekly</a><a alt="网易智能" class="footer-link__item" href="http://tech.163.com/smart" target="_blank">网易智能</a><a alt="动脉网" class="footer-link__item" href="http://www.vcbeat.net/" target="_blank">动脉网</a><a alt="硬蛋网" class="footer-link__item" href="http://www.ingdan.com/" target="_blank">硬蛋网</a><a alt="达观数据" class="footer-link__item" href="http://www.datagrand.com/" target="_blank">达观数据</a><a alt="品途商业评论" class="footer-link__item" href="https://www.pintu360.com/" target="_blank">品途商业评论</a></div></div></div><div class="footer-right"><div class="footer__social"><a class="iconfont icon-qq" href="http://wpa.qq.com/msgrd?v=1&amp;uin=2378836078&amp;site=jiqizhxin.com&amp;menu=yes" rel="noopener noreferrer" target="_blank"></a><div class="footer__tooltip"><a class="iconfont icon-wechat" href="javascript:;"></a><div class="footer__tooltip-box"><img alt="机器之心微信公众平台" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/weixinQR-2e2eb9e01d350c1ce6bd7cfe4cf8aa2209a1d72b50f1a6147a371e85548fcdf4.jpg"></div></div><a class="iconfont icon-weibo" href="http://weibo.com/synced" rel="noopener noreferrer" target="_blank"></a><a class="iconfont icon-rss" href="https://jiqizhixin.com/rss" rel="noopener noreferrer" target="_blank"></a></div><p>联系电话：+86 010-57150141</p><p>联系邮箱：contact@jiqizhixin.com</p></div></footer><div class="backtop is-show" id="js-backtop"><a class="backtop__link" href="javascript:;"><i class="iconfont icon-xiangshangjiantou backtop__icon"></i></a><a class="backtop__link is-second">返回顶部</a></div><div data-react-class="common/Modal" data-react-props="{}"><div class="modal-layer"><span></span><span></span></div></div><div data-react-class="common/Alert" data-react-props="{}"><div class="s-alert-wrapper"></div></div><div class="notice__container" id="js-notice-container"></div><script async="" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/analytics.js"></script><script src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/plupload_2.1.6_plupload.full.min.js"></script><script data="208" defsi="361" id="ParadigmSDK" src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/ParadigmSDK_v2_define_itemid.js"></script><script>ParadigmSDK.init("0c2abb5a135747ca9d0f5103e5e95cc3");
ParadigmSDK.trackDetailPageShow("8d3b1584-e945-4a8c-94ed-33bb162aa79b")</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-96130205-2', 'auto');
ga('send', 'pageview');</script><script src="./超越像素平面：聚焦3D深度学习的现在和未来 _ 机器之心_files/application-87b7a3d3c46cd6638757.js"></script><div class="tooltipster-base tooltipster-sidetip tooltipster-shadow tooltipster-fade tooltipster-show tooltipster-top" id="tooltipster-947797" style="pointer-events: auto; z-index: 9999999; left: 520px; top: 10693.8px; height: 405px; width: 400px; animation-duration: 350ms; transition-duration: 350ms;"><div class="tooltipster-box"><div class="tooltipster-content">
          <div class="graph__technologies">
            <div class="graph__title">MI 信息数据平台 · 技术</div>
            <div class="graphs__box">
              
              <div class="graph__header">
                <div class="u-main-title graph__keyword">深度学习</div>
                <div class="graph__title graph__translate">Deep learning</div>
              </div>
              <p class="graph__content">深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。
深度学习是机器学习中一种基于对数据进行表征学习的算法，至今已有数种深度学习框架，如卷积神经网络和深度置信网络和递归神经网络等已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。
</p>
              
              
            <div class="graph__title graph__origins">
              <i class="iconfont icon-link2 u-icon-right"></i>来源：
          <a class="graph__origin" href="https://www.nature.com/articles/nature14539" target="_blank">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. nature, 521(7553), 436.</a></div>
            </div>
            
            
          </div>
          <a class="graph__footer" href="https://www.jiqizhixin.com/technologies/01946acc-d031-4c0e-909c-f062643b7273" target="_blank">查看详情</a>
        </div></div><div class="tooltipster-arrow" style="left: 200px;"><div class="tooltipster-arrow-uncropped"><div class="tooltipster-arrow-border"></div><div class="tooltipster-arrow-background"></div></div></div></div></body></html>