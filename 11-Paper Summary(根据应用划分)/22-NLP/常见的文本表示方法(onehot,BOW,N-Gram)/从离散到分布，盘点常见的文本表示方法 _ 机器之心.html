<!DOCTYPE html>
<!-- saved from url=(0049)https://www.jiqizhixin.com/articles/2018-11-07-15 -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no" name="viewport"><meta content="ie=edge" http-equiv="X-UA-Compatible"><meta content="telephone=no" name="format-detection"><title>从离散到分布，盘点常见的文本表示方法 | 机器之心</title>
<meta name="description" content="本篇文章总结了NLP中常用的文本特征表示方式，并提供实际案例和代码实现，用于解决文本分类问题。">
<meta name="keywords" content="云脑科技, 自然语言处理, 人工智能, 文本分类, svm模型, 机器学习">
<meta property="og:url" content="https://www.jiqizhixin.com/articles/2018-11-07-15">
<meta property="og:title" content="从离散到分布，盘点常见的文本表示方法">
<meta property="og:type" content="website">
<meta property="og:site_name" content="机器之心">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@SyncedTech">
<meta name="twitter:title" content="从离散到分布，盘点常见的文本表示方法">
<meta name="twitter:description" content="本篇文章总结了NLP中常用的文本特征表示方式，并提供实际案例和代码实现，用于解决文本分类问题。">
<meta name="twitter:image" content="https://image.jiqizhixin.com/uploads/article/cover_image/0dcc35b8-d277-4f14-85fa-5ee8376d017d/code-1076536_1920.jpg"><meta name="csrf-param" content="authenticity_token">
<meta name="csrf-token" content="EC6/dI0Y6DHfcixLYgcNKohwef21cBS8U1n545H3I5CjS8HVMFRqPD11WxldoNzuPSZtUL0iESGFsLbqEOBE+Q=="><link rel="stylesheet" media="all" href="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/application-af5359bf0000ca318b23bfcfd2936dcd.css"><style type="text/css">.medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--open .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s}.medium-zoom-image--open{position:relative;z-index:1;cursor:pointer;cursor:zoom-out;will-change:transform}</style></head><body data-controller="articles" id="articles-show"><svg aria-hidden="true" style="position: absolute; width: 0px; height: 0px; overflow: hidden;"><symbol id="coloricon-pdf-" viewBox="0 0 1024 1024"><path d="M897.5 267.2v622.7c0 38.4-31.1 69.5-69.5 69.5H196c-38.4 0-69.5-31.1-69.5-69.5V133.7c0-38.4 31.1-69.5 69.5-69.5h501.5v147.4c0 30.7 24.9 55.6 55.6 55.6h144.4z" fill="#FF3F24"></path><path d="M290.1 357.6h437.6v437.6H290.1z" fill="#FFFFFF"></path><path d="M697.9 66.9v154.7c0 15.2 5.9 29.8 16.6 40.6l183 187.2V268.8L697.9 66.9z" fill="#D12003"></path><path d="M697.5 64.5v164c0 21.4 17.3 38.7 38.7 38.7h161.6L697.5 64.5z" fill="#FFC9C0"></path><path d="M622.9 631.1c-11.1-8-30.2-12.3-56.7-12.8-15.4-19.1-30.1-41.7-40.3-61.7 7.4-13.3 11.9-23.1 13.5-29.2 7.2-28.2 1.7-48.7-5.9-58.4-4.4-5.6-10-8.7-15.9-8.7-7.2 0-24 4.6-25.6 47.2-0.5 12.5 3.9 28.8 13 48.4-14.2 24.3-33.5 53-50.6 74.9-14.9 3.2-29.1 6.8-41.3 10.5-32.8 10.1-35.7 24.8-34.5 32.7 1.6 10.8 12.4 18.3 26.3 18.3 6.2 0 12.6-1.6 18.6-4.5 10.2-5.1 24.1-18.6 41.3-40.2 32.9-6.6 66.3-10.5 92.7-10.8 7.4 8.7 17.1 19.2 26 25.9 14.1 10.6 26.3 15.9 36.2 15.9 9.3 0 16.4-4.8 19-12.8 3.5-11.2-3-25.5-15.8-34.7zM415 671.3c-3.3 1.7-6.9 2.6-10.2 2.6-4.8 0-7.6-1.9-8-2.4 0.2-1.3 4.3-7.1 21.6-12.4 5-1.5 10.3-3 15.9-4.5-10.2 11-16.2 15.1-19.3 16.7z m95.4-163c0.8-21.8 6.2-28.8 6.8-29.4 3.4 1.3 11.1 17 4.2 44-0.3 1.2-1.4 4.7-5.1 12.3-5.1-13.3-6.1-21.8-5.9-26.9z m32.1 110.4c-18.5 1-39.3 3.3-60.4 6.8l0.5-0.6-0.8 0.2c11.4-15.7 22.9-33.1 32.9-49.3l0.2 0.3 0.3-0.5c7.9 14.1 17.5 28.8 27.7 42.5h-0.9l0.5 0.6z m78.7 41.2c-0.1 0.1-0.6 0.2-1.6 0.2-2.6 0-10.3-1.2-25.1-12.2-3.2-2.4-7-5.8-11.1-10 17.1 1.9 25.1 5.7 28.7 8.3 8 5.7 9.4 12.6 9.1 13.7z" fill="#FF3F24"></path></symbol><symbol id="coloricon-translation-line1" viewBox="0 0 1024 1024"><path d="M735.744 560.64H665.6l-99.84 275.968h56.832l20.992-58.368H752.64l20.992 58.368h59.392l-97.28-275.968zM658.432 732.16l40.448-117.76h1.536l37.376 117.76h-79.36z" fill="#EB2835"></path><path d="M325.12 126.976H269.312v73.216H146.944v174.592h50.688v-27.136h73.216V476.16h53.76V347.648H399.36v22.528h55.296V200.192H325.12V126.976zM270.336 307.2H197.632V241.664h73.216L270.336 307.2z m128.512 0H324.096V241.664h74.752V307.2z" fill="#9D9D9D"></path><path d="M374.784 643.072H95.744C43.008 643.072 0.512 600.576 0.512 547.84V96.768C0.512 44.032 43.008 1.536 95.744 1.536h451.072c52.736 0 95.232 42.496 95.232 95.232V378.88h-51.2V96.768c0-24.064-19.968-44.032-44.032-44.032H95.744c-24.064 0-44.032 19.968-44.032 44.032V547.84c0 24.064 19.968 44.032 44.032 44.032h279.04v51.2zM972.8 267.264h-51.2c0-90.624-73.728-164.352-164.352-164.352v-51.2c118.784 0 215.552 96.768 215.552 215.552zM266.752 972.288C147.968 972.288 51.2 875.52 51.2 756.736h51.2c0 90.624 73.728 164.352 164.352 164.352v51.2z" fill="#9D9D9D"></path><path d="M925.184 1019.392H474.112c-52.736 0-95.232-42.496-95.232-95.232V473.088c0-52.736 42.496-95.232 95.232-95.232h451.072c52.736 0 95.232 42.496 95.232 95.232v451.072c0 52.736-42.496 95.232-95.232 95.232zM474.112 429.056c-24.064 0-44.032 19.968-44.032 44.032v451.072c0 24.064 19.968 44.032 44.032 44.032h451.072c24.064 0 44.032-19.968 44.032-44.032V473.088c0-24.064-19.968-44.032-44.032-44.032H474.112z" fill="#EB2835"></path></symbol><symbol id="coloricon-translation-line" viewBox="0 0 1024 1024"><path d="M933.376 1004.544c38.4 0 69.632-31.232 69.632-69.632V483.84c0-38.4-31.232-69.632-69.632-69.632H482.304c-38.4 0-69.632 31.232-69.632 69.632v451.072c0 38.4 31.232 69.632 69.632 69.632h451.072z" fill="#EB2835"></path><path d="M90.624 609.792h274.944V461.824c0-56.832 46.08-102.912 102.912-102.912h142.848V89.088c0-38.4-31.232-69.632-69.632-69.632H90.624c-38.4 0-69.632 31.232-69.632 69.632v451.072c0 38.4 31.232 69.632 69.632 69.632z" fill="#9D9D9D"></path><path d="M751.104 581.12H680.96l-99.84 275.968h56.832l20.992-58.368H768l20.992 58.368h59.392l-97.28-275.968zM673.792 752.64l40.448-117.76h1.536l37.376 117.76h-79.36zM314.88 114.176H259.072v73.216H136.704v174.592h50.688v-27.136h73.216v128.512h53.76V334.848H389.12v22.528h55.296V187.392H314.88V114.176zM260.096 294.4H187.392V228.864h73.216l-0.512 65.536z m128.512 0H313.856V228.864h74.752v65.536z" fill="#FFFFFF"></path><path d="M972.8 267.264h-51.2c0-90.624-73.728-164.352-164.352-164.352v-51.2c118.784 0 215.552 96.768 215.552 215.552zM266.752 972.288C147.968 972.288 51.2 875.52 51.2 756.736h51.2c0 90.624 73.728 164.352 164.352 164.352v51.2z" fill="#9D9D9D"></path></symbol></svg><div class="article"><div class="header__hr is-progress-bar" id="js-progress-bar" style="width: 9.82%;"></div><div class="header__hr"></div><header class="header t-absolute t-hidden" id="header"><div class="u-container u-flex"><div class="header__left"><a alt="首页" class="header__logo" href="https://www.jiqizhixin.com/"><img alt="机器之心" height="32" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/logo-black-814ff978059dd2570cc09283d01d1e29ebec941b013fc43d3ae6ce3b3f6c2d69.png"></a><div class="header-nav__current u-flex js-nav-current"><span>知识</span><div class="header-nav__icon u-margin-left"><span class="header-nav__icon--line t-before"></span><span class="header-nav__icon--line t-after"></span></div></div></div><div id="js-site-search"><div class="u-in-center header-search "><a class="header-search__btn u-icon-right t-left" href="javascript:;" alt="搜索"><i class="iconfont icon-sreach"></i></a><input type="text" class="header-search__input" placeholder="全网搜索" value=""><a class="header-search__btn u-icon-left t-right" href="javascript:;" alt="清空搜索框"><i class="iconfont icon-iconguanbi"></i></a></div></div><div class="header__btns"><a class="header-other__link header-user__menu js-switch-sign" href="javascript:;" id="js-header-login">登录</a></div></div></header><nav class="header-nav__list js-nav-list"><div class="u-container header-nav__items"><a class="u-borbot__item header-nav__item " href="https://www.jiqizhixin.com/">探索</a><a class="u-borbot__item header-nav__item is-active" href="https://www.jiqizhixin.com/categories/basic">知识</a><a class="u-borbot__item header-nav__item " href="https://www.jiqizhixin.com/categories/industry">产业</a><a class="u-borbot__item header-nav__item " href="https://www.jiqizhixin.com/columns">专栏</a><a alt="AI商用搜索" class="u-borbot__item header-nav__item" href="https://handbook.jiqizhixin.com/" rel="noopener noreferrer" target="_blank">AI商用搜索</a></div></nav><div class="u-min-height-container u-container"><div class="u-col-8"><div class="u-flex article__header"><div class="u-flex article-author"><a alt="云脑科技" class="u-avatar-base article-author__avatar" href="https://www.jiqizhixin.com/users/69545969-4952-45a0-945c-74561e11c435" rel="noopener noreferrer" target="_blank"><img alt="云脑科技" class="u-image-center" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/avatar-ec28c4a3-c35b-42f9-8e9b-b771bc763189.png"></a><div><p class="article-author__attr"><a alt="云脑科技" class="article-author__name" href="https://www.jiqizhixin.com/users/69545969-4952-45a0-945c-74561e11c435" rel="noopener noreferrer" target="_blank">云脑科技</a><span class="article__type">原创</span></p><time class="article__published">2018/11/08 17:01</time></div></div><div class="article-des u-flex"><div></div></div></div><h1 class="article__title">从离散到分布，盘点常见的文本表示方法</h1></div><div class="u-col-8 article__inline" id="js-article-inline"><div class="article-sidebar js-article-sidebar is-show"><a alt="评论" class="u-btn--circle article-sidebar__item is-comment js-go-comment" data-tip="评论" href="javascript:;"><i class="iconfont icon-message-default"></i></a><a alt="收藏文章" class="u-btn--circle article-sidebar__item js-like-action" data-path="/articles/2018-11-07-15" data-tip="收藏文章" href="javascript:;"><i class="iconfont icon-like-default"></i></a><a alt="分享文章" class="u-btn--circle article-sidebar__item u-share__box" data-tip="分享文章" href="javascript:;"><i class="iconfont icon-share-default"></i><div class="u-share__list "><button alt="分享到微信" class="u-btn--circle u-share__item u-share__webtn" href="javascript:;"><i class="iconfont icon-wechat"></i><div class="u-share__wechat js-qrcode"><canvas width="120" height="120"></canvas></div></button><button alt="分享到微博" class="u-btn--circle u-share__item js-share-btn" data-title="" data-type="weibo" href="javascript:;"><i class="iconfont icon-weibo"></i></button><button alt="分享到推特" class="u-btn--circle u-share__item js-share-btn" data-title="" data-type="twitter" href="javascript:;"><i class="iconfont icon-twitter"></i></button></div></a></div><div class="article__content" id="js-article-content"><p>自然语言处理（NLP）的一些常见任务有：文本分类、指代消歧、自动摘要、机器翻译、主题识别等。传统的处理方法是基于规则的，现在更倾向于使用机器学习或深度学习的方法解决。那么如何在计算机中表达一段文本/一个词的意思呢？第一步必然是将这些语言特征转化为量化的表达方式。本篇文章总结一下NLP中常用的文本特征表示方式，并提供实际案例和代码实现，用于解决文本分类问题。</p><h2><strong>1. 离散表示（Discrete Representation）</strong></h2><h3><strong><em>1.1 One-Hot，独热表示法</em></strong></h3><p>NLP 中最常用、最传统的词特征表示方式是采用One-Hot 编码，即每一个词特征都被表示成一个很长的向量，其长度等于词表大小，当前词对应位置为1，其他位置为0。</p><p>举个简单的例子，如果语料中有以下三段内容：</p><p><img data-ratio="0.5613207547169812" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/oppRknZzPcvD7NHjr5Qccs0VapVKvdCWzbmscXgJNOniavAqaoHI4Bj84ib6oXVLf9cIlbaibl7kpJK49uZsR7r1A/640?wx_fmt=jpeg" data-w="212" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667270538.png" class="fr-fic fr-dib medium-zoom-image" style="width: 33.07%;"></p><p>建立的词表中词汇依次为：I，like，deep，learning，NLP，enjoy, flying .</p><p>将第一句中的词汇用 One-Hot 的方法表示：</p><p><img data-ratio="0.5714285714285714" data-src="https://mmbiz.qpic.cn/mmbiz_png/oppRknZzPcvD7NHjr5Qccs0VapVKvdCWTxRX95VzIwa9YJUeMoaU54J6aLcGbAibUJ6zmica3ZJm9OHk0yZM3UJQ/640?wx_fmt=png" data-w="245" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667292675.png" class="fr-fic fr-dib medium-zoom-image" style="width: 32.59%;"></p><p>但是这种表示方式存在显而易见的问题：</p><p>1. 不同词之间总是正交的，无法衡量不同词之间的相似关系。</p><p>2. 只能反映每个词是否出现，但无法突出词之间重要性的区别。</p><h3><strong><em>1.2 Bag of Words（BOW)，词袋表示法</em></strong></h3><p>在One-Hot 表示法的基础上，对词表中的每一个词在该文本出现的频次进行记录，以表示当前词在该文本的重要程度。例如，对上例中的文本进行 Bag of Words表示：</p><p><img data-ratio="0.28337874659400547" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/oppRknZzPcvD7NHjr5Qccs0VapVKvdCWrx3KHKEjozJu8fdG0Jro6lpdHJ3Eftl9IjxvkghFhyjz3ucWSpASQA/640?wx_fmt=jpeg" data-w="367" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667372486.png" class="fr-fic fr-dib medium-zoom-image" style="width: 47.33%;"></p><p>但这种表示方式只能表达词在当前文本中的重要程度。很多停用词由于频次较高，权重很大。为了表示词特征在整个语料中重要程度，可以使用TF-IDF对词特征加权。</p><p><strong>&gt;&gt;&nbsp;</strong>TF：词频，即每个词在该文本中的频数，表示该词的重要程度。</p><p><strong>&gt;&gt;&nbsp;</strong>IDF: 倒排文档频率。如果有些词在所有文章中出现的次数（DF，文档频率）都很多，可能是停用词或者常见词，一般重要性不高。IDF是DF的倒数形式，值越大说明该词越重要。</p><p>TF*IDF提供了一种词重要程度的较合理的度量。</p><p>对上述文本进行TF-IDF权重表示后的结果：</p><p><img data-ratio="0.27638190954773867" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/oppRknZzPcvD7NHjr5Qccs0VapVKvdCWULZgX8RROBNY0n0mBMAIR3qpbSkIgib36hOz06avwT16zCwOWBSahCw/640?wx_fmt=jpeg" data-w="398" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667394732.png" class="fr-fic fr-dib medium-zoom-image" style="width: 51.16%;"></p><figure>相比较只计算频数的Bag of Words表示，I 和 like 权重被降低了，看起来更合理。但这种方式仍然存在几点问题：</figure><p>1. 词之间是独立的，无法提供词序信息和上下文信息。</p><p>2. 数据十分稀疏。</p><h3><strong><em>1.3 N-Gram，N元组表示法</em></strong></h3><p>上述提到的Bag of Words表示方法每个词都是独立的，忽略了词序问题。增加N-Gram特征可以获取局部的上下文信息。</p><p>以 Bigram 为例重新构建词典：I, like, deep, learning, NLP, enjoy, flying, I like, deep learning, like deep，like NLP, I enjoy, enjoy flying</p><p>注意到词汇表的长度从7增加到13, 使用One-Hot 每个词被表示为13维的向量：</p><p><img data-ratio="0.38441558441558443" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/oppRknZzPcvD7NHjr5Qccs0VapVKvdCWeCg5jH7VvFWOaPbsNqvG0vQ1lricovSMZhic9mtDPNvZvdiaaT6HibCyiaQ/640?wx_fmt=jpeg" data-w="385" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667430962.png" class="fr-fic fr-dib medium-zoom-image" style="width: 48.16%;"></p><figure>使用 Bag of Words 结合TF-IDF Weight，每段文本被表示为13维的向量：</figure><p><img src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1.JPG" style="width: 73.09%;" class="fr-fic fr-dib medium-zoom-image"></p><p>当然，这种方法也有它自己的缺陷：</p><p>1. N-Gram 中随着N的增大增加了更多前后文信息，但是词表的维度也会急剧增大。（通常选取2~3）</p><p>2. 语料库的增加会导致词表维度不断增大，同时N-Gram词序列也急剧增大。</p><p>3. 基于词表示：词之间的关系无法度量 。</p><p>4. 数据十分稀疏。</p><h3><strong><em>1.4 实例: Bag of Words + SVM 分类实现</em></strong></h3><p>下面举个简单的例子来展示词袋模型的应用。</p><p><strong>&gt;&gt;&nbsp;</strong>训练语料：一些公开网站的信息流新闻语料，使用其中的语料标题和网站提供的分类标签。</p><p><strong>&gt;&gt;&nbsp;</strong>特征表示：使用 jieba 进行分词，并进行 Bag of Words 表示。</p><p><strong>&gt;&gt;&nbsp;</strong>模型: 使用机器学习模型SVM 实现文本分类。本次实现使用 sklearn中的SGD分类器，设置loss为hinge损失。</p><p>代码实现：</p><h2><img src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/2.JPG" style="width: 700%;" class="fr-fic fr-dib medium-zoom-image"></h2><h2><strong>2. 分布表示（Distributed Representation）</strong></h2><p>Distributed representation 被称为“Word Representation”或“Word Embedding”， 中文也叫“词向量”或“词嵌入”，1986 年由Hinton 在论文《Learning distributed representations of concepts》中提出。</p><h3><strong><em>2.1 Co-Occurrence 词向量</em></strong></h3><p>上文中提到的几种离散表示方式存在诸多问题，如无法提供充分的上下文的信息、词之间的联系无法度量，即使间接的增加n-gram特征也会导致词表维度急剧增大。一种解决办法是使用上下文来表示单词，这是NLP中很现代的一种想法。2005 年 Rohde等在《An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence》中介绍了使用共现矩阵(Co-Occurrence matrix) 结合SVD降维处理的方法，实现了使用上下文表示单词。</p><p>共现是指不同的词同时出现的文档数。继续使用上面的例子，选择窗宽为1，共现矩阵表示为&nbsp;</p><p><img data-src="https://mmbiz.qpic.cn/mmbiz_png/oppRknZzPcvD7NHjr5Qccs0VapVKvdCWZiags9aIzCV7GvfHpCBIuqg1CtJKRkWNRfQGWFG2UQM6k0e2ic27kWFQ/640?wx_fmt=png" data-w="917" width="612px" data-backh="249" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667486750.png" class="fr-fic fr-dib medium-zoom-image" style="width: 90.82%;"></p><figure>上述共线矩阵存在维度灾难和数据稀疏的问题。一种想法是高维信息用低维的向量表征，因此需要通过降维的方法来解决，一种常用的方法是奇异值分解（SVD）。</figure><p>对上例中的共现矩阵进行分解后，各词在二维坐标中的位置: &nbsp; &nbsp;</p><p><img data-src="https://mmbiz.qpic.cn/mmbiz_png/oppRknZzPcvD7NHjr5Qccs0VapVKvdCW3rtw3jodbtGcPxyq9F73FmcibrRnu5GpoukzxUvDicjUBUWB45Y377kA/640?wx_fmt=png" data-w="1248" width="612px" data-backh="299" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667521441.png" class="fr-fic fr-dib medium-zoom-image" style="width: 90.82%;"></p><figure>即使是结合降维技术的Co-Occurrence matrix 方法也存在一些问题：</figure><p>1. 时间复杂度高，尤其对百万级的单词或者文档表现就很糟糕了。</p><p>2. 新词或新文本难以做到及时更新。</p><p>3. 相对于 deep learning 模型， 会有不同的学习框架。</p><h3><strong><em>2.2 Word2Vec 词向量</em></strong></h3><p>2013年Google 开源了一款直接计算低维词向量的工具 ——Word2Vec，不仅能够在百万级的词典亿级数据集上高效训练，而且能够很好的度量词与词之间的相似性。</p><p>先回顾一下统计语言模型，2003年Bengio等人用三层的神经网络构建了统计语言模型的框架（Neural Network Language Model，简称NNLM)，其基本思想是：</p><p>1. 假定词表中的每个词都对应一个连续的特征向量。</p><p>2. 假定一个连续平滑的概率模型，输入一段词向量序列，可以输出这个序列的联合概率。</p><p>3. 同时学习词向量和概率模型中的参数。</p><p>模型的网络结构如下图：</p><figure><img src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/3.JPG" style="width: 700%;" class="fr-fic fr-dib medium-zoom-image"></figure><p>NNLM 的问题是：只能处理定长序列，而且训练速度慢。</p><p>2013 年Mikolov对原始NNLM 进行了一些改造：</p><p>1. 移除前向反馈神经网络中的非线性hidden layer，直接将中间层的embedding layer 与 softmax layer 连接。</p><p>2. 输入所有词向量到一个embedding layer 中 。</p><p>3. 将特征词嵌入上下文环境。这是Word2Vec的第一个模型——CBoW。</p><p>CBoW的结构图如下:</p><figure><img src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/4.JPG" style="width: 36.9%;" class="fr-fic fr-dib medium-zoom-image"></figure><p>从数学上看CBoW等价与一个词袋模型的向量乘以一个embedding 矩阵，从而得到一个embedding 向量。实际上CBoW是从周边词到中心词的训练中学习到的词向量，相反如果从中心词到周边词训练得到词向量的方法是word2vec的另一个模型——Skip-Gram。</p><p>Skip-Gram 的主要思路：预测一个中心词窗口内（窗口长度为c）的周边单词概率。&nbsp;<img data-src="https://mmbiz.qpic.cn/mmbiz_png/oppRknZzPcvD7NHjr5Qccs0VapVKvdCWpefUZEzlhBP7WBLcqfJJGZicg3M5Vma1FKazeQwjrGMEJppiboYRtDCw/640?wx_fmt=png" data-w="1337" width="612px" data-backh="194" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667570253.png" class="fr-fic fr-dib medium-zoom-image" style="width: 90.82%;"></p><figure>目标函数：对于一个中心词其目标为最大化其周边任意单词的log概率。</figure><figure><img src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/5.JPG" style="width: 41.69%;" class="fr-fic fr-dib medium-zoom-image"></figure><p>Skip-Gram 本质是计算输入词的输入向量与目标词的输出向量之间的余弦相似度，再经过softmax 归一化。显然对词典里的所有词计算相似度并归一化是一件极其耗时的事情。因此，Mikolov 引入两种优化算法：Herarchical Softmax 和 Negative Sampling。</p><h3><strong><em>2.3 GloVe 词向量</em></strong></h3><p>比较以Co-Occurrence为代表的计数方法和word2vec为代表的直接预测方法。</p><figure><img src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/6.JPG" style="width: 700%;" class="fr-fic fr-dib medium-zoom-image"></figure><p>GloVe 思路和Word2Vec很相似，但充分考虑了词的共现情况。而且训练速度更快，在大规模、小规模语料上性能都能表现的很好。其优化目标函数为：</p><figure><img src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/7.JPG" style="width: 67.33%;" class="fr-fic fr-dib medium-zoom-image"></figure><h3><strong><em>2.4 实例: fastText 分类中训练词向量</em></strong></h3><p>类似于 Word2Vec 中 CBoW 模型，fastText 的分类模型更灵活的使用了 Hierarchical Softmax，主要体现在:</p><p>1. Wordvec 最终在输入层得到词向量，输出层对应的 Herarchical Softmax 也会生成一系列的向量，但最终都不会使用。而fastText的输出层对应是分类的label，目的是遍历分类树的所有叶节点，找到概率最大的label。</p><p>2. Word2Vec的输入是上下文窗口内的词，而fastText 对应的整个文本，包括周边词和 N-Gram的内容。</p><p>模型： 继续使用上面处理好的test和train数据，训练fastText 的分类模型。</p><p><img data-ratio="0.875" data-src="https://mmbiz.qpic.cn/mmbiz_png/oppRknZzPcvD7NHjr5Qccs0VapVKvdCWrqLQ4QLV94ZaAbDFJqlOQtAvSVzPlKUGafMa4XHz5fNu6RP7mO18LQ/640?wx_fmt=png" data-w="768" data-backh="489" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667625725.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><figure>或直接使用命令行执行, 词向量结果将保存在文件model.vec中。</figure><p><img data-backw="558" data-copyright="0" data-ratio="0.09331651954602774" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/oppRknZzPcvD7NHjr5Qccs0VapVKvdCWolRzfwusEDibib6n77Vic0HZ8BG5q5h2gvjCNpBDRRs6YRcHwsn8lZ4HQ/640?wx_fmt=jpeg" data-w="793" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/1541667644823.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><figure>对于数据训练样本不充足时，最好使用别人训练好的词向量，但要注意得使用相同内容领域的词向量, 另外要调整dim 参数，使其与 pretrainedVectors 具有相同维数。</figure><figure><img src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/8.JPG" style="width: 700%;" class="fr-fic fr-dib medium-zoom-image"></figure><h2><strong>3. 小结（Brief Summary）</strong></h2><p>文本特征的向量表示是NLP的基础，也是直接影响模型效果的重要因素。离散的表示结合传统的机器学习模型已经有了较好的效果，但存在缺少上下文信息、数据稀疏等问题。分布式的表达方式不仅能够使用到上下文信息进行表征、建立词与词之间的联系，而且在具体任务中也能很好地利用神经网络进行传播。</p><h2><strong>4. 参考资料（Reference Material）</strong></h2><p><strong>&gt;&gt;&nbsp;</strong>A Neural Probabilistic Language Model(Bengio et al., 2003)</p><p><strong>&gt;&gt;&nbsp;</strong>word2vec(Mikolov et al. 2013)</p><p><strong>&gt;&gt;&nbsp;</strong>Improving Word Representations via Global Context and Multiple Word Prototypes(Eric H. Huang et al. 2012)</p><p><strong>&gt;&gt;&nbsp;</strong>Bag of Tricks for Efficient Text Classification (A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, 2016)</p><p><strong>&gt;&gt;&nbsp;</strong>A Primer on Neural Network Models for Natural Language Processing（Yoav Goldberg ， 2015）</p><p><strong>&gt;&gt;&nbsp;</strong>斯坦福大学“深度学习与自然语言处理”课程：CS224d: Deep Learning for Natural Language Processing，word vector部分的slides</p><h2><strong>5. 作者介绍（About The Author;）</strong></h2><p>章洁，机器学习算法工程师，参与自然语言处理，游戏AI 算法等相关项目。</p></div><div class="u-relative article-column"><i class="iconfont icon-iconmulu article-column__will t-top t-top-left"></i><i class="iconfont icon-iconmulu article-column__will t-top t-top-right"></i><i class="iconfont icon-iconmulu article-column__will t-bottom t-bottom-right"></i><i class="iconfont icon-iconmulu article-column__will t-bottom t-bottom-left"></i><div class="article-column__left"><div class="u-flex-center"><a alt="云脑科技" class="u-image-base article-column__avatar" href="https://www.jiqizhixin.com/columns/cloudbrain" target="_blank"><img alt="云脑科技" class="u-image-center" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/WechatIMG223.jpeg"></a><a alt="云脑科技" class="u-text-limit--one article-column__name" href="https://www.jiqizhixin.com/columns/cloudbrain" target="_blank">云脑科技</a></div><p class="article-column__bio">云脑科技是一家跨越中美两地的人工智能行业平台公司，在深度学习（RNN/CNN）、增强学习、NLP、知识图谱领域均拥有大规模项目成功实践经验。本专栏将持续输出云脑员工的原创技术解读，旨在与AI从业者共同探讨、进步。</p></div></div><div class="u-flex article__other js-article-other"><div class="article__other--tags"><a alt="practice" class="u-btn--gray category__link article__other--tag" href="https://www.jiqizhixin.com/categories/practice">工程</a><span class="u-btn--gray category__link article__other--tag">云脑科技</span><span class="u-btn--gray category__link article__other--tag">自然语言处理</span><span class="u-btn--gray category__link article__other--tag">人工智能</span><span class="u-btn--gray category__link article__other--tag">文本分类</span><span class="u-btn--gray category__link article__other--tag">SVM模型</span><span class="u-btn--gray category__link article__other--tag">机器学习</span></div><div class="article-action js-article-action"><a alt="收藏文章" class="u-like-icon article-action__item js-like-action" data-path="/articles/2018-11-07-15" href="javascript:;"><i class="iconfont icon-like-default"></i><span>2</span></a><a class="u-like-icon article-action__item js-go-comment js-switch-comment" href="javascript:;"><i class="iconfont icon-message-default"></i><span></span></a><a alt="分享" class="article-action__item u-like-icon u-share__box" href="javascript:;"><i class="iconfont icon-share-default"></i><div class="u-share__list t-vertical"><button alt="分享到微信" class="u-btn--circle u-share__item u-share__webtn" href="javascript:;"><i class="iconfont icon-wechat"></i><div class="u-share__wechat js-qrcode"><canvas width="120" height="120"></canvas></div></button><button alt="分享到微博" class="u-btn--circle u-share__item js-share-btn" data-title="从离散到分布，盘点常见的文本表示方法" data-type="weibo" href="javascript:;"><i class="iconfont icon-weibo"></i></button><button alt="分享到推特" class="u-btn--circle u-share__item js-share-btn" data-title="从离散到分布，盘点常见的文本表示方法" data-type="twitter" href="javascript:;"><i class="iconfont icon-twitter"></i></button></div></a></div></div></div><div class="u-col-8"><div class="article__hr"></div><div class="article-graph__container"><div class="article-similar__tip">相关数据</div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/72b0bcc0-d8f9-4edd-919f-fa7c2560388c" target="_blank">神经网络</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Neural Network</div></div><p class="graph__content">（人工）神经网络是一种起源于 20 世纪 50 年代的监督式机器学习模型，那时候研究者构想了「感知器（perceptron）」的想法。这一领域的研究者通常被称为「联结主义者（Connectionist）」，因为这种模型模拟了人脑的功能。神经网络模型通常是通过反向传播算法应用梯度下降训练的。目前神经网络有两大主要类型，它们都是前馈神经网络：卷积神经网络（CNN）和循环神经网络（RNN），其中 RNN 又包含长短期记忆（LSTM）、门控循环单元（GRU）等等。深度学习是一种主要应用于神经网络帮助其取得更好结果的技术。尽管神经网络主要用于监督学习，但也有一些为无监督学习设计的变体，比如自动编码器和生成对抗网络（GAN）。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5" target="_blank">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/87c62b00-48b2-4e2a-8122-9876a3d3e59e" target="_blank">词袋模型</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Bag of words</div></div><p class="graph__content">词袋模型（英语：Bag-of-words model）是个在自然语言处理和信息检索(IR)下被简化的表达模型。此模型下，像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式不考虑文法以及词的顺序。最近词袋模型也被应用在电脑视觉领域。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/aaeb1c15-66f2-4822-91bb-b441607f9ecf" target="_blank">分类问题</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Classification</div></div><p class="graph__content">分类问题是数据挖掘处理的一个重要组成部分，在机器学习领域，分类问题通常被认为属于监督式学习(supervised learning)，也就是说，分类问题的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类。根据类别的数量还可以进一步将分类问题划分为二元分类(binary classification)和多元分类(multiclass classification)。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Statistical_classification" target="_blank">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">fastText</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">fastText</div></div><p class="graph__content">Facebook开发的文本处理工具，是一个用于高效学习单词表示和句子分类的库。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://github.com/facebookresearch/fastText" target="_blank">Facebook</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">语料库</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Corpora</div></div><p class="graph__content">语料库一词在语言学上意指大量的文本，通常经过整理，具有既定格式与标记；事实上，语料库英文 "text corpus" 的涵意即为"body of text"。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/zh-cn/%E8%AF%AD%E6%96%99%E5%BA%93" target="_blank">维基百科</a></div></div></div><div class="u-display-none" id="js-hidden-nodes"><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/bf54ab8d-3458-4d37-89be-a3febc0f8d9d" target="_blank">降维</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Dimensionality reduction</div></div><p class="graph__content">降维算法是将 p+1 个系数的问题简化为 M+1 个系数的问题，其中 M&lt;p。算法执行包括计算变量的 M 个不同线性组合或投射（projection）。然后这 M 个投射作为预测器通过最小二乘法拟合一个线性回归模型。两个主要的方法是主成分回归（principal component regression）和偏最小二乘法（partial least squares）。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-08-31-2" target="_blank">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">GloVe</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">GloVe</div></div><p class="graph__content">Stanford开发的用于词向量表示的一个库/工具</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://nlp.stanford.edu/projects/glove/" target="_blank">Stanford NLP</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/1a0e9c5e-6502-4cd7-8683-6b5ca6c48be2" target="_blank">机器学习</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Machine Learning</div></div><p class="graph__content">机器学习是人工智能的一个分支，是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2018-11-07-15" target="_blank">Mitchell, T. (1997). Machine Learning. McGraw Hill.</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/fee178e8-ee20-42fc-8f8f-1d41c1f34e8f" target="_blank">语言模型</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Language models</div></div><p class="graph__content">语言模型经常使用在许多自然语言处理方面的应用，如语音识别，机器翻译，词性标注，句法分析和资讯检索。由于字词与句子都是任意组合的长度，因此在训练过的语言模型中会出现未曾出现的字串(资料稀疏的问题)，也使得在语料库中估算字串的机率变得很困难，这也是要使用近似的平滑n元语法(N-gram)模型之原因。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/c8ff5114-6cbb-49ca-8a89-3ee2826be0b4" target="_blank">自然语言处理</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Natural language processing</div></div><p class="graph__content">自然语言处理（英语：natural language processing，缩写作 NLP）是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。自然语言生成系统把计算机数据转化为自然语言。自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/7237ee05-f07e-4fd9-a9ac-96e5bc1f50e9" target="_blank">机器翻译</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Machine translation</div></div><p class="graph__content">机器翻译（MT）是利用机器的力量「自动将一种自然语言（源语言）的文本翻译成另一种语言（目标语言）」。机器翻译方法通常可分成三大类：基于规则的机器翻译（RBMT）、统计机器翻译（SMT）和神经机器翻译（NMT）。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5" target="_blank">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">概率模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">probabilistic models</div></div><p class="graph__content">概率模型（Statistical Model，也稱為Probabilistic Model）是用来描述不同随机变量之间关系的数学模型，通常情况下刻画了一个或多个随机变量之间的相互非确定性的概率关系。 从数学上讲，该模型通常被表达为 ，其中 是观测集合用来描述可能的观测结果， 是 对应的概率分布函数集合。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">目标函数</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Objective function</div></div><p class="graph__content">目标函数f(x)就是用设计变量来表示的所追求的目标形式，所以目标函数就是设计变量的函数，是一个标量。从工程意义讲，目标函数是系统的性能标准，比如，一个结构的最轻重量、最低造价、最合理形式；一件产品的最短生产时间、最小能量消耗；一个实验的最佳配方等等，建立目标函数的过程就是寻找设计变量与目标的关系的过程，目标函数和设计变量的关系可用曲线、曲面或超曲面表示。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://baike.baidu.com/item/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0" target="_blank">百度百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">参数</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">parameter</div></div><p class="graph__content">在数学和统计学裡，参数（英语：parameter）是使用通用变量来建立函数和变量之间关系（当这种关系很难用方程来阐述时）的一个数量。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E5%8F%82%E6%95%B0" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/56352968-c914-412f-bafd-77951006e0c8" target="_blank">奇异值分解</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Singular Value Decomposition</div></div><p class="graph__content">类似于特征分解将矩阵分解成特征向量和特征值，奇异值分解（singular value decomposition, SVD）将矩阵分解为奇异向量（singular vector）和奇异值（singular value）。通过分解矩阵，我们可以发现矩阵表示成数组元素时不明显的函数性质。而相比较特征分解，奇异值分解有着更为广泛的应用，这是因为每个实数矩阵都有一个奇异值分解，但未必都有特征分解。例如，非方阵型矩阵没有特征分解，这时只能使用奇异值分解。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://web.stanford.edu/~hastie/ElemStatLearn/" target="_blank">Trevor Hastie, Robert Tibshirani and Jerome Friedman (2nd ed., 2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction</a><a class="graph__origin" href="http://www.deeplearningbook.org/" target="_blank">Deep Learning Book</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/3fe4290f-6dd3-43ed-9324-cf23aa588830" target="_blank">文本分类</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">text classification</div></div><p class="graph__content">该技术可被用于理解、组织和分类结构化或非结构化文本文档。文本挖掘所使用的模型有词袋（BOW）模型、语言模型（ngram）和主题模型。隐马尔可夫模型通常用于词性标注（POS）。其涵盖的主要任务有句法分析、情绪分析和垃圾信息检测。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5" target="_blank">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">时间复杂度</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">time complexity</div></div><p class="graph__content">在计算机科学中，算法的时间复杂度是一个函数，它定量描述了该算法的运行时间。这是一个代表算法输入值的字符串的长度的函数。时间复杂度常用大O符号表述，不包括这个函数的低阶项和首项系数。使用这种方式时，时间复杂度可被称为是渐近的，亦即考察输入值大小趋近无穷时的情况。例如，如果一个算法对于任何大小为 n （必须比 n0 大）的输入，它至多需要 5n3 + 3n 的时间运行完毕，那么它的渐近时间复杂度是 O(n3)。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">反比文档频数权重评价方法</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">TF-IDF</div></div><p class="graph__content">tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">word2vec</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">word2vec</div></div><p class="graph__content">Word2vec，为一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。网络以词表现，并且需猜测相邻位置的输入词，在word2vec中词袋模型假设下，词的顺序是不重要的。
训练完成之后，word2vec模型可用来映射每个词到一个向量，可用来表示词对词之间的关系。该向量为神经网络之隐藏层。
Word2vec依赖skip-grams或连续词袋（CBOW）来建立神经词嵌入。Word2vec为托马斯·米科洛夫（Tomas Mikolov）在Google带领的研究团队创造。该算法渐渐被其他人所分析和解释。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/Word2vec" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">词嵌入</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Word embedding</div></div><p class="graph__content">词嵌入是自然语言处理（NLP）中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%AF%8D%E5%B5%8C%E5%85%A5" target="_blank">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">权重</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Weight</div></div><p class="graph__content">线性模型中特征的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/Wikipedia" target="_blank">Google AI Glossary</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><a class="article-graph__keyword" href="https://www.jiqizhixin.com/technologies/01946acc-d031-4c0e-909c-f062643b7273" target="_blank">深度学习</a><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Deep learning</div></div><p class="graph__content">深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。
深度学习是机器学习中一种基于对数据进行表征学习的算法，至今已有数种深度学习框架，如卷积神经网络和深度置信网络和递归神经网络等已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。
</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.nature.com/articles/nature14539" target="_blank">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. nature, 521(7553), 436.</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">奇异值分解（SVD）</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Singular Value Decomposition (SVD)</div></div><p class="graph__content">奇异值分解（Singular Value Decomposition）是特征分解在任意矩阵上的推广。它有着很明显的现实意义，可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述了矩阵的重要特性。例如描述一个人，给别人描述说这个人长得细眉凤眼、圆脸、络腮胡，而且带个近视眼镜，如此简单的特征描述就让别人脑海里面有一个较为清楚的印象。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://baike.baidu.com/item/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/4968432?fr=aladdin" target="_blank">百度百科</a></div></div></div></div><button class="u-flex u-reset-button article-similar__expand" id="js-switch-nodes"><span class="u-margin-right" id="js-switch-text">展开全部数据</span><i class="iconfont icon-arrowdown article-similar__icon"></i></button></div><div class="article__hr"></div><div class="article-similar"><div class="article-similar__tip">推荐文章</div><div class="u-flex"><article class="article-similar__inline"><a alt="词嵌入系列博客Part1：基于语言建模的词嵌入模型" class="u-image-base article-similar__cover" href="https://www.jiqizhixin.com/articles/2016-10-23-2" rel="noopener noreferrer" target="_blank"><img alt="词嵌入系列博客Part1：基于语言建模的词嵌入模型" class="u-image-center" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/580c3e8410d6f.JPG"></a><a alt="词嵌入系列博客Part1：基于语言建模的词嵌入模型" class="u-text-limit--two article-similar__title" href="https://www.jiqizhixin.com/articles/2016-10-23-2" rel="noopener noreferrer" target="_blank">词嵌入系列博客Part1：基于语言建模的词嵌入模型</a><footer class="article-similar__footer u-flex"><a alt="机器之心" class="u-flex article-similar__user" href="https://www.jiqizhixin.com/users/7f316f0c-8f72-4231-bb30-0eb1dd5a5660" rel="noopener noreferrer" target="_blank"><span class="u-avatar-base article-simple__avatar"><img alt="机器之心" class="u-image-center" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/58d0f028b9fe3.png"></span><span class="u-margin-left">机器之心</span></a><a alt="点赞" class="u-like-icon js-like-action" data-path="/articles/2016-10-23-2" href="javascript:;"><i class="iconfont icon-like-default"></i><span></span></a></footer></article><article class="article-similar__inline"><a alt="基于长短期记忆循环神经网络的对话文本主题分割" class="u-image-base article-similar__cover" href="https://www.jiqizhixin.com/articles/2017-04-26" rel="noopener noreferrer" target="_blank"><img alt="基于长短期记忆循环神经网络的对话文本主题分割" class="u-image-center" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/05.jpeg"></a><a alt="基于长短期记忆循环神经网络的对话文本主题分割" class="u-text-limit--two article-similar__title" href="https://www.jiqizhixin.com/articles/2017-04-26" rel="noopener noreferrer" target="_blank">基于长短期记忆循环神经网络的对话文本主题分割</a><footer class="article-similar__footer u-flex"><a alt="哈工大SCIR" class="u-flex article-similar__user" href="https://www.jiqizhixin.com/users/292961ca-69f3-4862-a754-896a8407fd97" rel="noopener noreferrer" target="_blank"><span class="u-avatar-base article-simple__avatar"><img alt="哈工大SCIR" class="u-image-center" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/avatar-1659ffc1-a6ae-425c-aaa4-764de8d246d3.png"></span><span class="u-margin-left">哈工大SCIR</span></a><a alt="点赞" class="u-like-icon js-like-action" data-path="/articles/2017-04-26" href="javascript:;"><i class="iconfont icon-like-default"></i><span></span></a></footer></article><article class="article-similar__inline"><a alt="就喜欢看综述论文：情感分析中的深度学习" class="u-image-base article-similar__cover" href="https://www.jiqizhixin.com/articles/Deep-Learning-for-Sentiment-Analysis" rel="noopener noreferrer" target="_blank"><img alt="就喜欢看综述论文：情感分析中的深度学习" class="u-image-center" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/top.jpg"></a><a alt="就喜欢看综述论文：情感分析中的深度学习" class="u-text-limit--two article-similar__title" href="https://www.jiqizhixin.com/articles/Deep-Learning-for-Sentiment-Analysis" rel="noopener noreferrer" target="_blank">就喜欢看综述论文：情感分析中的深度学习</a><footer class="article-similar__footer u-flex"><a alt="机器之心" class="u-flex article-similar__user" href="https://www.jiqizhixin.com/users/7f316f0c-8f72-4231-bb30-0eb1dd5a5660" rel="noopener noreferrer" target="_blank"><span class="u-avatar-base article-simple__avatar"><img alt="机器之心" class="u-image-center" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/58d0f028b9fe3.png"></span><span class="u-margin-left">机器之心</span></a><a alt="点赞" class="u-like-icon js-like-action" data-path="/articles/Deep-Learning-for-Sentiment-Analysis" href="javascript:;"><i class="iconfont icon-like-default"></i><span>4</span></a></footer></article></div></div></div></div><div class="article__comment"><div class="u-container"><div class="u-col-8" id="comment-container"><div data-react-class="shared/Comment" data-react-props="{&quot;currentUser&quot;:null,&quot;url&quot;:&quot;/articles/2018-11-07-15&quot;}"><div class="comment " id="comment"><div class="comment-inline comment-editor js-comment-area"><a class="is-disabled js-no-user-comment" href="javascript:;"><b>登录</b>后评论</a></div><div class="comment__container"><div class="comment-none__container"><img class="comment-none__cover" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/comment_none-80cb4ed688611aa4f23190a8338f8bab.png" alt="暂无评论"><div class="comment-none__title">暂无评论~</div></div></div></div></div></div></div></div></div><div data-user="null" id="js-login-functionality"></div><div class="u-relative"><div class="u-fix-right--list" id="js-fix-right-list"><div class="u-fix-right--btn backtop is-show" id="js-backtop"><a class="backtop__link" href="javascript:;"><i class="iconfont icon-xiangshangjiantou backtop__icon"></i></a><a class="backtop__link is-second">返回顶部</a></div></div></div><footer class="footer u-flex" id="footer"><div class="u-flex footer__wrapper"><div class="footer__left"><div class="footer__logo"><img height="50" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/logo-c617614d41c836153141ce68ff2b8be19e15cd9c16b2ef1936bc4ad734397392.png"></div><div class="footer-link"><a alt="关于我们" class="footer-link__item" href="https://www.jiqizhixin.com/about" target="_blank">关于我们</a><a alt="寻求报道" class="footer-link__item" href="https://www.jiqizhixin.com/report" target="_blank">寻求报道</a><a alt="商务合作" class="footer-link__item" href="https://www.jiqizhixin.com/business" target="_blank">商务合作</a><a alt="加入我们" class="footer-link__item" href="https://www.jiqizhixin.com/join" target="_blank">加入我们</a><a alt="服务条款" class="footer-link__item" href="https://www.jiqizhixin.com/terms" target="_blank">服务条款</a></div><p class="footer__other">©2018 机器之心（北京）科技有限公司</p><p class="footer__other">京 ICP 备 12027496</p></div><div class="footer__middle"><div class="footer__title">全球人工智能信息服务</div><h5 class="footer__sub-title">友情链接</h5><div class="footer-link"><a alt="Synced Global" class="footer-link__item" href="https://syncedreview.com/" target="_blank">Synced Global</a><a alt="机器之心 Medium 博客" class="footer-link__item" href="https://medium.com/@Synced" target="_blank">机器之心 Medium 博客</a><a alt="PaperWeekly" class="footer-link__item" href="http://paperweek.ly/" target="_blank">PaperWeekly</a><a alt="网易智能" class="footer-link__item" href="http://tech.163.com/smart" target="_blank">网易智能</a><a alt="动脉网" class="footer-link__item" href="http://www.vcbeat.net/" target="_blank">动脉网</a><a alt="硬蛋网" class="footer-link__item" href="http://www.ingdan.com/" target="_blank">硬蛋网</a><a alt="达观数据" class="footer-link__item" href="http://www.datagrand.com/" target="_blank">达观数据</a><a alt="品途商业评论" class="footer-link__item" href="https://www.pintu360.com/" target="_blank">品途商业评论</a></div></div></div><div class="footer__right"><div class="footer__social"><div class="footer__tooltip"><a class="iconfont icon-wechat" href="javascript:;"></a><div class="footer__tooltip-box"><img alt="机器之心微信公众平台" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/weixinQR-2e2eb9e01d350c1ce6bd7cfe4cf8aa2209a1d72b50f1a6147a371e85548fcdf4.jpg"></div></div><a class="iconfont icon-weibo" href="http://weibo.com/synced" rel="noopener noreferrer" target="_blank"></a><a class="iconfont icon-rss" href="https://jiqizhixin.com/rss" rel="noopener noreferrer" target="_blank"></a></div><p>联系电话：+86 010-57150141</p><p>联系邮箱：contact@jiqizhixin.com</p></div></footer><div data-react-class="utils/Modal" data-react-props="{}"><div class="modal-layer"><span></span><span></span></div></div><div data-react-class="utils/Alert" data-react-props="{}"><div class="s-alert-wrapper"></div></div><div class="notice__container" id="js-notice-container"></div><script async="" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/analytics.js"></script><script src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/plupload.full.min.js"></script><script data="208" defsi="361" id="ParadigmSDK" src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/ParadigmSDK_v2_define_itemid.js"></script><script>ParadigmSDK.init("0c2abb5a135747ca9d0f5103e5e95cc3");
ParadigmSDK.trackDetailPageShow("0dcc35b8-d277-4f14-85fa-5ee8376d017d")</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-96130205-2', 'auto');
ga('send', 'pageview');</script><script src="./从离散到分布，盘点常见的文本表示方法 _ 机器之心_files/application-1e0cfa1d1e0ca379ac65.js"></script></body></html>