<!DOCTYPE html>
<!-- saved from url=(0048)https://www.jiqizhixin.com/articles/2018-07-25-5 -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no" name="viewport"><meta content="ie=edge" http-equiv="X-UA-Compatible"><meta content="telephone=no" name="format-detection"><title>NLP概述和文本自动分类算法详解 | 机器之心</title>
<meta name="description" content="本文根据达观数据联合创始人张健的直播内容《NLP 概述及文本自动分类算法详解》整理而成">
<meta name="keywords" content="自然语言处理, 文本分类, 语义理解">
<meta property="og:url" content="https://www.jiqizhixin.com/articles/2018-07-25-5">
<meta property="og:title" content="NLP概述和文本自动分类算法详解">
<meta property="og:type" content="website">
<meta property="og:site_name" content="机器之心">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@SyncedTech">
<meta name="twitter:title" content="NLP概述和文本自动分类算法详解">
<meta name="twitter:description" content="本文根据达观数据联合创始人张健的直播内容《NLP 概述及文本自动分类算法详解》整理而成">
<meta name="twitter:image" content="https://image.jiqizhixin.com/uploads/article/cover_image/7f603528-b18f-4f5d-8b1f-6493422b2361/Untitled-design--2-.jpg"><meta name="csrf-param" content="authenticity_token">
<meta name="csrf-token" content="luoi37U9FcytBbm81ev88RqPBD9MooVUCe+z0og9i2ZgxUy413QZ4i/9Ge10S0JzLspXGNeumyorROoaU7pyWg=="><link rel="stylesheet" media="all" href="./NLP概述和文本自动分类算法详解 _ 机器之心_files/application-5b1eb5d9894f65143e5dc2f37fca9892.css"><style type="text/css">.medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--open .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s}.medium-zoom-image--open{position:relative;z-index:1;cursor:pointer;cursor:zoom-out;will-change:transform}</style></head><body data-controller="articles" id="articles-show"><svg aria-hidden="true" style="position: absolute; width: 0px; height: 0px; overflow: hidden;"><symbol id="coloricon-pdf-" viewBox="0 0 1024 1024"><path d="M897.5 267.2v622.7c0 38.4-31.1 69.5-69.5 69.5H196c-38.4 0-69.5-31.1-69.5-69.5V133.7c0-38.4 31.1-69.5 69.5-69.5h501.5v147.4c0 30.7 24.9 55.6 55.6 55.6h144.4z" fill="#FF3F24"></path><path d="M290.1 357.6h437.6v437.6H290.1z" fill="#FFFFFF"></path><path d="M697.9 66.9v154.7c0 15.2 5.9 29.8 16.6 40.6l183 187.2V268.8L697.9 66.9z" fill="#D12003"></path><path d="M697.5 64.5v164c0 21.4 17.3 38.7 38.7 38.7h161.6L697.5 64.5z" fill="#FFC9C0"></path><path d="M622.9 631.1c-11.1-8-30.2-12.3-56.7-12.8-15.4-19.1-30.1-41.7-40.3-61.7 7.4-13.3 11.9-23.1 13.5-29.2 7.2-28.2 1.7-48.7-5.9-58.4-4.4-5.6-10-8.7-15.9-8.7-7.2 0-24 4.6-25.6 47.2-0.5 12.5 3.9 28.8 13 48.4-14.2 24.3-33.5 53-50.6 74.9-14.9 3.2-29.1 6.8-41.3 10.5-32.8 10.1-35.7 24.8-34.5 32.7 1.6 10.8 12.4 18.3 26.3 18.3 6.2 0 12.6-1.6 18.6-4.5 10.2-5.1 24.1-18.6 41.3-40.2 32.9-6.6 66.3-10.5 92.7-10.8 7.4 8.7 17.1 19.2 26 25.9 14.1 10.6 26.3 15.9 36.2 15.9 9.3 0 16.4-4.8 19-12.8 3.5-11.2-3-25.5-15.8-34.7zM415 671.3c-3.3 1.7-6.9 2.6-10.2 2.6-4.8 0-7.6-1.9-8-2.4 0.2-1.3 4.3-7.1 21.6-12.4 5-1.5 10.3-3 15.9-4.5-10.2 11-16.2 15.1-19.3 16.7z m95.4-163c0.8-21.8 6.2-28.8 6.8-29.4 3.4 1.3 11.1 17 4.2 44-0.3 1.2-1.4 4.7-5.1 12.3-5.1-13.3-6.1-21.8-5.9-26.9z m32.1 110.4c-18.5 1-39.3 3.3-60.4 6.8l0.5-0.6-0.8 0.2c11.4-15.7 22.9-33.1 32.9-49.3l0.2 0.3 0.3-0.5c7.9 14.1 17.5 28.8 27.7 42.5h-0.9l0.5 0.6z m78.7 41.2c-0.1 0.1-0.6 0.2-1.6 0.2-2.6 0-10.3-1.2-25.1-12.2-3.2-2.4-7-5.8-11.1-10 17.1 1.9 25.1 5.7 28.7 8.3 8 5.7 9.4 12.6 9.1 13.7z" fill="#FF3F24"></path></symbol><symbol id="coloricon-translation-line1" viewBox="0 0 1024 1024"><path d="M735.744 560.64H665.6l-99.84 275.968h56.832l20.992-58.368H752.64l20.992 58.368h59.392l-97.28-275.968zM658.432 732.16l40.448-117.76h1.536l37.376 117.76h-79.36z" fill="#EB2835"></path><path d="M325.12 126.976H269.312v73.216H146.944v174.592h50.688v-27.136h73.216V476.16h53.76V347.648H399.36v22.528h55.296V200.192H325.12V126.976zM270.336 307.2H197.632V241.664h73.216L270.336 307.2z m128.512 0H324.096V241.664h74.752V307.2z" fill="#9D9D9D"></path><path d="M374.784 643.072H95.744C43.008 643.072 0.512 600.576 0.512 547.84V96.768C0.512 44.032 43.008 1.536 95.744 1.536h451.072c52.736 0 95.232 42.496 95.232 95.232V378.88h-51.2V96.768c0-24.064-19.968-44.032-44.032-44.032H95.744c-24.064 0-44.032 19.968-44.032 44.032V547.84c0 24.064 19.968 44.032 44.032 44.032h279.04v51.2zM972.8 267.264h-51.2c0-90.624-73.728-164.352-164.352-164.352v-51.2c118.784 0 215.552 96.768 215.552 215.552zM266.752 972.288C147.968 972.288 51.2 875.52 51.2 756.736h51.2c0 90.624 73.728 164.352 164.352 164.352v51.2z" fill="#9D9D9D"></path><path d="M925.184 1019.392H474.112c-52.736 0-95.232-42.496-95.232-95.232V473.088c0-52.736 42.496-95.232 95.232-95.232h451.072c52.736 0 95.232 42.496 95.232 95.232v451.072c0 52.736-42.496 95.232-95.232 95.232zM474.112 429.056c-24.064 0-44.032 19.968-44.032 44.032v451.072c0 24.064 19.968 44.032 44.032 44.032h451.072c24.064 0 44.032-19.968 44.032-44.032V473.088c0-24.064-19.968-44.032-44.032-44.032H474.112z" fill="#EB2835"></path></symbol><symbol id="coloricon-translation-line" viewBox="0 0 1024 1024"><path d="M933.376 1004.544c38.4 0 69.632-31.232 69.632-69.632V483.84c0-38.4-31.232-69.632-69.632-69.632H482.304c-38.4 0-69.632 31.232-69.632 69.632v451.072c0 38.4 31.232 69.632 69.632 69.632h451.072z" fill="#EB2835"></path><path d="M90.624 609.792h274.944V461.824c0-56.832 46.08-102.912 102.912-102.912h142.848V89.088c0-38.4-31.232-69.632-69.632-69.632H90.624c-38.4 0-69.632 31.232-69.632 69.632v451.072c0 38.4 31.232 69.632 69.632 69.632z" fill="#9D9D9D"></path><path d="M751.104 581.12H680.96l-99.84 275.968h56.832l20.992-58.368H768l20.992 58.368h59.392l-97.28-275.968zM673.792 752.64l40.448-117.76h1.536l37.376 117.76h-79.36zM314.88 114.176H259.072v73.216H136.704v174.592h50.688v-27.136h73.216v128.512h53.76V334.848H389.12v22.528h55.296V187.392H314.88V114.176zM260.096 294.4H187.392V228.864h73.216l-0.512 65.536z m128.512 0H313.856V228.864h74.752v65.536z" fill="#FFFFFF"></path><path d="M972.8 267.264h-51.2c0-90.624-73.728-164.352-164.352-164.352v-51.2c118.784 0 215.552 96.768 215.552 215.552zM266.752 972.288C147.968 972.288 51.2 875.52 51.2 756.736h51.2c0 90.624 73.728 164.352 164.352 164.352v51.2z" fill="#9D9D9D"></path></symbol></svg><div class="article"><div class="header__hr is-progress-bar" id="js-progress-bar" style="width: 49.99%;"></div><div class="header__hr"></div><header class="header has-hidden" id="header"><div class="u-container u-flex"><div class="u-flex header__right"><div class="header-logo"><a alt="首页" href="https://www.jiqizhixin.com/"><img alt="机器之心" class="header-logo__black" height="32" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/logo-black-814ff978059dd2570cc09283d01d1e29ebec941b013fc43d3ae6ce3b3f6c2d69.png"></a></div><div class="header-nav__current js-nav-current"><span>知识</span><i class="iconfont icon-arrowdown u-margin-left header-nav__icon is-first"></i><i class="iconfont icon-iconguanbi u-margin-left header-nav__icon is-second"></i></div></div><div id="js-site-search"><div class="u-in-center header-search"><a class="header-search__btn" href="javascript:;" alt="搜索"><i class="iconfont icon-sreach"></i></a><input type="text" class="header-search__input" placeholder="探索机器之心" value=""></div></div><div class="header__btns js-header-btns"><div class="header__btns"><a class="header-other__link header-user__menu" href="javascript:;">登录</a></div></div></div></header><nav class="header-nav__list js-nav-list"><div class="u-container header-nav__items"><a class="u-borbot__item header-nav__item " href="https://www.jiqizhixin.com/">探索</a><a class="u-borbot__item header-nav__item is-active" href="https://www.jiqizhixin.com/categories/basic">知识</a><a class="u-borbot__item header-nav__item " href="https://www.jiqizhixin.com/categories/industry">产业</a><a alt="AI商用搜索" class="u-borbot__item header-nav__item" href="https://handbook.jiqizhixin.com/" rel="noopener noreferrer" target="_blank">AI商用搜索</a></div></nav><div class="u-min-height-container u-container"><div class="u-col-8"><div class="u-flex article__header"><div class="u-flex article-author"><a alt="达观数据" class="u-avatar-base article-author__avatar" href="https://www.jiqizhixin.com/users/dda7327c-1b8b-4197-9783-fc0319cf6e61" rel="noopener noreferrer" target="_blank"><img alt="达观数据" class="u-image-center" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/586cb2b3c01ec.png"></a><div><p class="article-author__attr"><a alt="达观数据" class="article-author__name" href="https://www.jiqizhixin.com/users/dda7327c-1b8b-4197-9783-fc0319cf6e61" rel="noopener noreferrer" target="_blank">达观数据</a><span class="article__type">转载</span></p><time class="article__published">2018/07/25 12:45</time></div></div><div class="article-des u-flex"><div></div></div></div><h1 class="article__title">NLP概述和文本自动分类算法详解</h1></div><div class="u-col-8 article__inline" id="js-article-inline"><p class="article__summary"></p><div class="article__content" id="js-article-content"><blockquote><p><mark data-type="technologies" data-id="c8ff5114-6cbb-49ca-8a89-3ee2826be0b4" class="tooltipstered">自然语言处理</mark>一直是人工智能领域的重要话题，更是 18 年的热度话题，为了在海量文本中及时准确地获得有效信息，<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>技术获得广泛关注，也给大家带来了更多应用和想象的空间。本文根据达观数据联合创始人张健的直播内容《NLP 概述及文本自动分类算法详解》整理而成。</p></blockquote><p><strong>一、 &nbsp; &nbsp;NLP 概述</strong></p><p><strong>1.<mark data-type="technologies" data-id="e152be19-39f1-460f-bc7d-9a00fcd7c351" class="tooltipstered">文本挖掘</mark>任务类型的划分</strong></p><p><mark data-type="technologies" data-id="e152be19-39f1-460f-bc7d-9a00fcd7c351" class="tooltipstered">文本挖掘</mark>任务大致分为四个类型：类别到序列、序列到类别、同步的（每个输入位置都要产生输出）序列到序列、异步的序列到序列。</p><p>同步的序列到序列的例子包括中文分词，<mark data-type="technologies" data-id="958e7486-932d-4ff3-9a23-e38045c26f4b" class="tooltipstered">命名实体识别</mark>和<mark data-type="technologies" data-id="5b71072d-4494-43eb-8730-302c4a90f45e" class="tooltipstered">词性标注</mark>。一部的序列到序列包括<mark data-type="technologies" data-id="7237ee05-f07e-4fd9-a9ac-96e5bc1f50e9" class="tooltipstered">机器翻译</mark>和自动摘要。序列到类别的例子包括<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>和情感分析。类别（对象）到序列的例子包括文本生成和形象描述。</p><p><strong>2.<mark data-type="technologies" data-id="e152be19-39f1-460f-bc7d-9a00fcd7c351" class="tooltipstered">文本挖掘</mark>系统整体方案</strong></p><p>达观数据一直专注于文本语义，<mark data-type="technologies" data-id="e152be19-39f1-460f-bc7d-9a00fcd7c351" class="tooltipstered">文本挖掘</mark>系统整体方案包含了 NLP 处理的各个环节，从处理的文本粒度上来分，可以分为篇章级应用、短串级应用和词汇级应用。</p><p>篇章级应用有六个方面，已经有成熟的产品支持企业在不同方面的<mark data-type="technologies" data-id="e152be19-39f1-460f-bc7d-9a00fcd7c351" class="tooltipstered">文本挖掘</mark>需求：</p><ul><li><p>垃圾评论：精准识别广告、不文明用语及低质量文本。</p></li><li><p>黄反识别：准确定位文本中所含涉黄、涉政及反动内容。</p></li><li><p>标签提取：提取文本中的核心词语生成标签。</p></li><li><p>文章分类：依据预设分类体系对文本进行自动归类。</p></li><li><p>情感分析：准确分析用户透过文本表达出的情感倾向。</p></li><li><p>文章<mark data-type="technologies" data-id="e49b21d8-935a-4da6-910d-504c79b9785f" class="tooltipstered">主题模型</mark>：抽取出文章的隐含主题。</p></li></ul><p>为了实现这些顶层应用，达观数据掌握从词语短串分析个层面的分析技术，开发了包括中文分词、专名识别、语义分析和词串分析等模块。</p><p><img data-s="300,640" data-type="png" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/1532493680604.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"><em>达观数据<mark data-type="technologies" data-id="e152be19-39f1-460f-bc7d-9a00fcd7c351" class="tooltipstered">文本挖掘</mark>架构图</em></p><p><strong>3.序列标注应用：中文分词</strong></p><p>同步的序列到序列，其实就是序列标注问题，应该说是<mark data-type="technologies" data-id="c8ff5114-6cbb-49ca-8a89-3ee2826be0b4" class="tooltipstered">自然语言处理</mark>中最常见的问题。序列标注的应用包括中文分词、<mark data-type="technologies" data-id="958e7486-932d-4ff3-9a23-e38045c26f4b" class="tooltipstered">命名实体识别</mark>和<mark data-type="technologies" data-id="5b71072d-4494-43eb-8730-302c4a90f45e" class="tooltipstered">词性标注</mark>等。序列标注问题的输入是一个观测序列，输出的是一个标记序列或状态序列。</p><p><img data-s="300,640" data-type="png" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/1532493681575.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><p>举中文分词为例，处理「结合成分子」的观测序列，输出「结合/成/分子」的分词标记序列。针对中文分词的这个应用，有多种处理方法，包括基于词典的方法、<mark data-type="technologies" data-id="bbe58ec7-a3a6-4415-b416-76adb7c15434" class="tooltipstered">隐马尔可夫模型</mark>（HMM）、<mark data-type="technologies" data-id="142b4991-622c-4f24-b7ee-f442c106eacb" class="tooltipstered">最大熵模型</mark>、<mark data-type="technologies" data-id="b7aae8a7-9df4-4619-9281-5c11f2d256b1" class="tooltipstered">条件随机场</mark>（CRF）、<mark data-type="technologies" data-id="01946acc-d031-4c0e-909c-f062643b7273" class="tooltipstered">深度学习</mark>模型（双向 LSTM 等）和一些无<mark data-type="technologies" data-id="94fdbfed-9ebb-491b-b54e-9c2aae512f70" class="tooltipstered">监督学习</mark>的方法（基于凝聚度与自由度）。</p><p><strong>4.序列标注应用：NER</strong></p><p><mark data-type="technologies" data-id="958e7486-932d-4ff3-9a23-e38045c26f4b" class="tooltipstered">命名实体识别</mark>：Named Entity Recognition，简称 NER，又称作「专名识别」，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。通常包括实体边界识别和确定实体类别。</p><p><img data-s="300,640" data-type="png" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/1532493695304.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><p>对与<mark data-type="technologies" data-id="958e7486-932d-4ff3-9a23-e38045c26f4b" class="tooltipstered">命名实体识别</mark>，采取不同的标记方式，常见的标签方式包括 IO、BIO、BMEWO 和 BMEWO+。其中一些标签含义是：</p><ul><li><p>B：begin</p></li><li><p>I：一个词的后续成分</p></li><li><p>M：中间</p></li><li><p>E：结束</p></li><li><p>W：单个词作为实体</p></li></ul><p>大部分情况下，标签体系越复杂准确度也越高，但相应的训练时间也会增加。因此需要根据实际情况选择合适的标签体系。通常我们实际应用过程中，最难解决的还是标注问题。所以在做<mark data-type="technologies" data-id="958e7486-932d-4ff3-9a23-e38045c26f4b" class="tooltipstered">命名实体识别</mark>时，要考虑人工成本问题。</p><p><strong>5.英文处理</strong></p><p>在 NLP 领域，中文和英文的处理在大的方面都是相通的，不过在细节方面会有所差别。其中一个方面，就是中文需要解决分词的问题，而英文天然的就没有这个烦恼；另外一个方面，英文处理会面临词形还原和词根提取的问题，英文中会有时态变换（made==&gt;make），单复数变换（cats==&gt;cat），词根提取（arabic==&gt;arab）。</p><p>在处理上面的问题过程中，不得不提到的一个工具是 WordNet。WordNet 是一个由普林斯顿大学认识科学实验室在心理学教授乔治•A•米勒的指导下建立和维护的英语字典。在 WordNet 中，名词、动词、形容词和副词各自被组织成一个同义词的网络，每个同义词集合都代表一个基本的语义概念，并且这些集合之间也由各种关系连接。我们可以通过 WordNet 来获取同义词和上位词。</p><p><strong>6.<mark data-type="technologies" data-id="2feeb7b3-2bea-4238-9c79-0d235ffc71cc" class="tooltipstered">词嵌入</mark></strong></p><p>在处理文本过程中，我们需要将文本转化成数字可表示的方式。词向量要做的事就是将语言数学化表示。词向量有两种实现方式：One-hot 表示，即通过向量中的一维 0/1 值来表示某个词；<mark data-type="technologies" data-id="2feeb7b3-2bea-4238-9c79-0d235ffc71cc" class="tooltipstered">词嵌入</mark>，将词转变为固定维数的向量。</p><p><mark data-type="technologies" data-id="c61ba3b9-40e2-4864-a941-9adc19e6792e" class="tooltipstered">word2vec</mark> 是使用浅层和双层<mark data-type="technologies" data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" class="tooltipstered">神经网络</mark>产生生词向量的模型，产生的<mark data-type="technologies" data-id="2feeb7b3-2bea-4238-9c79-0d235ffc71cc" class="tooltipstered">词嵌入</mark>实际上是<mark data-type="technologies" data-id="fee178e8-ee20-42fc-8f8f-1d41c1f34e8f" class="tooltipstered">语言模型</mark>的一个副产品，网络以词表现，并且需猜测相邻位置的输入词。<mark data-type="technologies" data-id="c61ba3b9-40e2-4864-a941-9adc19e6792e" class="tooltipstered">word2vec</mark> 中词向量的训练方式有两种，cbow（continuous bags of word）和 skip-gram。cbow 和 skip-gram 的区别在于，cbow 是通过输入单词的上下文（周围的词的向量和）来预测中间的单词，而 skip-gram 是输入中间的单词来预测它周围的词。</p><p><strong>7.文档建模</strong></p><p>要使计算机能够高效地处理真实文本，就必须找到一种理想的形式化表示方法，这个过程就是文档建模。文档建模一方面要能够真实地反映文档的内容，另一方面又要对不同文档具有区分能力。文档建模比较通用的方法包括<mark data-type="technologies" data-id="83e47179-05a6-4372-a7e7-f08ee749c326" class="tooltipstered">布尔模型</mark>、<mark data-type="technologies" data-id="6f9a248b-7082-40f2-9e18-e351a57f5944" class="tooltipstered">向量空间模型</mark>（VSM）和<mark data-type="technologies" data-id="bfbadbc0-b9a9-48c1-8e14-983351e0603f" class="tooltipstered">概率模型</mark>。其中最为广泛使用的是<mark data-type="technologies" data-id="6f9a248b-7082-40f2-9e18-e351a57f5944" class="tooltipstered">向量空间模型</mark>。</p><p><strong>二、<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>的关键技术与重要方法</strong></p><p><strong>1.利用<mark data-type="technologies" data-id="1a0e9c5e-6502-4cd7-8683-6b5ca6c48be2" class="tooltipstered">机器学习</mark>进行模型训练</strong></p><p><mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>的流程包括训练、文本语义、文本特征处理、训练模型、模型评估和输出模型等几个主要环节。其中介绍一下一些主要的概念。</p><ul><li><p>文档建模：<mark data-type="technologies" data-id="bfbadbc0-b9a9-48c1-8e14-983351e0603f" class="tooltipstered">概率模型</mark>，<mark data-type="technologies" data-id="83e47179-05a6-4372-a7e7-f08ee749c326" class="tooltipstered">布尔模型</mark>，VSM；</p></li><li><p>文本语义：分词，<mark data-type="technologies" data-id="958e7486-932d-4ff3-9a23-e38045c26f4b" class="tooltipstered">命名实体识别</mark>，<mark data-type="technologies" data-id="5b71072d-4494-43eb-8730-302c4a90f45e" class="tooltipstered">词性标注</mark>等；</p></li><li><p>文本特征处理：特征<mark data-type="technologies" data-id="bf54ab8d-3458-4d37-89be-a3febc0f8d9d" class="tooltipstered">降维</mark>，包括使用评估函数（<mark data-type="technologies" data-id="9333733d-a7ac-407f-8116-87dce8230c3e" class="tooltipstered">TF-IDF</mark>，互信息方法，期望<mark data-type="technologies" data-id="1786086f-5b63-4eee-b9ed-dad4d64cdc86" class="tooltipstered">交叉熵</mark>，QEMI，统计量方法，遗传算法等）；特征向量权值计算；</p></li><li><p>样本分类训练：<mark data-type="technologies" data-id="8d4d7c16-ddb0-496f-89c4-0b22b7029ee0" class="tooltipstered">朴素贝叶斯</mark>分类器，SVM，<mark data-type="technologies" data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" class="tooltipstered">神经网络</mark>算法，决策树，Ensemble 算法等；</p></li><li><p>模型评估：召回率，正确率，F-测度值；</p></li></ul><p><strong>2.<mark data-type="technologies" data-id="6f9a248b-7082-40f2-9e18-e351a57f5944" class="tooltipstered">向量空间模型</mark></strong></p><p><mark data-type="technologies" data-id="6f9a248b-7082-40f2-9e18-e351a57f5944" class="tooltipstered">向量空间模型</mark>是常用来处理<mark data-type="technologies" data-id="e152be19-39f1-460f-bc7d-9a00fcd7c351" class="tooltipstered">文本挖掘</mark>的文档建模方法。VSM 概念非常直观——把对文本内容的处理简化为向量空间中的向量运算，并且它以空间上的相似度表达语义的相似度，直观易懂。</p><p>当文档被表示为文档空间的向量时，就可以通过计算向量之间的相似性来度量文档间的相似性。它的一些实现方式包括：</p><p>1）N-gram 模型：基于一定的语料库，可以利用 N-Gram 来预计或者评估一个句子是否合理；</p><p>2）<mark data-type="technologies" data-id="9333733d-a7ac-407f-8116-87dce8230c3e" class="tooltipstered">TF-IDF</mark> 模型：若某个词在一篇文档中出现频率 TF 高，却在其他文章中很少出现，则认为此词具有很好的类别区分能力；</p><p>3）Paragraph Vector 模型：其实是 word vector 的一种扩展。Gensim 中的 Doc2Vec 以及 Facebook 开源的 Fasttext 工具也是采取了这么一种思路，它们将文本的词向量进行相加/求平均的结果作为 Paragraph Vector。</p><p><strong>3.文本特征提取算法</strong></p><p>目前大多数中文<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>系统都采用词作为特征项，作为特征项的词称作特征词。这些特征词作为文档的中间表示形式，用来实现文档与文档、文档与用户目标之间的相似度计算。如果把所有的词都作为特征项，那么特征向量的维数将过于巨大。有效的特征提取算法，不仅能降低运算复杂度，还能提高分类的效率和精度。</p><p>文本特征提取的算法包含下面三个方面：</p><p>1）从原始特征中挑选出一些最具代表文本信息的特征，例如词频、<mark data-type="technologies" data-id="9333733d-a7ac-407f-8116-87dce8230c3e" class="tooltipstered">TF-IDF</mark> 方法;</p><p>2）基于数学方法找出对分类信息共现比较大的特征，主要例子包括互信息法、<mark data-type="technologies" data-id="ecbad6d7-cde5-46ea-9bc6-960549585514" class="tooltipstered">信息增益</mark>、期望<mark data-type="technologies" data-id="1786086f-5b63-4eee-b9ed-dad4d64cdc86" class="tooltipstered">交叉熵</mark>和统计量方法;</p><p>3）以特征量分析多元统计分布，例如<mark data-type="technologies" data-id="c9520edd-b2a1-4d42-9736-9885456c2280" class="tooltipstered">主成分分析</mark>（PCA）。</p><p><strong>4.文本<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>计算方法</strong></p><p>特征<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>用于衡量某个特征项在文档表示中的重要程度或区分能力的强弱。选择合适的<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>计算方法，对<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>系统的分类效果能有较大的提升作用。</p><p>特征<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>的计算方法包括：</p><p>1）<mark data-type="technologies" data-id="9333733d-a7ac-407f-8116-87dce8230c3e" class="tooltipstered">TF-IDF</mark>；</p><p>2）词性；</p><p>3）标题；</p><p>4）位置；</p><p>5）句法结构；</p><p>6）专业词库；</p><p>7）<mark data-type="technologies" data-id="7afc5fc3-a61c-4238-b496-9c39392a91fe" class="tooltipstered">信息熵</mark>；</p><p>8）文档、词语长度；</p><p>9）词语间关联；</p><p>10）词语直径；</p><p>11）词语分布偏差。</p><p>其中提几点，词语直径是指词语在文本中首次出现的位置和末次出现的位置之间的距离。词语分布偏差所考虑的是词语在文章中的统计分布。在整篇文章中分布均匀的词语通常是重要的词汇。</p><p><strong>5.分类器设计</strong></p><p>由于<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>本身是一个<mark data-type="technologies" data-id="aaeb1c15-66f2-4822-91bb-b441607f9ecf" class="tooltipstered">分类问题</mark>，所以一般的模式分类方法都可以用于<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>应用中。</p><p>常用分类算法的思路包括下面四种：</p><p>1）<mark data-type="technologies" data-id="8d4d7c16-ddb0-496f-89c4-0b22b7029ee0" class="tooltipstered">朴素贝叶斯</mark>分类器：利用特征项和类别的联合概率来估计文本的类别概率；</p><p>2）<mark data-type="technologies" data-id="9cbd3df9-2050-4c7c-8eaa-ca53c512083c" class="tooltipstered">支持向量机</mark>分类器：在向量空间中找到一个决策平面，这个平面能够最好的切割两个分类的数据点，主要用于解决二<mark data-type="technologies" data-id="aaeb1c15-66f2-4822-91bb-b441607f9ecf" class="tooltipstered">分类问题</mark>；</p><p>3）KNN 方法：在训练集中找到离它最近的 k 个临近文本，并根据这些文本的分类来给测试文档分类；</p><p>4）决策树方法：将文本处理过程看作是一个等级分层且分解完成的复杂任务。</p><p><strong>6.分类算法融合</strong></p><p>聚合多个分类器，提高分类<mark data-type="technologies" data-id="8be77eae-12da-4e9e-9a88-b7f5bae98c2e" class="tooltipstered">准确率</mark>称为 Ensemble 方法。</p><p>利用不同分类器的优势，取长补短，最后综合多个分类器的结果。Ensemble 可设定<mark data-type="technologies" data-id="a3ee878c-7013-408b-8e6b-7b1038057f8f" class="tooltipstered">目标函数</mark> (组合多个分类器)，通过训练得到多个分类器的组合<mark data-type="technologies" data-id="2e982b73-88e2-41e8-a430-f7ae5a9af4bf" class="tooltipstered">参数</mark> (并非简单的累加或者多数)。</p><p><img data-s="300,640" data-type="png" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/1532493676235.png" class="fr-fic fr-dib medium-zoom-image" style="width: 64.81%;"></p><p>我们这里提到的 ensemble 可能跟通常说的 ensemble learning 有区别。主要应该是指 stacking。<mark data-type="technologies" data-id="efb128f3-0b7c-45a8-a6ae-065fa93c6556" class="tooltipstered">Stacking</mark> 是指训练一个模型用于组合其他各个模型。即首先我们先训练多个不同的模型，然后再以之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。在处理 ensemble 方法的时候，需要注意几个点。基础模型之间的相关性要尽可能的小，并且它们的性能表现不能差距太大。</p><p>多个模型分类结果如果差别不大，那么叠加效果也不明显；或者如果单个模型的效果距离其他模型比较差，也是会对整体效果拖后腿。</p><p><strong>三、<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>在<mark data-type="technologies" data-id="01946acc-d031-4c0e-909c-f062643b7273" class="tooltipstered">深度学习</mark>中的应用</strong></p><p><strong>1.CNN <mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark></strong></p><p>采取 CNN 方法进行<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>，相比传统方法会在一些方面有优势。</p><p><img data-s="300,640" data-type="png" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/1532493676909.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><p><em>基于<mark data-type="technologies" data-id="87c62b00-48b2-4e2a-8122-9876a3d3e59e" class="tooltipstered">词袋模型</mark>的<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>方法，没有考虑到词的顺序。</em></p><p>基于卷积<mark data-type="technologies" data-id="72b0bcc0-d8f9-4edd-919f-fa7c2560388c" class="tooltipstered">神经网络</mark>（CNN）来做<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>，可以利用到词的顺序包含的信息。如图展示了比较基础的一个用 CNN 进行<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>的网络结构。CNN 模型把原始文本作为输入，不需要太多的人工特征。CNN 模型的一个实现，共分四层：</p><ul><li><p>第一层是词向量层，doc 中的每个词，都将其<mark data-type="technologies" data-id="8ec6a68f-ad96-4b85-ab72-6f8931886922" class="tooltipstered">映射</mark>到词向量空间，假设词向量为 k 维，则 n 个词<mark data-type="technologies" data-id="8ec6a68f-ad96-4b85-ab72-6f8931886922" class="tooltipstered">映射</mark>后，相当于生成一张 n*k 维的图像；</p></li><li><p>第二层是卷积层，多个滤波器作用于词向量层，不同滤波器生成不同的 feature map；</p></li><li><p>第三层是 pooling 层，取每个 feature map 的最大值，这样操作可以处理变长文档，因为第三层输出只依赖于滤波器的个数；</p></li><li><p>第四层是一个全连接的 softmax 层，输出是每个类目的概率，中间一般加个 dropout，防止<mark data-type="technologies" data-id="af836eef-be90-4143-a022-46fae3904f0e" class="tooltipstered">过拟合</mark>。</p></li></ul><p>有关 CNN 的方法一般都围绕这个基础模型进行，再加上不同层的创新。</p><p>比如第一个模型在输入层换成 RNN，去获得文本通过 rnn 处理之后的输出作为卷积层的输入。比如说第二个是在 pooling 层使用了动态 kmax pooling，来解决样本集合文本长度变化较大的问题。比如说第三种是极深网络，在卷积层做多层卷积，以获得长距离的依赖信息。CNN 能够提取不同长度范围的特征，网络的层数越多，意味着能够提取到不同范围的特征越丰富。不过 cnn 层数太多会有梯度弥散、梯度爆炸或者退化等一系列问题。</p><p>为了解决这些问题，极深网络就通过 shortcut 连接。残差网络其实是由多种路径组合的一个网络，残差网络其实是很多并行子网络的组合，有些点评评书残差网络就说它其实相当于一个 Ensembling。</p><p><strong>2.RNN 与 LSTM <mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark></strong></p><p>CNN 有个问题是卷积时候是固定 filter_size，就是无法建模更长的序列信息，虽然这个可以通过多次卷积获得不同范围的特征，不过要付出增加网络深度的代价。</p><p>Rnn 的出现是解决变长序列信息建模的问题，它会将每一步中产生的信息都传递到下一步中。</p><p>首先我们在输入层之上，套上一层双向 LSTM 层，LSTM 是 RNN 的改进模型，相比 RNN，能够更有效地处理句子中单词间的长距离影响；而双向 LSTM 就是在隐层同时有一个正向 LSTM 和反向 LSTM，正向 LSTM 捕获了上文的特征信息，而反向 LSTM 捕获了下文的特征信息，这样相对单向 LSTM 来说能够捕获更多的特征信息，所以通常情况下双向 LSTM 表现比单向 LSTM 或者单向 RNN 要好。</p><p><img data-s="300,640" data-type="png" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/1532493678330.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><p>如何从物理意义上来理解求平均呢？这其实可以理解为在这一层，两个句子中每个单词都对最终分类结果进行投票，因为每个 BLSTM 的输出可以理解为这个输入单词看到了所有上文和所有下文（包含两个句子）后作出的两者是否语义相同的判断，而通过 Mean <mark data-type="technologies" data-id="0a4cedf0-0ee0-4406-946e-2877950da91d" class="tooltipstered">Pooling</mark> 层投出自己宝贵的一票。</p><p><strong>3.Attention Model 与 seq2seq</strong></p><p>注意力模型 Attention Model 是传统自编码器的一个升级版本。传统 RNN 的 Encoder-Decoder 模型，它的缺点是不管无论之前的 context 有多长，包含多少信息量，最终都要被压缩成固定的 vector，而且各个维度维度收到每个输入维度的影响都是一致的。为了解决这个问题，它的 idea 其实是赋予不同位置的 context 不同的<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>，越大的<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>表示对应位置的 context 更加重要。</p><p><img data-s="300,640" data-type="png" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/1532493677815.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><p>现实中，举一个翻译问题：jack ma dances very well 翻译成中文是马云跳舞很好。其中，马云应该是和 jack ma 关联的。</p><p>Attention Model 是当前的研究热点，它广泛地可应用于文本生成、<mark data-type="technologies" data-id="7237ee05-f07e-4fd9-a9ac-96e5bc1f50e9" class="tooltipstered">机器翻译</mark>和<mark data-type="technologies" data-id="fee178e8-ee20-42fc-8f8f-1d41c1f34e8f" class="tooltipstered">语言模型</mark>等方面。</p><p><strong>4.Hierarchical Attention Network</strong></p><p>下面介绍层次化注意力网络。</p><p><img data-s="300,640" data-type="png" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/1532493678750.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><p>词编码层是首先把词转化成词向量，然后用双向的 GRU 层, 可以将正向和反向的上下文信息结合起来，获得隐藏层输出。第二层是 word attention 层。attention 机制的目的是要把一个句子中，对句子的含义最重要，贡献最大的词语找出来。</p><p>为了衡量单词的重要性, 我们用 u_it 和一个随机初始化的上下文向量 u_w 的相似度来表示，然后经过 softmax 操作获得了一个归一化的 attention <mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>矩阵 a_it，代表句子 i 中第 t 个词的<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>。结合词的<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>，句子向量 s_i 看作组成这些句子的词向量的加权求和。</p><p>第三层是句子编码层，也是通过双向 GRU 层, 可以将正向和反向的上下文信息结合起来，获得隐藏层输出。</p><p>到了第四层是句子的注意力层，同词的注意力层差不多，也是提出了一个句子级别的上下文向量 u_s, 来衡量句子在文中的重要性。输出也是结合句子的<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>，全文的向量表示看做是句子向量的加权求和。</p><p>到了最后，有了全文的向量表示，我们就直接通过全连接 softmax 来进行分类。</p><p><strong>四、案例介绍</strong></p><p><strong>1.新闻分类</strong></p><p>新闻分类是最常见的一种分类。其处理方法包括：</p><p>1）定制行业专业语料，定期更新语料<mark data-type="technologies" data-id="6d74712b-ff57-46af-bd46-cd8e9fc29d20" class="tooltipstered">知识库</mark>，构建行业垂直语义模型。</p><p>2）计算 term <mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>，考虑到位置特征，网页特征，以及结合离线统计结果获取到核心的关键词。</p><p>3）使用<mark data-type="technologies" data-id="e49b21d8-935a-4da6-910d-504c79b9785f" class="tooltipstered">主题模型</mark>进行语义扩展</p><p>4）监督与半监督方式的<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark></p><p><img data-s="300,640" data-type="png" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/1532493679231.png" class="fr-fic fr-dib medium-zoom-image" style="width: 700%;"></p><p><strong>2.垃圾广告黄反识别</strong></p><p>垃圾广告过滤作为<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>的一个场景有其特殊之处，那就是它作为一种防攻击手段，会经常面临攻击用户采取许多变换手段来绕过检查。</p><p>处理这些变换手段有多重方法：</p><ul><li><p>一是对变形词进行识别还原，包括要处理间杂特殊符号，同音、简繁变换，和偏旁拆分、形近变换。</p></li><li><p>二是通过<mark data-type="technologies" data-id="fee178e8-ee20-42fc-8f8f-1d41c1f34e8f" class="tooltipstered">语言模型</mark>识别干扰文本，如果识别出文本是段不通顺的「胡言乱语」，那么他很可能是一段用于规避关键字审查的垃圾文本。</p></li><li><p>三是通过计算主题和评论的相关度匹配来鉴别。</p></li><li><p>四是基于多种表达特征的分类器模型识别来提高分类的泛化能力。</p></li></ul><p><strong>3.情感分析</strong></p><p>情感分析的处理办法包括：</p><p>1）基于词典的情感分析，主要是线设置情感词典，然后基于规则匹配（情感词对应的<mark data-type="technologies" data-id="149a12cf-10c2-4555-9899-cc6dee319ef5" class="tooltipstered">权重</mark>进行加权）来识别样本是否是正负面。</p><p>2）基于<mark data-type="technologies" data-id="1a0e9c5e-6502-4cd7-8683-6b5ca6c48be2" class="tooltipstered">机器学习</mark>的情感分析，主要是采取<mark data-type="technologies" data-id="87c62b00-48b2-4e2a-8122-9876a3d3e59e" class="tooltipstered">词袋模型</mark>作为基础特征，并且将复杂的情感处理规则命中的结果作为一维或者多维特征，以一种更为「柔性」的方法融合到情感分析中，扩充我们的<mark data-type="technologies" data-id="87c62b00-48b2-4e2a-8122-9876a3d3e59e" class="tooltipstered">词袋模型</mark>。</p><p>3）使用 dnn 模型来进行<mark data-type="technologies" data-id="3fe4290f-6dd3-43ed-9324-cf23aa588830" class="tooltipstered">文本分类</mark>，解决传统<mark data-type="technologies" data-id="87c62b00-48b2-4e2a-8122-9876a3d3e59e" class="tooltipstered">词袋模型</mark>难以处理<mark data-type="technologies" data-id="c27b329a-4d4c-4471-89ca-71dce63c0605" class="tooltipstered">长距离依赖</mark>的缺点。</p><p><strong>4.NLP 其他应用</strong></p><p>NLP 在达观的其他一些应用包括：</p><p>1）标签抽取；</p><p>2）观点挖掘；</p><p>3）应用于<mark data-type="technologies" data-id="6ca1ea2d-6bca-45b7-9c93-725d288739c3" class="tooltipstered">推荐系统</mark>；</p><p>4）应用于搜索引擎。</p><p>标签抽取有多种方式：基于聚类的方法实现。此外，现在一些<mark data-type="technologies" data-id="01946acc-d031-4c0e-909c-f062643b7273" class="tooltipstered">深度学习</mark>的算法，通过有监督的手段实现标签抽取功能。</p><p>就观点挖掘而言，举例：床很破，睡得不好。我抽取的观点是「床破」，其中涉及到语法句法分析，将有关联成本提取出来。</p><p>搜索及推荐，使用到 NLP 的地方也很多，如搜索引擎处理用户<mark data-type="technologies" data-id="bf740558-f0f7-41a8-87a0-e695a97563b3" class="tooltipstered">查询</mark>的纠错，就用到信道噪声模型实行纠错处理。</p></div><div class="article-sidebar js-article-sidebar is-show"><a alt="评论" class="u-btn--circle article-sidebar__item is-comment js-go-comment" data-tip="评论" href="javascript:;"><i class="iconfont icon-message-default"></i></a><a alt="收藏文章" class="u-btn--circle article-sidebar__item js-like-action" data-id="7f603528-b18f-4f5d-8b1f-6493422b2361" data-path="/articles/2018-07-25-5" data-tip="收藏文章" href="javascript:;"><i class="iconfont icon-like-default"></i></a><a alt="分享文章" class="u-btn--circle article-sidebar__item article-share__box" data-tip="分享文章" href="javascript:;"><i class="iconfont icon-share-default"></i><div class="article-share__list "><button alt="分享到微信" class="u-btn--circle article-share__item article-share__webtn" href="javascript:;"><i class="iconfont icon-wechat"></i><div class="article-share__wechat js-qrcode"><canvas width="120" height="120"></canvas></div></button><button alt="分享到微博" class="u-btn--circle article-share__item js-share-btn" data-title="" data-type="weibo" href="javascript:;"><i class="iconfont icon-weibo"></i></button><button alt="分享到推特" class="u-btn--circle article-share__item js-share-btn" data-title="" data-type="twitter" href="javascript:;"><i class="iconfont icon-twitter"></i></button></div></a></div><div class="u-flex article__other js-article-other"><div class="article__other--tags"><a alt="basic" class="u-btn--gray category__link article__other--tag" href="https://www.jiqizhixin.com/categories/basic">入门</a><a alt="自然语言处理" class="u-btn--gray category__link article__other--tag" href="javascript:;">自然语言处理</a><a alt="文本分类" class="u-btn--gray category__link article__other--tag" href="javascript:;">文本分类</a><a alt="语义理解" class="u-btn--gray category__link article__other--tag" href="javascript:;">语义理解</a></div><div class="article-action js-article-action"><a alt="收藏文章" class="u-like-icon article-action__item js-like-action" data-id="7f603528-b18f-4f5d-8b1f-6493422b2361" data-path="/articles/2018-07-25-5" href="javascript:;"><i class="iconfont icon-like-default"></i><span class="article-action__count">4</span></a><a class="u-like-icon article-action__item js-go-comment js-switch-comment" href="javascript:;"><i class="iconfont icon-message-default"></i><span class="article-action__count"></span></a><a alt="分享" class="article-action__item u-like-icon article-share__box" href="javascript:;"><i class="iconfont icon-share-default"></i><div class="article-share__list is-vertical"><button alt="分享到微信" class="u-btn--circle article-share__item article-share__webtn" href="javascript:;"><i class="iconfont icon-wechat"></i><div class="article-share__wechat js-qrcode"><canvas width="120" height="120"></canvas></div></button><button alt="分享到微博" class="u-btn--circle article-share__item js-share-btn" data-title="NLP概述和文本自动分类算法详解" data-type="weibo" href="javascript:;"><i class="iconfont icon-weibo"></i></button><button alt="分享到推特" class="u-btn--circle article-share__item js-share-btn" data-title="NLP概述和文本自动分类算法详解" data-type="twitter" href="javascript:;"><i class="iconfont icon-twitter"></i></button></div></a></div></div></div><div class="u-col-8"><div class="article__hr"></div><div class="article-graph__container"><div class="article-similar__tip">相关数据</div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">神经网络</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Neural Network</div></div><p class="graph__content">（人工）神经网络是一种起源于 20 世纪 50 年代的监督式机器学习模型，那时候研究者构想了「感知器（perceptron）」的想法。这一领域的研究者通常被称为「联结主义者（Connectionist）」，因为这种模型模拟了人脑的功能。神经网络模型通常是通过反向传播算法应用梯度下降训练的。目前神经网络有两大主要类型，它们都是前馈神经网络：卷积神经网络（CNN）和循环神经网络（RNN），其中 RNN 又包含长短期记忆（LSTM）、门控循环单元（GRU）等等。深度学习是一种主要应用于神经网络帮助其取得更好结果的技术。尽管神经网络主要用于监督学习，但也有一些为无监督学习设计的变体，比如自动编码器和生成对抗网络（GAN）。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">词袋模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Bag of words</div></div><p class="graph__content">词袋模型（英语：Bag-of-words model）是个在自然语言处理和信息检索(IR)下被简化的表达模型。此模型下，像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式不考虑文法以及词的顺序。最近词袋模型也被应用在电脑视觉领域。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">分类问题</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Classification</div></div><p class="graph__content">分类问题是数据挖掘处理的一个重要组成部分，在机器学习领域，分类问题通常被认为属于监督式学习(supervised learning)，也就是说，分类问题的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类。根据类别的数量还可以进一步将分类问题划分为二元分类(binary classification)和多元分类(multiclass classification)。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Statistical_classification">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">降维</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Dimensionality reduction</div></div><p class="graph__content">降维算法是将 p+1 个系数的问题简化为 M+1 个系数的问题，其中 M&lt;p。算法执行包括计算变量的 M 个不同线性组合或投射（projection）。然后这 M 个投射作为预测器通过最小二乘法拟合一个线性回归模型。两个主要的方法是主成分回归（principal component regression）和偏最小二乘法（partial least squares）。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-08-31-2">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">布尔模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Boolean model</div></div><p class="graph__content">在数理逻辑中，布尔值模型是普通的塔斯基主义者的结构或模型概念的推广，在其中命题的真值不被限定为"真"和"假"，而是从某个固定的完全布尔代数中取值，布尔值模型是 Dana Scott、Robert M. Solovay 和 Petr Vopěnka 在1960年代为了帮助理解 Paul Cohen 的力迫方法而介入的。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E5%B8%83%E5%B0%94%E5%80%BC%E6%A8%A1%E5%9E%8B">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">条件随机场</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Conditional random field</div></div><p class="graph__content">条件随机场（conditional random field，简称 CRF），是一种鉴别式机率模型，是随机场的一种，常用于标注或分析序列资料，如自然语言文字或是生物序列。
如同马尔可夫随机场，条件随机场为无向性之图模型，图中的顶点代表随机变量，顶点间的连线代表随机变量间的相依关系，在条件随机场当中，随机变量 Y 的分布为条件机率，给定的观察值则为随机变量 X。原则上，条件随机场的图模型布局是可以任意给定的，一般常用的布局是链接式的架构，链接式架构不论在训练（training）、推论（inference）、或是解码（decoding）上，都存在有效率的算法可供演算。
条件随机场跟隐马尔可夫模型常被一起提及，条件随机场对于输入和输出的机率分布，没有如隐马尔可夫模型那般强烈的假设存在。 线性链条件随机场应用于标注问题是由Lafferty等人与2001年提出的。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E6%A2%9D%E4%BB%B6%E9%9A%A8%E6%A9%9F%E5%9F%9F">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">卷积神经网络</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Convolutional neural network</div></div><p class="graph__content">卷积神经网路（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网路由一个或多个卷积层和顶端的全连通层（对应经典的神经网路）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网路能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网路在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网路，卷积神经网路需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。
卷积网络是一种专门用于处理具有已知的、网格状拓扑的数据的神经网络。例如时间序列数据，它可以被认为是以一定时间间隔采样的一维网格，又如图像数据，其可以被认为是二维像素网格。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2018-07-25-5">Goodfellow, I.; Bengio Y.; Courville A. (2016). Deep Learning.  MIT Press.</a><a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">交叉熵</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Cross-entropy</div></div><p class="graph__content">交叉熵（Cross Entropy）是Loss函数的一种（也称为损失函数或代价函数），用于描述模型预测值与真实值的差距大小</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">隐马尔可夫模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Hidden Markov models (HMM)</div></div><p class="graph__content">隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">信息熵</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Information entropy</div></div><p class="graph__content">在信息论中，熵是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。熵的单位通常为比特，但也用Sh、nat、Hart计量，取决于定义用到对数的底。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">知识库</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Knowledge base</div></div><p class="graph__content">知识库是用于知识管理的一种特殊的数据库，以便于有关领域知识的采集、整理以及提取。知识库中的知识源于领域专家，它是求解问题所需领域知识的集合，包括基本事实、规则和其它有关信息。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/zh-cn/%E7%9F%A5%E8%AF%86%E5%BA%93">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">长距离依赖</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Long-distance dependencies</div></div><p class="graph__content">也作“长距离调序”问题，在机器翻译中，比如中英文翻译，其语言结构差异比较大，词语顺序存在全局变化，不容易被捕捉</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-08-22-6">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">信息增益</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Information gain (Ratio)</div></div><p class="graph__content">在决策树学习中，信息增益比是信息增益与固有信息的比率。 它被用来通过在选择属性时考虑分支的数量和大小来减少对多值属性的偏见.</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Information_gain_ratio">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">机器学习</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Machine Learning</div></div><p class="graph__content">机器学习是人工智能的一个分支，是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2018-07-25-5">Mitchell, T. (1997). Machine Learning. McGraw Hill.</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">映射</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Mapping</div></div><p class="graph__content">映射指的是具有某种特殊结构的函数，或泛指类函数思想的范畴论中的态射。 逻辑和图论中也有一些不太常规的用法。其数学定义为：两个非空集合A与B间存在着对应关系f，而且对于A中的每一个元素x，B中总有有唯一的一个元素y与它对应，就这种对应为从A到B的映射，记作f：A→B。其中，y称为元素x在映射f下的象，记作：y=f(x)。x称为y关于映射f的原象*。*集合A中所有元素的象的集合称为映射f的值域，记作f(A)。同样的，在机器学习中，映射就是输入与输出之间的对应关系。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Mapping">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">语言模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Language models</div></div><p class="graph__content">语言模型经常使用在许多自然语言处理方面的应用，如语音识别，机器翻译，词性标注，句法分析和资讯检索。由于字词与句子都是任意组合的长度，因此在训练过的语言模型中会出现未曾出现的字串(资料稀疏的问题)，也使得在语料库中估算字串的机率变得很困难，这也是要使用近似的平滑n元语法(N-gram)模型之原因。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">命名实体识别</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Named entity recognition</div></div><p class="graph__content">命名实体识别（NER）是信息提取（Information Extraction）的一个子任务，主要涉及如何从文本中提取命名实体并将其分类至事先划定好的类别，如在招聘信息中提取具体招聘公司、岗位和工作地点的信息，并将其分别归纳至公司、岗位和地点的类别下。命名实体识别往往先将整句拆解为词语并对每个词语进行此行标注，根据习得的规则对词语进行判别。这项任务的关键在于对未知实体的识别。基于此，命名实体识别的主要思想在于根据现有实例的特征总结识别和分类规则。这些方法可以被分为有监督（supervised）、半监督（semi-supervised）和无监督（unsupervised）三类。有监督学习包括隐形马科夫模型（HMM）、决策树、最大熵模型（ME）、支持向量机（SVM）和条件随机场（CRF）。这些方法主要是读取注释语料库，记忆实例并进行学习，根据这些例子的特征生成针对某一种实例的识别规则。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="http://nlp.cs.nyu.edu/sekine/papers/li07.pdf">David, N. &amp;  Satoshi, S. (2007).  A survey of named entity recognition and classification</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">自然语言处理</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Natural language processing</div></div><p class="graph__content">自然语言处理（英语：natural language processing，缩写作 NLP）是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。自然语言生成系统把计算机数据转化为自然语言。自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">机器翻译</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Machine translation</div></div><p class="graph__content">机器翻译（MT）是利用机器的力量「自动将一种自然语言（源语言）的文本翻译成另一种语言（目标语言）」。机器翻译方法通常可分成三大类：基于规则的机器翻译（RBMT）、统计机器翻译（SMT）和神经机器翻译（NMT）。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">朴素贝叶斯</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Naive Bayesian (classifier)</div></div><p class="graph__content">朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">最大熵模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Maximum Entropy Modeling</div></div><p class="graph__content">最大熵原理是概率模型学习的一个准则：学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。 应用最大熵原理得到的模型就是最大熵模型。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://homepages.inf.ed.ac.uk/lzhang10/maxent.html">Jaynes, E. T. (1957). Information theory and statistical mechanics. Physical review, 106(4), 620.</a><a class="graph__origin" href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">过拟合</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Overfitting</div></div><p class="graph__content">过拟合是指为了得到一致假设而使假设变得过度严格。避免过拟合是分类器设计中的一个核心任务。通常采用增大数据量和测试样本集的方法对分类器性能进行评价。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Overfitting">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">池化</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Pooling</div></div><p class="graph__content">池化（Pooling）是卷积神经网络中的一个重要的概念，它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效的原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的卷积层之间都会周期性地插入池化层。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="http://cs231n.github.io/convolutional-networks/">cs231n</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">概率模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">probabilistic models</div></div><p class="graph__content">概率模型（Statistical Model，也稱為Probabilistic Model）是用来描述不同随机变量之间关系的数学模型，通常情况下刻画了一个或多个随机变量之间的相互非确定性的概率关系。 从数学上讲，该模型通常被表达为 ，其中 是观测集合用来描述可能的观测结果， 是 对应的概率分布函数集合。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">推荐系统</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Recommender system</div></div><p class="graph__content">推荐系统（RS）主要是指应用协同智能（collaborative intelligence）做推荐的技术。推荐系统的两大主流类型是基于内容的推荐系统和协同过滤（Collaborative Filtering）。另外还有基于知识的推荐系统（包括基于本体和基于案例的推荐系统）是一类特殊的推荐系统，这类系统更加注重知识表征和推理。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">目标函数</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Objective function</div></div><p class="graph__content">目标函数f(x)就是用设计变量来表示的所追求的目标形式，所以目标函数就是设计变量的函数，是一个标量。从工程意义讲，目标函数是系统的性能标准，比如，一个结构的最轻重量、最低造价、最合理形式；一件产品的最短生产时间、最小能量消耗；一个实验的最佳配方等等，建立目标函数的过程就是寻找设计变量与目标的关系的过程，目标函数和设计变量的关系可用曲线、曲面或超曲面表示。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://baike.baidu.com/item/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0">百度百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">词性标注</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">part of speech (tagging)</div></div><p class="graph__content">词性标注是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或其他词性的过程。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">参数</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">parameter</div></div><p class="graph__content">在数学和统计学裡，参数（英语：parameter）是使用通用变量来建立函数和变量之间关系（当这种关系很难用方程来阐述时）的一个数量。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E5%8F%82%E6%95%B0">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">查询</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Query</div></div><p class="graph__content">一般来说，查询是询问的一种形式。它在不同的学科里涵义有所不同。在信息检索领域，查询指的是数据库和信息系统对信息检索的精确要求</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Query">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">主成分分析</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Principal component analysis</div></div><p class="graph__content">在多元统计分析中，主成分分析（Principal components analysis，PCA）是一种分析、简化数据集的技术。主成分分析经常用于减少数据集的维数，同时保持数据集中的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是，这也不是一定的，要视具体应用而定。由于主成分分析依赖所给数据，所以数据的准确性对分析结果影响很大。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-07-05-2">机器之心</a><a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">监督学习</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Supervised learning</div></div><p class="graph__content">监督式学习（Supervised learning），是机器学习中的一个方法，可以由标记好的训练集中学到或建立一个模式（函数 / learning model），并依此模式推测新的实例。训练集是由一系列的训练范例组成，每个训练范例则由输入对象（通常是向量）和预期输出所组成。函数的输出可以是一个连续的值（称为回归分析），或是预测一个分类标签（称作分类）。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://en.wikipedia.org/wiki/Supervised_learning#Applications">Wikipedia</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">堆叠</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Stacking</div></div><p class="graph__content">堆叠泛化是一种用于最小化一个或多个泛化器的泛化误差率的方法。它通过推导泛化器相对于所提供的学习集的偏差来发挥其作用。这个推导的过程包括：在第二层中将第一层的原始泛化器对部分学习集的猜测进行泛化，以及尝试对学习集的剩余部分进行猜测，并且输出正确的结果。当与多个泛化器一起使用时，堆叠泛化可以被看作是一个交叉验证的复杂版本，利用比交叉验证更为复杂的策略来组合各个泛化器。当与单个泛化器一起使用时，堆叠泛化是一种用于估计（然后纠正）泛化器的错误的方法，该泛化器已经在特定学习集上进行了训练并被询问了特定问题。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.sciencedirect.com/science/article/pii/S0893608005800231">Wolpert, D. H. (1992). Stacked generalization. Neural networks, 5(2), 241-259</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">文本分类</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">text classification</div></div><p class="graph__content">该技术可被用于理解、组织和分类结构化或非结构化文本文档。文本挖掘所使用的模型有词袋（BOW）模型、语言模型（ngram）和主题模型。隐马尔可夫模型通常用于词性标注（POS）。其涵盖的主要任务有句法分析、情绪分析和垃圾信息检测。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/2017-12-27-5">机器之心</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">反比文档频数权重评价方法</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">TF-IDF</div></div><p class="graph__content">tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/Tf-idf">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">主题模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">topic models</div></div><p class="graph__content">主题模型（Topic Model）在机器学习和自然语言处理等领域是用来在一系列文档中发现抽象主题的一种统计模型。直观来讲，如果一篇文章有一个中心思想，那么一些特定词语会更频繁的出现。比方说，如果一篇文章是在讲狗的，那“狗”和“骨头”等词出现的频率会高些。如果一篇文章是在讲猫的，那“猫”和“鱼”等词出现的频率会高些。而有些词例如“这个”、“和”大概在两篇文章中出现的频率会大致相等。但真实的情况是，一篇文章通常包含多种主题，而且每个主题所占比例各不相同。因此，如果一篇文章10%和猫有关，90%和狗有关，那么和狗相关的关键字出现的次数大概会是和猫相关的关键字出现次数的9倍。一个主题模型试图用数学框架来体现文档的这种特点。主题模型自动分析每个文档，统计文档内的词语，根据统计的信息来断定当前文档含有哪些主题，以及每个主题所占的比例各为多少。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">word2vec</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">word2vec</div></div><p class="graph__content">Word2vec，为一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。网络以词表现，并且需猜测相邻位置的输入词，在word2vec中词袋模型假设下，词的顺序是不重要的。
训练完成之后，word2vec模型可用来映射每个词到一个向量，可用来表示词对词之间的关系。该向量为神经网络之隐藏层。
Word2vec依赖skip-grams或连续词袋（CBOW）来建立神经词嵌入。Word2vec为托马斯·米科洛夫（Tomas Mikolov）在Google带领的研究团队创造。该算法渐渐被其他人所分析和解释。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/Word2vec">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">向量空间模型</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Vector space model</div></div><p class="graph__content">向量空间模型是一个把文本文件表示为标识符（比如索引）向量的代数模型。它应用于信息过滤、信息检索、索引以及相关排序。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F%E7%A9%BA%E9%96%93%E6%A8%A1%E5%9E%8B">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">词嵌入</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Word embedding</div></div><p class="graph__content">词嵌入是自然语言处理（NLP）中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E8%AF%8D%E5%B5%8C%E5%85%A5">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">权重</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Weight</div></div><p class="graph__content">线性模型中特征的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.jiqizhixin.com/articles/Wikipedia">Google AI Glossary</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">深度学习</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Deep learning</div></div><p class="graph__content">深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。
深度学习是机器学习中一种基于对数据进行表征学习的算法，至今已有数种深度学习框架，如卷积神经网络和深度置信网络和递归神经网络等已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。
</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://www.nature.com/articles/nature14539">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. nature, 521(7553), 436.</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">文本挖掘</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Text Mining</div></div><p class="graph__content">文本挖掘有时也被称为文字探勘、文本数据挖掘等，大致相当于文字分析，一般指文本处理过程中产生高质量的信息。高质量的信息通常通过分类和预测来产生，如模式识别。文本挖掘通常涉及输入文本的处理过程，产生结构化数据，并最终评价和解释输出。'高品质'的文本挖掘通常是指某种组合的相关性，新颖性和趣味性。</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://zh.wikipedia.org/wiki/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98">维基百科</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">准确率</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Accuracy</div></div><p class="graph__content">分类模型的正确预测所占的比例。在多类别分类中，准确率的定义为：正确的预测数/样本总数。
在二元分类中，准确率的定义为：(真正例数+真负例数)/样本总数
</p></div><div class="graph__title graph__link graph__origins"><i class="iconfont icon-link2 u-margin-right"></i>来源：<a class="graph__origin" href="https://developers.google.com/machine-learning/crash-course/glossary?hl=zh-cn">Google ML Glossary</a></div></div></div><div class="article-graph__item"><div class="article-graph__header js-tips-header"><span class="article-graph__keyword">支持向量机</span><span class="article-graph__type">技术</span><div class="article-graph__arrow"><i class="iconfont icon-arrowdown"></i></div></div><div class="article-graph__body"><div class="graphs__box"><div class="graph__header"><div class="graph__title graph__translate">Support Vector Machines</div></div><p class="graph__content"></p></div></div></div></div><div class="article__hr"></div><div class="article-from u-clearfix"><div class="article-from__author"><div class="u-flex article-from__inline"><a alt="达观数据" class="u-avatar-base article-from__avatar" href="https://www.jiqizhixin.com/users/dda7327c-1b8b-4197-9783-fc0319cf6e61" rel="noopener noreferrer" target="_blank"><img alt="达观数据" class="u-image-center" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/586cb2b3c01ec.png"></a><div><a alt="达观数据" class="u-text-limit--one article-from__name" href="https://www.jiqizhixin.com/users/dda7327c-1b8b-4197-9783-fc0319cf6e61" rel="noopener noreferrer" target="_blank">达观数据</a><p class="u-text-limit--two article-from__bio">机器之心编辑</p></div></div></div></div><div class="article__hr"></div><div class="article-similar"><div class="article-similar__tip">推荐文章</div><div class="u-flex article-similar__list"><div class="article-similar__item"><article class="article-similar__inline"><a alt="相关文章" class="u-image-base article-similar__cover" href="https://www.jiqizhixin.com/articles/2016-08-17-3" rel="noopener noreferrer" target="_blank"><img alt="计算机科学领导者：卡内基梅隆大学ACL2016论文汇总" class="u-image-center" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/57b425b993fcb.jpg"></a><a alt="相关文章" class="u-text-limit--two article-similar__title" href="https://www.jiqizhixin.com/articles/2016-08-17-3" rel="noopener noreferrer" target="_blank">计算机科学领导者：卡内基梅隆大学ACL2016论文汇总</a><footer class="article-similar__footer u-flex"><div class="u-flex"><a alt="机器之心" class="u-avatar-base article-simple__avatar" href="https://www.jiqizhixin.com/users/7f316f0c-8f72-4231-bb30-0eb1dd5a5660" rel="noopener noreferrer" target="_blank"><img alt="机器之心" class="u-image-center" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/58d0f028b9fe3.png"></a><span class="u-margin-left">机器之心</span></div><a alt="点赞文章" class="u-like-icon js-like-action" data-id="5f678c03-d735-4fc9-98f9-b6822dbd879c" data-path="/articles/2016-08-17-3" href="javascript:;"><i class="iconfont icon-like-default"></i><span class="article-action__count"></span></a></footer></article></div><div class="article-similar__item"><article class="article-similar__inline"><a alt="相关文章" class="u-image-base article-similar__cover" href="https://www.jiqizhixin.com/articles/2017-02-16" rel="noopener noreferrer" target="_blank"><img alt="理解和实现自然语言处理终极指南（附Python代码）" class="u-image-center" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/58a548004e24f.png"></a><a alt="相关文章" class="u-text-limit--two article-similar__title" href="https://www.jiqizhixin.com/articles/2017-02-16" rel="noopener noreferrer" target="_blank">理解和实现自然语言处理终极指南（附Python代码）</a><footer class="article-similar__footer u-flex"><div class="u-flex"><a alt="吴攀" class="u-avatar-base article-simple__avatar" href="https://www.jiqizhixin.com/users/b7f7030b-9b4e-4852-8ab6-fa80c7af0d94" rel="noopener noreferrer" target="_blank"><img alt="吴攀" class="u-image-center" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/57bab6b3b4a48.jpg"></a><span class="u-margin-left">吴攀</span></div><a alt="点赞文章" class="u-like-icon js-like-action" data-id="504450c5-b552-4250-950c-7e9ec1cc0512" data-path="/articles/2017-02-16" href="javascript:;"><i class="iconfont icon-like-default"></i><span class="article-action__count">4</span></a></footer></article></div><div class="article-similar__item"><article class="article-similar__inline"><a alt="相关文章" class="u-image-base article-similar__cover" href="https://www.jiqizhixin.com/articles/2015-12-13-3" rel="noopener noreferrer" target="_blank"><img alt="评价对象抽取综述" class="u-image-center" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/屏幕快照_2017-08-15_下午3.09.00.png"></a><a alt="相关文章" class="u-text-limit--two article-similar__title" href="https://www.jiqizhixin.com/articles/2015-12-13-3" rel="noopener noreferrer" target="_blank">评价对象抽取综述</a><footer class="article-similar__footer u-flex"><div class="u-flex"><a alt="哈工大SCIR" class="u-avatar-base article-simple__avatar" href="https://www.jiqizhixin.com/users/292961ca-69f3-4862-a754-896a8407fd97" rel="noopener noreferrer" target="_blank"><img alt="哈工大SCIR" class="u-image-center" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/avatar-1659ffc1-a6ae-425c-aaa4-764de8d246d3.png"></a><span class="u-margin-left">哈工大SCIR</span></div><a alt="点赞文章" class="u-like-icon js-like-action" data-id="5ed7138c-2f5e-4ce3-b7ca-4fb773e6aca9" data-path="/articles/2015-12-13-3" href="javascript:;"><i class="iconfont icon-like-default"></i><span class="article-action__count"></span></a></footer></article></div></div></div></div></div><div class="article__comment"><div class="u-container"><div class="u-col-8" id="comment-container"><div data-react-class="common/Comment" data-react-props="{&quot;currentUser&quot;:null,&quot;url&quot;:&quot;/articles/2018-07-25-5&quot;}"><div class="comment" id="comment"><div class="comment-inline comment-editor js-comment-area"><a class="is-disabled js-no-user-comment" href="javascript:;"><b>登录</b>后评论</a></div><div class="comment__container"><div class="comment-none__container"><img class="comment-none__cover" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/comment_none-80cb4ed688611aa4f23190a8338f8bab.png" alt="暂无评论"><div class="comment-none__title">暂无评论~</div></div></div></div></div></div></div></div></div><div data-user="null" id="js-login-functionality"></div><footer class="footer u-flex"><div class="u-flex footer__wrapper"><div class="footer__left"><div class="footer__logo"><img height="50" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/logo-c617614d41c836153141ce68ff2b8be19e15cd9c16b2ef1936bc4ad734397392.png"></div><div class="footer-link"><a alt="关于我们" class="footer-link__item" href="https://www.jiqizhixin.com/about" target="_blank">关于我们</a><a alt="寻求报道" class="footer-link__item" href="https://www.jiqizhixin.com/report" target="_blank">寻求报道</a><a alt="商务合作" class="footer-link__item" href="https://www.jiqizhixin.com/business" target="_blank">商务合作</a><a alt="加入我们" class="footer-link__item" href="https://www.jiqizhixin.com/join" target="_blank">加入我们</a><a alt="服务条款" class="footer-link__item" href="https://www.jiqizhixin.com/terms" target="_blank">服务条款</a></div><p class="footer__other">©2017 机器之心（北京）科技有限公司</p><p class="footer__other">京 ICP 备 12027496</p></div><div class="footer__middle"><div class="footer__title">全球人工智能信息服务</div><h5 class="footer__sub-title">友情链接</h5><div class="footer-link"><a alt="Synced Global" class="footer-link__item" href="https://syncedreview.com/" target="_blank">Synced Global</a><a alt="机器之心 Medium 博客" class="footer-link__item" href="https://medium.com/@Synced" target="_blank">机器之心 Medium 博客</a><a alt="PaperWeekly" class="footer-link__item" href="http://paperweek.ly/" target="_blank">PaperWeekly</a><a alt="网易智能" class="footer-link__item" href="http://tech.163.com/smart" target="_blank">网易智能</a><a alt="动脉网" class="footer-link__item" href="http://www.vcbeat.net/" target="_blank">动脉网</a><a alt="硬蛋网" class="footer-link__item" href="http://www.ingdan.com/" target="_blank">硬蛋网</a><a alt="达观数据" class="footer-link__item" href="http://www.datagrand.com/" target="_blank">达观数据</a><a alt="品途商业评论" class="footer-link__item" href="https://www.pintu360.com/" target="_blank">品途商业评论</a></div></div></div><div class="footer-right"><div class="footer__social"><a class="iconfont icon-qq" href="http://wpa.qq.com/msgrd?v=1&amp;uin=2378836078&amp;site=jiqizhxin.com&amp;menu=yes" rel="noopener noreferrer" target="_blank"></a><div class="footer__tooltip"><a class="iconfont icon-wechat" href="javascript:;"></a><div class="footer__tooltip-box"><img alt="机器之心微信公众平台" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/weixinQR-2e2eb9e01d350c1ce6bd7cfe4cf8aa2209a1d72b50f1a6147a371e85548fcdf4.jpg"></div></div><a class="iconfont icon-weibo" href="http://weibo.com/synced" rel="noopener noreferrer" target="_blank"></a><a class="iconfont icon-rss" href="https://jiqizhixin.com/rss" rel="noopener noreferrer" target="_blank"></a></div><p>联系电话：+86 010-57150141</p><p>联系邮箱：contact@jiqizhixin.com</p></div></footer><div class="backtop is-show" id="js-backtop"><a class="backtop__link" href="javascript:;"><i class="iconfont icon-xiangshangjiantou"></i></a><a class="backtop__link is-second">返回顶部</a></div><div data-react-class="common/Modal" data-react-props="{}"><div class="modal-layer"><span></span><span></span></div></div><div data-react-class="common/Alert" data-react-props="{}"><div class="s-alert-wrapper"></div></div><div class="notice__container" id="js-notice-container"></div><script async="" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/analytics.js"></script><script src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/plupload_2.1.6_plupload.full.min.js"></script><script data="208" defsi="361" id="ParadigmSDK" src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/ParadigmSDK_v2_define_itemid.js"></script><script>ParadigmSDK.init("0c2abb5a135747ca9d0f5103e5e95cc3");
ParadigmSDK.trackDetailPageShow("7f603528-b18f-4f5d-8b1f-6493422b2361")</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-96130205-2', 'auto');
ga('send', 'pageview');</script><script src="./NLP概述和文本自动分类算法详解 _ 机器之心_files/application-519a083400a58281ee16.js"></script></body></html>