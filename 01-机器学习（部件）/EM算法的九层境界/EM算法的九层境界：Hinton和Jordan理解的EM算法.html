<!DOCTYPE html>
<!-- saved from url=(0070)https://baijiahao.baidu.com/s?id=1586809503418533687&wfr=spider&for=pc -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="referrer" content="always"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no"><link rel="shortcut icon" href="https://gss0.bdstatic.com/5bd1bjqh_Q23odCf/static/wiseindex/img/favicon64.ico" type="image/x-icon"><link rel="apple-touch-icon-precomposed" href="https://gss0.bdstatic.com/5bd1bjqh_Q23odCf/static/wiseindex/img/screen_icon_new.png"><title>EM算法的九层境界：Hinton和Jordan理解的EM算法</title><link rel="stylesheet" href="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/land_min_pack_8a6173f.css"><meta itemprop="dateUpdate" content="2017-12-15 08:58:44"></head><body style=""><div id="BAIDU_DUP_fp_wrapper" style="position: absolute; left: -1px; bottom: -1px; z-index: 0; width: 0px; height: 0px; overflow: hidden; visibility: hidden; display: none;"><iframe id="BAIDU_DUP_fp_iframe" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/o.html" style="width: 0px; height: 0px; visibility: hidden; display: none;"></iframe></div><script>/* eslint-disable */var s_domain = {"protocol":"https:","staticUrl":"https://ss0.bdstatic.com/5aV1bjqh_Q23odCf/","base":"","baseuri":"","passconf":"http://passport.baidu.com/ubrwsbas","logout":"https://passport.baidu.com/?logout&u=","bs":"https://www.baidu.com","sp":"http://hi.baidu.com/","ssllist":{"a.hiphotos.baidu.com":"ss0.baidu.com/94o3dSag_xI4khGko9WTAnF6hhy","b.hiphotos.baidu.com":"ss1.baidu.com/9vo3dSag_xI4khGko9WTAnF6hhy","c.hiphotos.baidu.com":"ss3.baidu.com/9fo3dSag_xI4khGko9WTAnF6hhy","d.hiphotos.baidu.com":"ss0.baidu.com/-Po3dSag_xI4khGko9WTAnF6hhy","e.hiphotos.baidu.com":"ss1.baidu.com/-4o3dSag_xI4khGko9WTAnF6hhy","f.hiphotos.baidu.com":"ss2.baidu.com/-vo3dSag_xI4khGko9WTAnF6hhy","g.hiphotos.baidu.com":"ss3.baidu.com/-fo3dSag_xI4khGko9WTAnF6hhy","h.hiphotos.baidu.com":"ss0.baidu.com/7Po3dSag_xI4khGko9WTAnF6hhy","1.su.bdimg.com":"ss0.bdstatic.com/k4oZeXSm1A5BphGlnYG","t10.baidu.com":"ss0.baidu.com/6ONWsjip0QIZ8tyhnq","t11.baidu.com":"ss1.baidu.com/6ONXsjip0QIZ8tyhnq","t12.baidu.com":"ss2.baidu.com/6ONYsjip0QIZ8tyhnq","himg.bdimg.com":"ss1.bdstatic.com/7Ls0a8Sm1A5BphGlnYG","cdn00.baidu-img.cn":"ss0.bdstatic.com/9bA1vGba2gU2pMbfm9GUKT-w","cdn01.baidu-img.cn":"ss0.bdstatic.com/9bA1vGfa2gU2pMbfm9GUKT-w"}};var s_session = {"ssid":"9bca5b13","logid":"","sid":"1448_21117_26350","nid":"9935449564123784227","qid":""};var s_advert = {"isBjh":"1","contentUrl":"http://baijiahao.baidu.com/s?id=1586809503418533687","contentPlatformId":"3","contentType":"1","pvid":"ede8689a35fbb93e","time":"2018-11-09 17:30:32","contentAccType":"1","ctk":"8e79b722b101a263","contentAccId":"pQfSxJQA8G6P6RYpB","ctk_b":"944677d71ea03bd8","logid":"1832428193","dtime":"1541755832","grade":"2","createTimeAccLevel":"4"};</script><script>/* eslint-disable */var bds={se:{},su:{urdata:[],urSendClick:function(){},urStatic:"https://ss.bdimg.com"},util:{},use:{},comm:{domain:"",ubsurl:"",tn:"",queryEnc:"",queryId:"",inter:"",sugHost:"",query:"",qid:"",cid:"",sid:"",stoken:"",serverTime:"",user:"",username:"",loginAction:[],useFavo:"",pinyin:"",favoOn:"",curResultNum:"0",rightResultExist:false,protectNum:0,zxlNum:0,pageNum:1,pageSize:10,ishome:1,newindex:1}};var name,navigate,al_arr=[];var selfOpen=window.open;eval("var open = selfOpen;");var isIE=navigator.userAgent.indexOf("MSIE")!=-1&&!window.opera;var E=bds.ecom={};bds.se.mon={loadedItems:[],load:function(){},srvt:-1};try{bds.se.mon.srvt=parseInt(document.cookie.match(new RegExp("(^| )BDSVRTM=([^;]*)(;|$)"))[2]);document.cookie="BDSVRTM=;expires=Sat, 01 Jan 2000 00:00:00 GMT"}catch(e){}var bdUser=bds.comm.user?bds.comm.user:null,bdQuery=bds.comm.query,bdUseFavo=bds.comm.useFavo,bdFavoOn=bds.comm.favoOn,bdCid=bds.comm.cid,bdSid=bds.comm.sid,bdServerTime=bds.comm.serverTime,bdQid=bds.comm.queryId,bdstoken=bds.comm.stoken,login_success=[];</script><div id="detail-page"><div class="line-shadow"></div><div id="content-container" class="content-container clearfix" data-showlog="tid:126;cst:1;logInfo:landing;" data-extralog="flow:2;st:news;rid:9935449564123784227;pos:0;extra:;source:1;isBaijiahao:1;login:1;appId:1536771608122703;" data-ratio="1" data-rid="page"><div class="item-wrap"><div id="header_wrap" class="header_wrap"><div class="header_content"><div class="header_logo"><a href="https://www.baidu.com/" id="result_logo" data-clklog="tid:139;cst:2;logInfo:head_logo;" data-extralog="rid:;pos:;extra:;isBaiJiaHao:1;login:1;" data-rid="head_0"><img src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/logo_top.png" alt="到百度首页" title="到百度首页"></a></div><div class="header_menu" data-rid="header_menu"><a href="https://www.baidu.com/" class="index" data-clklog="tid:142;cst:2;logInfo:head_menu" data-extralog="rid:;pos:;extra:;isBaiJiaHao:1;login:1;type:backindex" data-rid="head_1">百度首页</a><div id="userBlock" class="login" style="display: inline-block;"><a href="https://passport.baidu.com/v2/?login&amp;tpl=mn&amp;u=http://www.baidu.com" class="login" data-clklog="tid:142;cst:2;logInfo:head_menu;" data-extralog="rid:;pos:;extra:;isBaiJiaHao:1;login:1;type:login" data-rid="head_2">登录</a><a href="http://i.baidu.com/" id="usrname" class="usrname" data-clklog="tid:142;cst:2;logInfo:head_menu;" data-extralog="rid:;pos:;extra:;isBaiJiaHao:1;login:1;type:usrname;" data-rid="head_2"><span id="nametxt">wydtc1h</span><i class="c-icon"></i></a><div id="user_menu" data-showlog="tid:131;cst:1;logInfo:usrmenu;" data-extralog="rid:;pos:;extra:;isBaiJiaHao:1;login:1;" class="s-isindex-wrap s-user-set-menu menu-top" style="display: none;"><div><a href="http://i.baidu.com/center" target="_blank" data-tid="1000" data-clklog="tid:146;cst:2;logInfo:usrmenu;" data-extralog="rid:;pos:1;extra:;isBaiJiaHao:1;login:1;type:center;" data-rid="usr_menu_1">个人中心</a><a href="http://passport.baidu.com/" data-tid="1001" target="_blank" data-clklog="tid:146;cst:2;logInfo:usrmenu;" data-extralog="rid:;pos:2;extra:;isBaiJiaHao:1;login:1;type:passport;" data-rid="usr_menu_2">帐号设置</a><a class="s-feedback" style="overflow:hidden" href="https://baijiahao.baidu.com/s?id=1586809503418533687&amp;wfr=spider&amp;for=pc#" onclick="return false;" data-clklog="tid:146;cst:2;logInfo:usrmenu;" data-extralog="rid:;pos:3;extra:;isBaiJiaHao:1;login:1;type:feedback;" data-rid="header_menu_3">意见反馈</a><a class="quit" style="overflow:hidden" href="https://passport.baidu.com/?logout&amp;u=https://www.baidu.com" data-clklog="tid:146;cst:2;logInfo:usrmenu;" data-extralog="rid:;pos:4;extra:;isBaiJiaHao:1;login:1;type:quit;" data-rid="usr_menu_4">退出</a></div><span class="menu-arrow"><em></em></span></div></div></div></div></div></div><div id="left-container" class="left-container"><div class="item-wrap"><div class="article " id="article" data-islow-browser="0"><div class="article-title"><h2>EM算法的九层境界：Hinton和Jordan理解的EM算法</h2></div><div class="article-desc clearfix"><div class="author-icon"><img src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/503d269759ee3d6dbc41f75245166d224e4adea0.jpg"></div><div class="author-txt"><p class="author-name">新智元</p><div class="article-source"><span class="source">百家号</span><span class="date">17-12-15</span><span class="time">08:58</span></div></div></div><div class="article-content"><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=3144961579,403124246&amp;fm=173&amp;s=2D448B4E03E0A55750ED5C120300A0D1&amp;w=640&amp;h=331&amp;img.JPEG" data-loaded="0"></div><p>Hinton, 这个深度学习的缔造者( 参考 攒说 Geoff Hinton) ， Jordan 当世概率图模型的集大成者（参考 “乔丹上海行”）， 他们碰撞的领域，EM算法！这个是PCA外的，另外一个无监督学习的经典，是我们的主题。</p><p>他们怎么认识的呢？Jordan的导师，就是著名的链接主义核心人物Rumelhart</p><p>（参考“易图秒懂の连接主义诞生”）。在“人工智能深度学习人物关系[全]”里面我们介绍到，Hinton和Rumelhart是同事，都在Francis Crick的小组。</p><p>前言</p><p><span class="bjh-strong">为什么说EM算法是他们强强发力的领域呢？</span></p><p>这里我们讨论Hinton和统计大神Jordan的强强发力的领域。当Bayes网络发展到高级阶段， 概率图模型使得计算成为问题，由此开启了Variational Bayes领域。在“变の贝叶斯”里面， 我们解释了研究Variational Bayes，有3拨人。 第一拨人， 把物理的能量搬到了机器学习（参考 “给能力以自由吧！”）。 第二拨人， 就是Hinton，他将VB和EM算法联系了起来，奠定了现在我们看到的VB的基础。 第三拨人，就是Jordan， 他重建了VB的框架ELBO的基础。所以说EM算法扩展的VBEM算法，就是Hinton和Jordan共同发力的部分。</p><p>Hinton曾在采访中，不无感慨的说到， 他当时研究VB和EM算法的关系的时候， 主动去请教当时的EM算法的大佬们， 结果那些人说Hinton是异想天开，神经有问题。 但是最终， 他还是突破重围， 搞定了VBEM算法，打下了VB世界最闪光的那盏灯。老爷子真心不容易！ 如果想切实深入到VB的世界， 我推荐Daphne Koller的神书“Probabilistic Graphical Models: Principles and Techniques”， 尤其其中的第8章：The Exponential Family 和第19章 Partially Observed Data。 这两章几乎是Hinton对VBEM算法研究的高度浓缩。 国内机器学习牛人王飞跃老师， 率领各路弟子花了5年时间翻译了这本神书！所以有中文版， 买了，反复阅读8、19章，要的！</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=2941552679,623899775&amp;fm=173&amp;s=728CB444082C309AC0870D5D0300D0AA&amp;w=640&amp;h=290&amp;img.JPEG" data-loaded="0"></div><p><span class="bjh-strong">为什么无监督深度学习突出成果都是Hinton和Jordan家的？</span></p><p>无监督深度学习，除了强化学习，主要包括BM、自动编码器AE和GAN领域。 1）这些领域中的DBN和DBM是Hinton搞的。2）AE中的经典，VAE是DP Kingma和M Welling搞得。 DP Kingma硕士导师是LeCun，LeCun的博士后导师是Hinton，并且Welling的博士后导师是Hinton。 3）而GAN是Ian Goodfellow和Yoshua Bengio的杰作， Goodfellow是Bengio的学生， 而Bengio的博士后导师是Jordan。 一句话， 无监督深度学习的经典模型几乎全是Hinton和Jordan家的。 为什么？ 因为能彻底理解EM算法到深不见底的人非Hinton和Jordan莫属。</p><p>你现在明白彻底理解EM算法的重要性了吧？ 下面我浅薄的纵向理解（忽略EM的各种变种的横向）EM算法的9层境界，再回头反思一下Hinton和Jordan等会对EM算法的理解到何种程度， 简直叹而观止！</p><p>EM算法理解的九层境界</p><p>EM 就是 E + M</p><p>EM 是一种局部下限构造</p><p>K-Means是一种Hard EM算法</p><p>从EM 到 广义EM</p><p>广义EM的一个特例是VBEM</p><p>广义EM的另一个特例是WS算法</p><p>广义EM的再一个特例是Gibbs抽样算法</p><p>WS算法是VAE和GAN组合的简化版</p><p>KL距离的统一</p><p>第一层境界， EM算法就是E 期望 + M 最大化</p><p>最经典的例子就是抛3个硬币，跑I硬币决定C1和C2，然后抛C1或者C2决定正反面， 然后估算3个硬币的正反面概率值。</p><div class="img-container"><img class="normal" width="418px" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1329529650,3285568963&amp;fm=173&amp;s=83B66423418940EA0A51A1CF000060B3&amp;w=418&amp;h=243&amp;img.JPEG" data-loaded="0"></div><p>这个例子为什么经典， 因为它告诉我们，当存在隐变量I的时候， 直接的最大似然估计无法直接搞定。 什么是隐变量？为什么要引入隐变量？ 对隐变量的理解是理解EM算法的第一要义！Chuong B Do &amp; Serafim Batzoglou的Tutorial论文“What is the expectation maximization algorithm?”对此有详细的例子进行分析。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=2708767222,900817948&amp;fm=173&amp;s=C912E51A4D0FF4CE0C5100C10300D0B3&amp;w=640&amp;h=671&amp;img.JPEG" data-loaded="0"></div><p>通过隐变量，我们第一次解读了EM算法的伟大！突破了直接MLE的限制（不详细解释了）。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=863775579,4100567932&amp;fm=173&amp;s=0892EC12410ECCEC0879A0DA0000C0B2&amp;w=640&amp;h=470&amp;img.JPEG" data-loaded="0"></div><p>至此， 你理解了EM算法的第一层境界，看山是山。</p><p>第二层境界， EM算法就一种局部下限构造</p><p>如果你再深入到基于隐变量的EM算法的收敛性证明， 基于log(x)函数的Jensen不等式构造， 我们很容易证明，EM算法是在反复的构造新的下限，然后进一步求解。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1433938504,4255583146&amp;fm=173&amp;s=482834721BFEE4CE0C55D5CF000050B3&amp;w=640&amp;h=664&amp;img.JPEG" data-loaded="0"></div><p>所以，先固定当前参数， 计算得到当前隐变量分布的一个下届函数， 然后优化这个函数， 得到新的参数， 然后循环继续。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1742898962,2139324534&amp;fm=173&amp;s=0432E0326992CDC04C4409D8000050B3&amp;w=640&amp;h=622&amp;img.JPEG" data-loaded="0"></div><p>也正是这个不停的构造下限的思想未来和VB方法联系起来了。 如果你理解了这个， 恭喜你， 进入理解EM算法的第二层境界， 看山看石。</p><p>第三层境界， K-均值方法是一种Hard EM算法</p><p>在第二层境界的基础上， 你就能随意傲游EM算法用到GMM和HMM模型中去了。 尤其是对GMM的深入理解之后， 对于有隐变量的联合概率，如果利用高斯分布代入之后：</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1839371820,322941542&amp;fm=173&amp;s=CDA43472D1624D2258DDD4CB000070B1&amp;w=640&amp;h=175&amp;img.JPEG" data-loaded="0"></div><p>很容易就和均方距离建立联系：</p><p>但是，能不能说K-均值就是高斯分布的EM算法呢？不是， 这里虽然拓展到了相同的距离公式， 但是背后逻辑还是不一样， 不一样在哪里呢？K-均值在讨论隐变量的决定时候，用的是dirac delta 分布， 这个分布是高斯分布的一种极限。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1627041180,1758990176&amp;fm=173&amp;s=58223472033F44290E5591CA0000B0B1&amp;w=640&amp;h=464&amp;img.JPEG" data-loaded="0"></div><div class="img-container"><img class="normal" width="199px" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/a8014c086e061d954b4cd6b370f40ad162d9ca37.jpg" data-loaded="0"></div><p>如果你觉得这个扩展不太好理解， 那么更为简单直观的就是， k-均值用的hard EM算法， 而我们说的EM算法是soft EM算法。 所谓hard 就是要么是，要么不是0-1抉择。 而Soft是0.7比例是c1，0.3比例是c2的情况。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=355729686,3073305236&amp;fm=173&amp;s=E192653213726C2054DC14CA0000D0B3&amp;w=640&amp;h=227&amp;img.JPEG" data-loaded="0"></div><p>那么充分理解了k-均值和EM算法本身的演化和差异有什么帮助呢？让你进一步理解到隐变量是存在一种分布的。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=826230690,86495719&amp;fm=173&amp;s=45A4357283E0690158D514CB0000F0B1&amp;w=576&amp;h=199&amp;img.JPEG" data-loaded="0"></div><p>如果你理解了这个， 恭喜你， 进入理解EM算法的第三层境界， 看山看峰。</p><p>第四层境界，EM 是 广义EM的特例</p><p>通过前3层境界， 你对EM算法的理解要跨过隐变量， 进入隐分布的境界。 如果我们把前面的EM收敛证明稍微重复一下，但是引入隐分布。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=2440403528,656948762&amp;fm=173&amp;s=1AAE7C2285D07DC2587DF0CB0000D0B1&amp;w=640&amp;h=268&amp;img.JPEG" data-loaded="0"></div><p>这样我们把Jensen不等收右边的部分定义为自由能（如果你对自由能有兴趣，请参考“给能量以自由吧！”，如果没有兴趣， 你就视为一种命名）。 那么E步骤是固定参数优化隐分布， M步骤是固定隐分布优化参数，这就是广义EM算法了。</p><p>有了广义EM算法之后， 我们对自由能深入挖掘， 发现自由能和似然度和KL距离之间的关系：</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=2047456037,2203880370&amp;fm=173&amp;s=88207C3287846D535A7505DE0000C0B3&amp;w=640&amp;h=217&amp;img.JPEG" data-loaded="0"></div><p>所以固定参数的情况下， 那么只能最优化KL距离了， 那么隐分布只能取如下分布：</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=2057782239,62335207&amp;fm=173&amp;s=07F6ED32C5376A3050FC01E002007032&amp;w=640&amp;h=126&amp;img.JPEG" data-loaded="0"></div><p>而这个在EM算法里面是直接给出的。 所以EM算法是广义EM算法的天然最优的隐分布情况。 但是很多时候隐分布不是那么容易计算的！</p><p>前面的推理虽然很简单， 但是要理解到位真心不容易， 首先要深入理解KL距离是如何被引入的？</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=2838424755,1637034516&amp;fm=173&amp;s=04B06432CB264D0B504935DA0000C0B2&amp;w=640&amp;h=384&amp;img.JPEG" data-loaded="0"></div><p>其次要理解， 为什么传统的EM算法， 不存在第一个最优化？因为在没有限制的隐分布（天然情况下）情况下， 第一个最优就是要求：</p><p>而这个隐分布， EM算法里面是直接给出的，而不是让你证明得到的。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=4031281130,2344553025&amp;fm=173&amp;s=6924387293FB602346CDF5CE0000C0B3&amp;w=640&amp;h=254&amp;img.JPEG" data-loaded="0"></div><p>这样， 在广义EM算法中，你看到两个优化步骤，我们进入了两个优化步骤理解EM算法的境界了。</p><p>如果你理解了这个， 恭喜你， 进入理解EM算法的第四层境界， 有水有山。</p><p>第五层境界，广义EM的一个特例是VBEM</p><p>在隐分布没有限制的时候， 广义EM算法就是EM算法， 但是如果隐分布本身是有限制的呢？譬如有个先验分布的限制， 譬如有计算的限制呢？</p><p>例如先验分布的限制：从pLSA到LDA就是增加了参数的先验分布！</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=2106786826,3705702174&amp;fm=173&amp;s=41F22D73099F49CA4E5501C00000E0B2&amp;w=640&amp;h=370&amp;img.JPEG" data-loaded="0"></div><p>例如计算上的限制：mean-field计算简化的要求，分量独立。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=3893946158,3430174536&amp;fm=173&amp;s=0CA074320BD64CC8424120DA0000A0B1&amp;w=594&amp;h=360&amp;img.JPEG" data-loaded="0"></div><p>诸如此类限制， 都使得广义EM里面的第一步E优化不可能达到无限制最优， 所以KL距离无法为0。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1498826247,3554908190&amp;fm=173&amp;s=69A634728FA3480B4CFDF5CE000090B3&amp;w=640&amp;h=268&amp;img.JPEG" data-loaded="0"></div><p>基于有限制的理解， 再引入模型变分的思想， 根据模型m的变化， 对应参数和隐变量都有相应的分布：</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=2889647379,1036012876&amp;fm=173&amp;s=692034729F504CC05E7CF4CE000070B1&amp;w=640&amp;h=238&amp;img.JPEG" data-loaded="0"></div><p>并且满足分布独立性简化计算的假设：</p><p>在变分思想下， 自由能被改写了：</p><p>这样我们就得到了VBEM算法了：</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=785333938,1855786943&amp;fm=173&amp;s=01B0ED3215DA4DC25E7970CA0000C0B2&amp;w=640&amp;h=347&amp;img.JPEG" data-loaded="0"></div><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1317516151,2110580524&amp;fm=173&amp;s=19AE743387BA482B087470CE0000C0B3&amp;w=640&amp;h=256&amp;img.JPEG" data-loaded="0"></div><p>如果你理解了这个， 恭喜你， 进入理解EM算法的第五层境界， 水转山回。</p><p>第六层境界，广义EM的另一个特例是WS算法</p><p>Hinton老爷子搞定VBEM算法后， 并没有停滞， 他在研究DBN和DBM的Fine-Tuning的时候， 提出了Wake-Sleep算法。 我们知道在有监督的Fine-Tuning可以使用BP算法， 但是无监督的Fine-Tuning，使用的是Wake-Sleep算法。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=816071074,1763562398&amp;fm=173&amp;s=7EAC346295105DC246F575C70000A0B1&amp;w=640&amp;h=265&amp;img.JPEG" data-loaded="0"></div><p>就是这个WS算法，也是广义EM算法的一种特例。 WS算法分为认知阶段和生成阶段。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=3326734539,522864845&amp;fm=173&amp;s=2BAA7A22C371758A5E481CE1020010B3&amp;w=640&amp;h=332&amp;img.JPEG" data-loaded="0"></div><p>在前面自由能里面，我们将KL距离引入了， 这里刚好这两个阶段分别优化了KL距离的两种形态。 固定P优化Q，和固定Q优化P。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=884372277,2786041438&amp;fm=173&amp;s=8325F10293AE0EAD6115F693030070A1&amp;w=640&amp;h=458&amp;img.JPEG" data-loaded="0"></div><p>所以当我们取代自由能理解， 全部切换到KL距离的理解， 广义EM算法的E步骤和M步骤就分别是E投影和M投影。 因为要求KL距离最优， 可以等价于垂直。 而这个投影， 可以衍生到数据D的流形空间， 和模型M的流形空间。</p><div class="img-container"><img class="normal" width="464px" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1326314710,2059183740&amp;fm=173&amp;s=4DAC3D72599EECC81A7990CE0000E0B1&amp;w=464&amp;h=377&amp;img.JPEG" data-loaded="0"></div><p>所以你认同WS算法是一种广义EM算法（GEM）之后， 基于KL距离再认识GEM算法。 引入了数据流形和模型流形。 引入了E投影和M投影。</p><p>不过要注意的wake识别阶段对应的是M步骤， 而sleep生成阶段对应的E步骤。 所以WS算法对应的是广义ME算法。</p><p>如果你理解了这个， 恭喜你， 进入理解EM算法的第六层境界， 山高水深。</p><p>第七层境界，广义EM的再一个特例是Gibbs Sampling</p><p>其实，前面基于KL距离的认知， 严格放到信息理论的领域， 对于前面E投影和M投影都有严格的定义。 M投影的名称是类似的，但是具体是moment projection，但是E投影应该叫I投影，具体是information projection。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=3709146074,3145476919&amp;fm=173&amp;s=5A8C3C6211D2DDCC5CDD15DA000050B0&amp;w=640&amp;h=400&amp;img.JPEG" data-loaded="0"></div><p>上面这种可能不太容易体会到M投影和I投影的差异， 如果再回到最小KL距离，有一个经典的比较。 可以体会M投影和I投影的差异。 上面是I投影，只覆盖一个峰。 下面是M投影， 覆盖了两个峰。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1219497513,595321857&amp;fm=173&amp;s=0D887433111265C446C445CA000070B2&amp;w=640&amp;h=352&amp;img.JPEG" data-loaded="0"></div><p>当我们不是直接计算KL距离， 而是基于蒙特卡洛抽样方法来估算KL距离。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1075991536,3772138707&amp;fm=173&amp;s=41F2AC72095E5DCC1C5DF0CA000080B2&amp;w=640&amp;h=421&amp;img.JPEG" data-loaded="0"></div><p>有兴趣对此深入的，可以阅读论文“On Monte Carlo methods for estimating ratios of normalizing constants”</p><p>这时候， 广义EM算法，就是Gibbs Sampling了。 所以Gibbs Sampling，本质上就是采用了蒙特卡洛方法计算的广义EM算法。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=455675167,1478884832&amp;fm=173&amp;s=00186C3259CFE4CA185914DE000090B1&amp;w=640&amp;h=568&amp;img.JPEG" data-loaded="0"></div><p>所以， 如果把M投影和I投影看成是一个变量上的最小距离点，那么Gibbs Sampling和广义EM算法的收敛过程是一致的。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1005373050,4113731742&amp;fm=173&amp;s=5CAC3C72C942EF4D5CDD15CA0000A0B1&amp;w=640&amp;h=471&amp;img.JPEG" data-loaded="0"></div><p>VAE的发明者，Hinton的博士后， Max Welling在论文“Bayesian K-Means as a “Maximization-Expectation” Algorithm”中， 对这种关系有如下很好的总结！</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=3916924860,3500441145&amp;fm=173&amp;s=3EAE742385D87DC25EF544DB0000A0B1&amp;w=640&amp;h=270&amp;img.JPEG" data-loaded="0"></div><p>另外， Zoubin Ghahramani， Jordan的博士， 在“Factorial Learning and the EM Algorithm”等相关论文也反复提到他们之间的关系。</p><p>这样， 通过广义EM算法把Gibbs Sampling和EM， VB， K-Means和WS算法全部联系起来了。 有了Gibbs Sampling的背书， 你是不是能更好的理解， 为什么WS算法可以是ME步骤，而不是EM的步骤呢？另外，我们知道坐标下降Coordinate Descent也可以看成一种Gibbs Sampling过程， 如果有人把Coordinate Descent和EM算法联系起来， 你还会觉得奇怪么？</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1471732861,3295770296&amp;fm=173&amp;s=1C2870321B0F654B5ADDC5DA0000E0B3&amp;w=640&amp;h=387&amp;img.JPEG" data-loaded="0"></div><p>现在我们发现VB和Gibbs Sampling都可以放到广义EM的大框架下， 只是求解过程一个采用近似逼近， 一个采用蒙特卡洛采样。 有了EM算法和Gibbs Sampling的关系， 现在你理解， 为什么Hinton能够发明CD算法了么？ 细节就不展开了。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=925420284,4077330478&amp;fm=173&amp;s=8A40E113C534CE210A7431DB0300D0B2&amp;w=640&amp;h=141&amp;img.JPEG" data-loaded="0"></div><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=307165134,3489957463&amp;fm=173&amp;s=C410ED33C1F74C315AF4A4CA000010B2&amp;w=640&amp;h=136&amp;img.JPEG" data-loaded="0"></div><p>如果你理解了这个， 恭喜你， 进入理解EM算法的第七层境界， 山水轮回。</p><p>第八层境界，WS算法是VAE和GAN组合的简化版</p><p>Jordan的弟子邢波老师，他的学生胡志挺，发表了一篇文章， On Unifying Deep Generative Models，试图通过WS算法，统一对VAE和GAN的理解。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1174302309,1147310677&amp;fm=173&amp;s=4192E933071159CA48F8B5DA0000C0B3&amp;w=640&amp;h=293&amp;img.JPEG" data-loaded="0"></div><p>对VAE的理解， 变了加了正则化的KL距离， 而对于GAN的理解变成了加Jensen–Shannon 散度。 所以， 当我们把广义EM算法的自由能， 在WS算法中看成KL散度， 现在看成扩展的KL散度。 对于正则化扩展， 有很多类似论文， “Mode Regularized Generative Adversarial Networks”， “Stabilizing Training of Generative Adversarial Networks through Regularization” 有兴趣可以读读。</p><p>所以对于VAE，类比WS算法的Wake认知阶段， 不同的是在ELBO这个VBEM目标的基础上加了KL散度作为正则化限制。 再应用再参数化技巧实现了VAE。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=2138979236,1454215608&amp;fm=173&amp;s=0192ED32195ADDCC1870A0DA0000C0B2&amp;w=640&amp;h=359&amp;img.JPEG" data-loaded="0"></div><p>而对应到GAN，类比Sleep阶段，正则化限制换了JSD距离， 然后目标KL距离也随着不同GAN的变体也可以变化。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=139194951,1108455821&amp;fm=173&amp;s=0D92E812195CDCCC04D8D1DE0000C0B3&amp;w=640&amp;h=360&amp;img.JPEG" data-loaded="0"></div><p>所以， VAE和GAN都可以理解为有特殊正则化限制的Wake-Sleep步骤， 那么组合起来也并不奇怪。</p><p>这就是为什么那么多论文研究如何组合VAE/GAN到同一个框架下面去。目前对这方面的理解还在广泛探讨中。</p><p>如果你理解了这个， 恭喜你， 进入理解EM算法的第八层境界， 水中有水、山外有山。</p><p>第九层境界，KL距离的统一</p><p>Jordan 大佬的一片论文， 开启了KL距离的统一， “On surrogate loss functions and f-divergences”。 里面对于所谓的正反KL距离全部统一到 f 散度的框架下面。 Jordan 首先论述了对于损失函数统一的Margin理论的意义。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1839172048,229813051&amp;fm=173&amp;s=00F0E132095E55CE12CDE0DA0000C0B2&amp;w=640&amp;h=437&amp;img.JPEG" data-loaded="0"></div><p>然后把这些损失函数也映射到 f 散度：</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=388514779,4182625928&amp;fm=173&amp;s=5A28346213B66D221E5C94DA0000C0B1&amp;w=640&amp;h=230&amp;img.JPEG" data-loaded="0"></div><p>然后微软的 Sebastian Nowozin， 把 f-散度扩展到GAN “f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization”。</p><p>然后对正反KL散度也做了一次统一。</p><p>对于 f-散度的理解离不开对Fenchel对偶的理解（参考“走近中神通Fenchel”）。</p><p>除了f-散度， 还有人基于bregman散度去统一正反KL散度的认知。 KL散度就是香农熵的bregman散度。</p><p>而Bregman散度本身是基于一阶泰勒展开的一种偏离度的度量。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=3336025316,2464697583&amp;fm=173&amp;s=68A43C7225FAE9C6107C2DCE000050B2&amp;w=640&amp;h=368&amp;img.JPEG" data-loaded="0"></div><p>然后再基于Bregman距离去研究最小KL投影， 函数空间采用香农熵（参考“信息熵的由来”）。</p><div class="img-container"><img class="large" data-loadfunc="0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1064104765,338471376&amp;fm=173&amp;s=4092EE3A53785D880068A5D30000C0B2&amp;w=640&amp;h=287&amp;img.JPEG" data-loaded="0"></div><p>无论f-散度还是bregman散度对正反KL距离的统一， 之后的广义EM算法， 都会变得空间的最优投影的交替出现。 或许广义EM算法也成了不同流形空间上的坐标梯度下降算法而已coodinate descent。</p><p>如果你理解了这个， 恭喜你， 进入理解EM算法的第九层境界，山水合一。</p><p>小结</p><p>这里浅薄的介绍了理解EM算法的9层境界，托名Hinton和Jordan，着实是因为佩服他们俩和各自的弟子们对EM算法，甚至到无监督深度学习的理解和巨大贡献。想来Hinton和Jordan对此必定会有更为深刻的理解， 很好奇会到何种程度 。。。 最后依然好奇， 为啥只有他们两家的子弟能够不停的突破无监督深度学习？Hinton 老仙说， 机器学习的未来在于无监督学习！</p><p><span class="bjh-strong">本文经授权转载自AI2ML人工智能to机器学习，点击阅读原文查看原文。</span></p></div><div class="notice"><p>本文由百家号作者上传并发布，百家号仅提供信息发布平台。文章仅代表作者个人观点，不代表百度立场。未经作者许可，不得转载。</p></div><audio height="0" width="0" id="musicAudio" data-play-index=""><source></audio></div></div></div><div id="right-container" class="right-container"><div class="item-wrap"><div class="author"><div class="author-info clearfix"><div class="author-icon"><img src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/503d269759ee3d6dbc41f75245166d224e4adea0.jpg"></div><div class="author-txt"><p class="author-name">新智元</p><div class="article-source"><span class="source">百家号</span><span>最近更新：</span><span class="date">17-12-15</span><span class="time">08:58</span></div></div></div><div class="author-desc"><p>简介:人工智能垂直媒体。</p></div></div></div><div class="item-wrap"><div class="recent-article" data-pos=""><h2>作者最新文章<ul><li data-showlog="tid:135;cst:1;logInfo:recent_article;" data-extralog="rid:;pos:0;extra:;isBaiJiaHao:1;login:1;" data-rid="recent_article_0"><h3 class="item-title"><a href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_9534703603296327440%22%7D&amp;n_type=1&amp;p_from=3" data-clklog="tid:147;cst:2;logInfo:recent_article;" data-extralog="rid:;pos:0;extra:;isBaiJiaHao:1;login:1;" target="_blank">7位数年薪抢人，DeepMind与Facebook人才大战又开火</a></h3><div class="item-desc hide"><span class="info-date">11-09</span><span class="info-time">15:38</span></div></li><li data-showlog="tid:135;cst:1;logInfo:recent_article;" data-extralog="rid:;pos:1;extra:;isBaiJiaHao:1;login:1;" data-rid="recent_article_1"><h3 class="item-title"><a href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_9435431962512233159%22%7D&amp;n_type=1&amp;p_from=3" data-clklog="tid:147;cst:2;logInfo:recent_article;" data-extralog="rid:;pos:1;extra:;isBaiJiaHao:1;login:1;" target="_blank">2012年来AI计算量暴增30万倍！王恩东院士：算力就是生产力</a></h3><div class="item-desc hide"><span class="info-date">11-09</span><span class="info-time">15:19</span></div></li><li data-showlog="tid:135;cst:1;logInfo:recent_article;" data-extralog="rid:;pos:2;extra:;isBaiJiaHao:1;login:1;" data-rid="recent_article_2"><h3 class="item-title"><a href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_9642466020615364512%22%7D&amp;n_type=1&amp;p_from=3" data-clklog="tid:147;cst:2;logInfo:recent_article;" data-extralog="rid:;pos:2;extra:;isBaiJiaHao:1;login:1;" target="_blank">周志华作序！高徒魏秀参新书《解析深度学习》试读（评论赠书）</a></h3><div class="item-desc hide"><span class="info-date">11-09</span><span class="info-time">15:19</span></div></li></ul></h2></div></div><div class="item-wrap"><div class="related-news"><div class="news-content "><h2>相关文章</h2><ul><li data-showlog="tid:136;cst:1;logInfo:related_news;" data-extralog="flow:2;rid:;pos:0;extra:;isBaiJiaHao:1;login:1;" data-rid="related_news_0"><div class="item-content clearfix"><div class="news-pic"><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:0;extra:;isBaiJiaHao:1;login:1;type:&#39;img&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_9189192197455293575%22%7D&amp;n_type=1&amp;p_from=4" target="_blank"><img src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1469754019,50103762&amp;fm=173&amp;app=49&amp;f=JPEG"></a></div><div class="news-info"><div class="news-title"><h3><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:0;extra:;isBaiJiaHao:1;login:1;type:&#39;title&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_9189192197455293575%22%7D&amp;n_type=1&amp;p_from=4" target="_blank">迈克尔·乔丹 戳人工智能泡沫，关注数据与现实</a></h3></div><div class="news-desc"><span class="info-num">新京报</span><span class="info-date">11-07</span></div></div></div></li><li data-showlog="tid:136;cst:1;logInfo:related_news;" data-extralog="flow:2;rid:;pos:1;extra:;isBaiJiaHao:1;login:1;" data-rid="related_news_1"><div class="item-content clearfix"><div class="news-pic"><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:1;extra:;isBaiJiaHao:1;login:1;type:&#39;img&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_8944173160632091331%22%7D&amp;n_type=1&amp;p_from=4" target="_blank"><img src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=1801202959,2835153596&amp;fm=173&amp;s=C581BA504EAEA449582F323E0300805C&amp;w=218&amp;h=146&amp;img.JPEG"></a></div><div class="news-info"><div class="news-title"><h3><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:1;extra:;isBaiJiaHao:1;login:1;type:&#39;title&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_8944173160632091331%22%7D&amp;n_type=1&amp;p_from=4" target="_blank">迷你世界算法器双控开关教程分享</a></h3></div><div class="news-desc"><span class="info-num">索尼游戏俱乐部</span><span class="info-date">12-19</span></div></div></div></li><li data-showlog="tid:136;cst:1;logInfo:related_news;" data-extralog="flow:2;rid:;pos:2;extra:;isBaiJiaHao:1;login:1;" data-rid="related_news_2"><div class="item-content clearfix"><div class="news-pic"><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:2;extra:;isBaiJiaHao:1;login:1;type:&#39;img&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_10000774753864221542%22%7D&amp;n_type=1&amp;p_from=4" target="_blank"><img src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=36267840,2597715951&amp;fm=175&amp;s=B10640B654CF43F99E317793030080AC&amp;w=218&amp;h=146&amp;img.JPEG"></a></div><div class="news-info"><div class="news-title"><h3><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:2;extra:;isBaiJiaHao:1;login:1;type:&#39;title&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_10000774753864221542%22%7D&amp;n_type=1&amp;p_from=4" target="_blank">男生穿Air Jordan 1 怎么搭配才好看？</a></h3></div><div class="news-desc"><span class="info-num">请叫我军活馆</span><span class="info-date">11-16</span></div></div></div></li><li data-showlog="tid:136;cst:1;logInfo:related_news;" data-extralog="flow:2;rid:;pos:3;extra:;isBaiJiaHao:1;login:1;" data-rid="related_news_3"><div class="item-content clearfix"><div class="news-pic"><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:3;extra:;isBaiJiaHao:1;login:1;type:&#39;img&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_9887036589692791984%22%7D&amp;n_type=1&amp;p_from=4" target="_blank"><img src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=3829452342,3213899396&amp;fm=173&amp;s=8031CF30C0B901882DC0E8D5010010A1&amp;w=218&amp;h=146&amp;img.JPEG"></a></div><div class="news-info"><div class="news-title"><h3><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:3;extra:;isBaiJiaHao:1;login:1;type:&#39;title&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_9887036589692791984%22%7D&amp;n_type=1&amp;p_from=4" target="_blank">Facebook动态消息排序算法的工作原理</a></h3></div><div class="news-desc"><span class="info-num">幸福的耗子</span><span class="info-date">12-17</span></div></div></div></li><li data-showlog="tid:136;cst:1;logInfo:related_news;" data-extralog="flow:2;rid:;pos:4;extra:;isBaiJiaHao:1;login:1;" data-rid="related_news_4"><div class="item-content clearfix"><div class="news-pic"><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:4;extra:;isBaiJiaHao:1;login:1;type:&#39;img&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_8974536938796253063%22%7D&amp;n_type=1&amp;p_from=4" target="_blank"><img src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/u=886933959,1078575601&amp;fm=173&amp;s=B837CE14647851284EAE5156030070BA&amp;w=218&amp;h=146&amp;img.JPEG"></a></div><div class="news-info"><div class="news-title"><h3><a class="upgrade" data-clklog="tid:148;cst:2;logInfo:related_news;" data-extralog="rid:;pos:4;extra:;isBaiJiaHao:1;login:1;type:&#39;title&#39;" href="https://mbd.baidu.com/newspage/data/landingsuper?context=%7B%22nid%22%3A%22news_8974536938796253063%22%7D&amp;n_type=1&amp;p_from=4" target="_blank">人生要经历三种境界，说得真好！</a></h3></div><div class="news-desc"><span class="info-num">阳光灿烂的你丶</span><span class="info-date">12-20</span></div></div></div></li></ul></div></div><div id="_5fvcnq0yppc" ad-id="relate-js" class="wangmeng-ad" data-showlog="tid:369;cst:1;logInfo:adsjs;" data-clklog="tid:370;cst:2;logInfo:adsjs;" data-extralog="rid:;pos:5;extra:;baijiahao:1;login:1;"><iframe id="iframeu2913176_0" name="iframeu2913176_0" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/zcxm.html" width="300" height="250" align="center,center" vspace="0" hspace="0" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" style="border:0;vertical-align:bottom;margin:0;width:300px;height:250px" allowtransparency="true"></iframe></div></div></div></div><div id="bottom-container" class="bottom-container "><div class="copy-right"><div class="baidu-info"><a class="sethome" href="https://www.baidu.com/cache/sethelp/index.html" target="_blank"><span>设为首页</span></a><span class="copyright-text"><span>©&nbsp;Baidu&nbsp;</span><a href="https://www.baidu.com/duty/" target="_blank">使用百度前必读</a>&nbsp;<a href="http://jianyi.baidu.com/" target="_blank">意见反馈</a>&nbsp;<span>京ICP证030173号&nbsp;</span><img width="13" height="16" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/copy_rignt_24.png"></span></div><div class="recordcode"><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11000002000001" target="_blank"><i></i>京公网安备11000002000001号</a></div></div><div class="back-to-top" data-showlog="tid:133;cst:1;logInfo:back_to_top;" data-clklog="tid:138;cst:2;logInfo:back_to_top;" data-extralog="rid:;pos:;extra:;isBaiJiaHao:1;login:1;" data-rid="back_to_top" style="visibility: visible;"><div class="icon-text"><span>返回顶部</span></div><div class="icon-arrow"><span></span></div></div></div></div><script type="text/javascript">window.onload = function () {var contentContainer = document.getElementById('content-container');var bottomContainer = document.getElementById('bottom-container');var rightContainer = document.getElementById('right-container');var minContentHeight = window.innerHeight - bottomContainer.offsetHeight;if (contentContainer.offsetHeight < minContentHeight) {contentContainer.style.height = minContentHeight + 'px';bottomContainer.className += ' fixed';}if (rightContainer.children.length === 0) {rightContainer.style.width = parseInt(rightContainer.offsetWidth + 1, 10) + 'px';}};</script><script type="text/javascript" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/jquery_0affbc1.js"></script><script type="text/javascript" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/land_min_pack_a8303a0.js"></script><script type="text/javascript" src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/index_31f386d.js"></script><script src="./EM算法的九层境界：Hinton和Jordan理解的EM算法_files/c.js"></script></body></html>