<!DOCTYPE html>
<!-- saved from url=(0037)https://zhuanlan.zhihu.com/p/42123894 -->
<html lang="zh" data-hairline="true" data-theme="light" data-focus-method="pointer"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>（概率）PCA和（变分）自编码器</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><title>知乎 - 有问题上知乎</title><meta name="description" content="有问题，上知乎。知乎是中文互联网知名知识分享平台，以「知识连接一切」为愿景，致力于构建一个人人都可以便捷接入的知识分享网络，让人们便捷地与世界分享知识、经验和见解，发现更大的世界。"><meta data-react-helmet="true" property="description" content="1.介绍主成分分析(PCA)和自编码器(AutoEncoders, AE)是无监督学习中的两种代表性方法。PCA的地位不必多说，只要是讲到降维的书，一定会把PCA放到最前面，它与LDA同为机器学习中最基础的线性降维算法，SVM/Logistic…"><meta data-react-helmet="true" property="og:title" content="（概率）PCA和（变分）自编码器"><meta data-react-helmet="true" property="og:url" content="http://zhuanlan.zhihu.com/p/42123894"><meta data-react-helmet="true" property="og:description" content="1.介绍主成分分析(PCA)和自编码器(AutoEncoders, AE)是无监督学习中的两种代表性方法。PCA的地位不必多说，只要是讲到降维的书，一定会把PCA放到最前面，它与LDA同为机器学习中最基础的线性降维算法，SVM/Logistic…"><meta data-react-helmet="true" property="og:image" content=""><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><link href="./（概率）PCA和（变分）自编码器_files/column.app.90f8f3b3dcb85a93acc9.css" rel="stylesheet"><script type="text/javascript" charset="utf-8" async="" src="./（概率）PCA和（变分）自编码器_files/column.modals.65b99577578f7387c73d.js"></script><script type="text/javascript" charset="utf-8" async="" src="./（概率）PCA和（变分）自编码器_files/column.richinput.f6500c16b34f2413f9aa.js"></script></head><body class=""><div id="root"><div class="App" data-reactroot=""><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;Wwhitenight&quot;}" data-zop="{&quot;authorName&quot;:&quot;DeAlVe&quot;,&quot;itemId&quot;:42123894,&quot;title&quot;:&quot;（概率）PCA和（变分）自编码器&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;42123894&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader is-fixed" style="width: 1840px; top: 0px; left: 0px;"><div class="ColumnPageHeader-content"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" class="Icon ZhihuLogo ZhihuLogo--blue Icon--logo" style="height:30px;width:64px" width="64" height="30" aria-hidden="true"><title></title><g><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="https://zhuanlan.zhihu.com/paperweekly"><img class="Avatar Avatar--round" width="30" height="30" src="./（概率）PCA和（变分）自编码器_files/v2-8ae80aa47b735441469b01414fc5dcce_is.jpg" srcset="https://pic3.zhimg.com/v2-8ae80aa47b735441469b01414fc5dcce_im.jpg 2x" alt="PaperWeekly"></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://zhuanlan.zhihu.com/paperweekly">PaperWeekly</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button FollowButton ColumnPageHeader-FollowButton Button--primary Button--blue">关注专栏</button><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button><div class="Popover"><button title="更多" type="button" id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content" class="Button ColumnPageHeader-MenuToggler Button--plain"><svg class="Zi Zi--Dots" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></button></div></div></div></div><div class="Sticky--holder" style="position: relative; top: 0px; right: 0px; bottom: 0px; left: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div></div><article class="Post-Main Post-NormalMain"><header class="Post-Header"><h1 class="Post-Title">（概率）PCA和（变分）自编码器</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="DeAlVe"><meta itemprop="image" content="https://pic4.zhimg.com/da8e974dc_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/dealve"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/dealve"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="./（概率）PCA和（变分）自编码器_files/da8e974dc_xs.jpg" srcset="https://pic4.zhimg.com/da8e974dc_l.jpg 2x" alt="DeAlVe"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/dealve">DeAlVe</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText ztext AuthorInfo-badgeText">研二，PR/ML方向，2020年毕业。</div></div></div></div></div><button type="button" class="Button FollowButton Button--primary Button--blue"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Plus FollowButton-icon" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.491 10.488s-.012-5.387 0-5.998c-.037-1.987-3.035-1.987-2.997 0-.038 1.912 0 5.998 0 5.998H4.499c-1.999.01-1.999 3.009 0 3.009s5.995-.01 5.995-.01v5.999c0 2.019 3.006 2.019 2.997 0-.01-2.019 0-5.998 0-5.998s3.996.009 6.004.009c2.008 0 2.008-3-.01-3.009h-5.994z" fill-rule="evenodd"></path></svg></span>关注</button></div><div><span class="Voters"><button type="button" class="Button Button--plain">46 人<!-- -->赞了该文章</button></span></div></header><div><div class="RichText ztext Post-RichText"><p></p><hr><h2>1.介绍</h2><p>主成分分析(PCA)和自编码器(AutoEncoders, AE)是无监督学习中的两种代表性方法。</p><p>PCA的地位不必多说，只要是讲到降维的书，一定会把PCA放到最前面，它与LDA同为机器学习中最基础的线性降维算法，SVM/Logistic Regression、PCA/LDA也是最常被拿来作比较的两组算法。</p><p>自编码器虽然不像PCA那般在教科书上随处可见，但是在早期被拿来做深度网络的逐层预训练，其地位可见一斑。尽管在ReLU、Dropout等神器出现之后，人们不再使用AutoEncoders来预训练，但它延伸出的稀疏AutoEncoders，降噪AutoEncoders等仍然被广泛用于表示学习。2017年的kaggle比赛“Porto Seguro’s Safe Driver Prediction”的<a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">冠军</a>就是使用了降噪AutoEncoders来做表示学习，最终以绝对优势击败了手工特征工程的选手们。</p><p>PCA和AutoEncoders都是非概率的方法，它们分别有一种对应的概率形式叫做概率PCA(Probabilistic PCA)和变分自编码器(Variational AE, VAE)，本文的主要目的就是整理一下PCA、概率PCA、AutoEncoders、变分AutoEncoders这四者的关系。</p><p>先放结论，后面就围绕这个表格展开：</p><figure><noscript><img src="https://pic1.zhimg.com/v2-273a5ac9f5e4e3d4d99612b327a37274_b.jpg" data-caption="" data-size="normal" data-rawwidth="748" data-rawheight="165" class="origin_image zh-lightbox-thumb" width="748" data-original="https://pic1.zhimg.com/v2-273a5ac9f5e4e3d4d99612b327a37274_r.jpg"></noscript><img src="./（概率）PCA和（变分）自编码器_files/v2-273a5ac9f5e4e3d4d99612b327a37274_hd.jpg" data-caption="" data-size="normal" data-rawwidth="748" data-rawheight="165" class="origin_image zh-lightbox-thumb lazy" width="748" data-original="https://pic1.zhimg.com/v2-273a5ac9f5e4e3d4d99612b327a37274_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-273a5ac9f5e4e3d4d99612b327a37274_b.jpg"></figure><h2>2.降维的线性方法和非线性方法</h2><p>降维分为线性降维和非线性降维，这是最普遍的分类方法。</p><p>PCA和LDA是最常见的线性降维方法，它们按照某种准则为数据集 <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1"> 找到一个最优投影方向 <img src="./（概率）PCA和（变分）自编码器_files/equation(1)" alt="W" eeimg="1"> 和截距和截距 <img src="./（概率）PCA和（变分）自编码器_files/equation(2)" alt="b" eeimg="1"> ，然后做变换 <img src="./（概率）PCA和（变分）自编码器_files/equation(3)" alt="z_i = Wx_i+b" eeimg="1"> 得到降维后的数据集 <img src="./（概率）PCA和（变分）自编码器_files/equation(4)" alt="\lbrace z_i \rbrace_{i=1}^n" eeimg="1"> 。因为 <img src="./（概率）PCA和（变分）自编码器_files/equation(3)" alt="z_i = Wx_i+b" eeimg="1"> 是一个线性变换（严格来说叫仿射变换，因为有截距项），所以这两种方法叫做线性降维。</p><p>非线性降维的两类代表方法是流形降维和AutoEncoders，这两类方法也体现出了两种不同角度的“非线性”。流形方法的非线性体现在它认为数据分布在一个低维流形上，而流形本身就是非线性的，流形降维的代表方法是两篇2000年的scinece论文提出的：多维放缩(multidimensional scaling, MDS)和局部线性嵌入(locally linear embedding, LLE)。（不得不说实在太巧了，两种流形方法发表在同一年的science上）</p><p>AutoEncoders的非线性和神经网络的非线性是一回事，都是利用堆叠非线性激活函数来近似任意函数。事实上，AutoEncoders就是一种神经网络，只不过它的输入和输出相同，真正有意义的地方不在于网络的输出，而是在于网络的权重。</p><h2>3. 降维的生成式方法和非生成式方法</h2><h2>3.1 两类方法</h2><p>降维还可以分为生成式方法（概率方法）接非生成式方法（非概率方法）。</p><p>教科书对PCA的推导一般是基于最小化重建误差或者最大化可分性的，或者说是通过提取数据集的结构信息来建模一个约束最优化问题来推导的。事实上，PCA还有一种概率形式的推导，那就是概率PCA，PRML里面有对概率PCA的详细讲解，感兴趣的读者可以去阅读。需要注意的是，概率PCA不是PCA的变体，它就是PCA本身，概率PCA是从另一种角度来推导和理解PCA，它把PCA纳入了生成式的框架。</p><p>分类中的生成式方法和判别式方法已经为大家所熟知，其实降维中也有生成式方法和非生成式方法。首先来看一下降维问题的形式化描述：</p><p>设 <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1"> 是我们拿到的数据集，我们的目的是得到数据集中每个样本的低维表示 <img src="./（概率）PCA和（变分）自编码器_files/equation(4)" alt="\lbrace z_i \rbrace_{i=1}^n" eeimg="1">，其中 <img src="./（概率）PCA和（变分）自编码器_files/equation(5)" alt="dim(z_i) &lt; dim(x_i)" eeimg="1"> 。</p><p>降维的非生成式方法不需要概率知识，而是直接利用数据集 <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1">的结构信息建模一个最优化问题，然后求解这个问题得到 <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1">对应的 <img src="./（概率）PCA和（变分）自编码器_files/equation(4)" alt="\lbrace z_i \rbrace_{i=1}^n" eeimg="1">。</p><p>降维的生成式方法认为数据集 <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1"> 是对一个随机变量 <img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1"> 的 <img src="./（概率）PCA和（变分）自编码器_files/equation(7)" alt="n" eeimg="1"> 次采样，而随机变量<img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1">依赖于随机变量 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> ，对 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 进行建模：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(9)" alt="z \sim p_\theta(z) \\" eeimg="1"> </p><p>再对这个依赖关系进行建模：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(10)" alt="x|z \sim p_\theta(x|z)\\" eeimg="1"> </p><p>有了这两个公式，我们就可以表达出随机变量 <img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1"> 的分布：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(11)" alt="x \sim p_\theta(x) = \int_Z p_\theta(x|z)p_\theta(z)dz\\" eeimg="1"> </p><p>随后我们利用数据集 <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1"> 对分布的参数 <img src="./（概率）PCA和（变分）自编码器_files/equation(12)" alt="\theta" eeimg="1"> 进行估计，就得到这几个分布。好了，设定了这么多，可是降维降在哪里了呢，为什么没有看到？？？</p><p>回想一下降维的定义：降维就是给定一个高维样本 <img src="./（概率）PCA和（变分）自编码器_files/equation(13)" alt="x_i" eeimg="1"> ，给出对应的低维表示 <img src="./（概率）PCA和（变分）自编码器_files/equation(14)" alt="z_i" eeimg="1"> ，这恰好就是 <img src="./（概率）PCA和（变分）自编码器_files/equation(15)" alt="p(z|x)" eeimg="1"> 的含义！所以我们只要应用Bayes定理求出这个概率即可：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(16)" alt="p_\theta(z|x)=\frac{p_\theta(x|z)p_\theta(z)}{\int_Z p_\theta(x|z)p_\theta(z)dz} \\" eeimg="1"> </p><p>这样我们就可以得到每个样本点 <img src="./（概率）PCA和（变分）自编码器_files/equation(13)" alt="x_i" eeimg="1"> 上的 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(17)" alt="p(z|x=x_i)" eeimg="1"> ，可以选择这个分布的峰值点作为 <img src="./（概率）PCA和（变分）自编码器_files/equation(14)" alt="z_i" eeimg="1">，降维就完成了。</p><p>Q：那么问题来了，生成式方法和非生成式方法哪个好呢？</p><p>A：当然是非生成式方法好了，一两行就能设定完，君不见生成式方法你设定了一大段？？？</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(18)" alt="\uparrow" eeimg="1"> 应该会有很多人这样想吧？事实也的确如此，上面这个回答<b>在一定意义上</b>是正确的。如果你只是为了对现有的数据 <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1"> 进行降维，而没有其他需求，那么简单粗暴的非生成式方法当然是更好的选择。</p><p>那么，在什么情况下，或者说什么需求下，生成式方法是更好的选择更好呢？答案就蕴含在“生成式”这个名称中：在需要生成新样本的情况下，生成式方法是更好的选择。</p><h2>3.2 生成式方法的应用场景</h2><p>相似图片生成就是一种最常见的应用场景，现在我们考虑生成MNIST风格的手写体数字。假设 <img src="./（概率）PCA和（变分）自编码器_files/equation(13)" alt="x_i" eeimg="1"> 代表一张图片， <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1"> 是整个MNIST数据集，我们该怎样建模才能生成一张新图片呢？</p><p>最容易想到的方法就是：对 <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1"> 进行KDE(核密度估计)得到 <img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1"> 的分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(19)" alt="p(x)" eeimg="1"> ，如果顺利的话 <img src="./（概率）PCA和（变分）自编码器_files/equation(19)" alt="p(x)" eeimg="1"> 应该是一个10峰分布，一个峰代表一个数字，从对应的峰中采样一个样本 <img src="./（概率）PCA和（变分）自编码器_files/equation(20)" alt="x^{new}" eeimg="1"> ，它就代表了相应的数字。</p><p>是不是看起来很简单，然而 <img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1"> 的维度太高（等于MNIST的分辨率, <img src="./（概率）PCA和（变分）自编码器_files/equation(21)" alt="28\times28=784" eeimg="1"> ），每一维中包含的信息又十分有限，直接对 <img src="./（概率）PCA和（变分）自编码器_files/equation" alt="\lbrace x_i \rbrace_{i=1}^n" eeimg="1"> 进行KDE完全没有可行性，所以更好的方法是先对数据集进行降维得到 <img src="./（概率）PCA和（变分）自编码器_files/equation(4)" alt="\lbrace z_i \rbrace_{i=1}^n" eeimg="1"> ，然后再对 <img src="./（概率）PCA和（变分）自编码器_files/equation(4)" alt="\lbrace z_i \rbrace_{i=1}^n" eeimg="1"> 进行KDE，再从 <img src="./（概率）PCA和（变分）自编码器_files/equation(22)" alt="p(z)" eeimg="1"> 中采样 <img src="./（概率）PCA和（变分）自编码器_files/equation(23)" alt="z^{new}" eeimg="1"> 并通过逆变换得到 <img src="./（概率）PCA和（变分）自编码器_files/equation(20)" alt="x^{new}" eeimg="1"> 。</p><p>这样做当然也是可以的，但是依然存在严重的问题。上面的方法相当于把新样本生成拆分成了降维、KDE和采样这三个步骤。降维这一步骤可以使用PCA或者AutoEncoders等方法，这一步不会有什么问题。存在严重问题的步骤是KDE和采样。回想一下KDE其实是一种懒惰学习方法，每来一个样本 <img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1"> ，它就会计算一下这个样本和数据集中每一个样本 <img src="./（概率）PCA和（变分）自编码器_files/equation(13)" alt="x_i" eeimg="1"> 的核距离 <img src="./（概率）PCA和（变分）自编码器_files/equation(24)" alt="k(\frac{x-x_i}{h})" eeimg="1"> ，然后估计出这一点的密度。这就意味着我们需要把 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 所属的空间划分成网格，估计每个网格点上的密度，才能近似得到 <img src="./（概率）PCA和（变分）自编码器_files/equation(25)" alt=" p(z)" eeimg="1"> ，计算复杂度是 <img src="./（概率）PCA和（变分）自编码器_files/equation(26)" alt="O(n*grid\_scale)" eeimg="1">，而<img src="./（概率）PCA和（变分）自编码器_files/equation(27)" alt="grid\_scale" eeimg="1">关于 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的维数是指数级的，这个计算复杂度是十分恐怖的。即使得到了近似的 <img src="./（概率）PCA和（变分）自编码器_files/equation(22)" alt="p(z)" eeimg="1"> ，从这样一个没有解析形式的分布中采样也是很困难的，依然只能求助于网格点近似。因此，KDE和采样这两步无论是计算效率还是计算精度都十分堪忧。</p><p>这时候就要求助于生成式方法了。注意到生成式方法中建模了 <img src="./（概率）PCA和（变分）自编码器_files/equation(28)" alt="p_\theta(z)" eeimg="1"> 和 <img src="./（概率）PCA和（变分）自编码器_files/equation(29)" alt="p_\theta(x|z)" eeimg="1"> ，一旦求出了参数 <img src="./（概率）PCA和（变分）自编码器_files/equation(12)" alt="\theta" eeimg="1"> ，我们就得到了变量 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的解析形式的分布！只要从 <img src="./（概率）PCA和（变分）自编码器_files/equation(28)" alt="p_\theta(z)" eeimg="1"> 中采样出一个 <img src="./（概率）PCA和（变分）自编码器_files/equation(23)" alt="z^{new}" eeimg="1"> ，再取 <img src="./（概率）PCA和（变分）自编码器_files/equation(30)" alt="p_\theta(x|z=z^{new})" eeimg="1"> 的峰值作为我们的 <img src="./（概率）PCA和（变分）自编码器_files/equation(20)" alt="x^{new}" eeimg="1"> ，新样本生成就完成了。</p><p>在需要生成新样本时，非生成式方法需要对 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的概率分布进行代价巨大的数值逼近，然后才能从分布中采样；生成式方法本身就对 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的概率分布进行了建模，因此可以直接从分布中进行采样。所以，在需要生成新样本时，生成式方法是更好的选择，甚至是必然的选择。</p><h2>4. （概率）PCA和（变分）AutoEncoders</h2><p>下面简单整理一下这四种降维方法。注意一些术语，编码=降维，解码=重建，原数据=观测变量，降维后的数据=隐变量。</p><h2>4.1 PCA</h2><p>原数据：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(31)" alt="x_i \in R^d\\" eeimg="1"> </p><p>编码后的数据： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(32)" alt="z_i = W^T(x_i+b) \in R^c\\" eeimg="1"> </p><p>解码后的数据： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(33)" alt="\hat{x}_i = Wz_i - b \in R^d\\" eeimg="1"> </p><p>重建误差：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(34)" alt="\sum_{i=1}^n||x_i - \hat{x}_i||_p^p\\" eeimg="1"> </p><p>最小化重建误差，就可以得到 <img src="./（概率）PCA和（变分）自编码器_files/equation(1)" alt="W" eeimg="1"> 和 <img src="./（概率）PCA和（变分）自编码器_files/equation(2)" alt="b" eeimg="1"> 的最优解&amp;解析解，PCA的求解就完成了。</p><p><br></p><p>补充说明：</p><p>PCA中的 <img src="./（概率）PCA和（变分）自编码器_files/equation(35)" alt="p=2" eeimg="1"> ，即最小化二范数意义下的重建误差，如果 <img src="./（概率）PCA和（变分）自编码器_files/equation(36)" alt="p=1" eeimg="1"> 的话我们就得到了鲁棒PCA(Robust PCA)。而最小化误差的二范数等价于对高斯噪声的MLE，最小化误差的一范数等价于对拉普拉斯噪声的MLE。因此，PCA其实是在假设存在高斯噪声的条件下对数据集进行重建，这个高斯误差就是我们将要在§4.3的概率PCA中提到的 <img src="./（概率）PCA和（变分）自编码器_files/equation(37)" alt="\epsilon" eeimg="1"> 。你看，即使不是概率PCA，其中也隐含着概率的思想。</p><p>编码和解码用到的 <img src="./（概率）PCA和（变分）自编码器_files/equation(1)" alt="W" eeimg="1"> 和 <img src="./（概率）PCA和（变分）自编码器_files/equation(2)" alt="b" eeimg="1"> 是一样的，即编码过程和解码过程是对称的，这一点与下面要讲的AutoEncoders是不同的。</p><p>求解上述最优化问题可以得到 <img src="./（概率）PCA和（变分）自编码器_files/equation(38)" alt="b = - \frac{1}{n}\sum_{i=1}^n x_i" eeimg="1"> ，这恰好是样本均值的相反数。也就是说，PCA中截距项的含义是让每个样本都减去样本均值，这正是“样本中心化”的含义。既然我们已经知道求出来的截距就是样本均值，所以干脆一开始就对样本进行中心化，这样在使用PCA的时候就可以忽略截距项 <img src="./（概率）PCA和（变分）自编码器_files/equation(2)" alt="b" eeimg="1"> 而直接使用 <img src="./（概率）PCA和（变分）自编码器_files/equation(39)" alt="z_i=W^Tx_i" eeimg="1"> ，变量就只剩下 <img src="./（概率）PCA和（变分）自编码器_files/equation(1)" alt="W" eeimg="1"> 了。教科书上讲解PCA时一般都是上来就说“使用PCA之前需要进行样本中心化”，但是没有人告诉我们为什么要这样做，现在大家应该明白为什么要进行中心化了吧。</p><h2>4.2 AutoEncoders</h2><p>原数据： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(31)" alt="x_i \in R^d\\" eeimg="1"> </p><p>编码后的数据： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(40)" alt="z_i = \sigma (W^Tx_i+b) \in R^c\\" eeimg="1"> </p><p>解码后的数据： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(41)" alt="\hat{x}_i = \hat{\sigma} (\hat{W}z_i+\hat{b}) \in R^d\\" eeimg="1"> </p><p>重建误差： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(34)" alt="\sum_{i=1}^n||x_i - \hat{x}_i||_p^p\\" eeimg="1"> </p><p>最小化重建误差，利用反向传播算法可以得到 <img src="./（概率）PCA和（变分）自编码器_files/equation(42)" alt="W、b、\hat{W}、\hat{b}" eeimg="1"> 的局部最优解&amp;数值解，AutoEncoders的求解完成。</p><p><br></p><p>补充说明：</p><p>这里可以使用任意范数，每一个范数都代表我们对数据的一种不同的假设。为了和PCA对应，我们也取 <img src="./（概率）PCA和（变分）自编码器_files/equation(35)" alt="p=2" eeimg="1"> 。</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(43)" alt="\sigma (\cdot)" eeimg="1"> 是非线性激活函数。AutoEncoder一般都会堆叠多层，方便起见我们只写了一层。</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(1)" alt="W" eeimg="1"> 和 <img src="./（概率）PCA和（变分）自编码器_files/equation(44)" alt="\hat{W}" eeimg="1"> 完全不是一个东西，这是因为经过非线性变换之后我们已经无法将样本再用原来的基 <img src="./（概率）PCA和（变分）自编码器_files/equation(1)" alt="W" eeimg="1"> 进行表示了，必须要重新训练解码的基 <img src="./（概率）PCA和（变分）自编码器_files/equation(44)" alt="\hat{W}" eeimg="1"> 。甚至，AutoEncoders的编码器和解码器堆叠的层数都可以不同，例如可以用4层来编码，用3层来解码。</p><h2>4.3 概率PCA</h2><p>隐变量边缘分布： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(45)" alt="p(z)=N(z|0,I)\\" eeimg="1"> </p><p>观测变量条件分布： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(46)" alt="p_\theta(x|z) = N(x|f(z;\theta), \sigma^2I)\\" eeimg="1"> </p><p>确定函数： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(47)" alt="f(z;\theta) = Wz+\mu\\" eeimg="1"> </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1"> 的生成过程：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(48)" alt="x = Wz + \mu + \epsilon, 其中\epsilon \sim N(0, \sigma^2I)\\" eeimg="1"> </p><p><br></p><p>因为 <img src="./（概率）PCA和（变分）自编码器_files/equation(22)" alt="p(z)" eeimg="1"> 和 <img src="./（概率）PCA和（变分）自编码器_files/equation(29)" alt="p_\theta(x|z)" eeimg="1"> 都是高斯分布，且 <img src="./（概率）PCA和（变分）自编码器_files/equation(29)" alt="p_\theta(x|z)" eeimg="1"> 的均值 <img src="./（概率）PCA和（变分）自编码器_files/equation(49)" alt="f(z;\theta) = Wz+\mu" eeimg="1"> 是 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的线性函数，所以这是一个线性高斯模型。线性高斯模型有一个非常重要的性质： <img src="./（概率）PCA和（变分）自编码器_files/equation(50)" alt="p_\theta(x)" eeimg="1"> 和 <img src="./（概率）PCA和（变分）自编码器_files/equation(51)" alt="p_\theta(z|x)" eeimg="1"> 也都是高斯分布！千万不要小瞧这个性质，这个性质保证了我们能够使用极大似然估计或者EM算法来求解PCA。如果没有这个性质的话，我们就只能借助变分法（变分AE采用的）或者对抗训练（GAN采用的）来近似 <img src="./（概率）PCA和（变分）自编码器_files/equation(50)" alt="p_\theta(x)" eeimg="1">和 <img src="./（概率）PCA和（变分）自编码器_files/equation(51)" alt="p_\theta(z|x)" eeimg="1"> 了。有了这个优秀的性质之后，我们至少有三种方法可以求解概率PCA：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(52)" alt="p_\theta(x)=\int_Zp(z)p_\theta(x|z)dz\\" eeimg="1"> </p><p>是一个形式已知，仅参数未知的高斯分布，因此可以用极大似然估计来求解 <img src="./（概率）PCA和（变分）自编码器_files/equation(12)" alt="\theta" eeimg="1"> 。</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(16)" alt="p_\theta(z|x)=\frac{p_\theta(x|z)p_\theta(z)}{\int_Z p_\theta(x|z)p_\theta(z)dz} \\" eeimg="1"> </p><p>也是一个形式已知，仅参数未知的高斯分布，因此可以用EM算法来求解 <img src="./（概率）PCA和（变分）自编码器_files/equation(12)" alt="\theta" eeimg="1"> ，顺便还能得到隐变量 <img src="./（概率）PCA和（变分）自编码器_files/equation(14)" alt="z_i" eeimg="1"> 。</p><p>如果你足够无聊，甚至也可以引入一个变分分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(53)" alt="q_\phi(z|x)" eeimg="1"> 来求解概率PCA...不过似乎没什么意义，也算是一种方法吧。</p><p>一旦求出了<img src="./（概率）PCA和（变分）自编码器_files/equation(12)" alt="\theta" eeimg="1"> ，我们就得到了所有的四个概率： </p><p><br></p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(54)" alt="p(z)、p_\theta(x|z)、p_\theta(x)、p_\theta(z|x)\\" eeimg="1"> </p><p><br></p><p>有了这四个概率，我们就可以做这些事情了：</p><p>1.降维：给定样本 <img src="./（概率）PCA和（变分）自编码器_files/equation(13)" alt="x_i" eeimg="1"> ，就得到了分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(55)" alt="p_\theta(z|x=x_i)" eeimg="1"> ，取这个分布的峰值点 <img src="./（概率）PCA和（变分）自编码器_files/equation(14)" alt="z_i" eeimg="1"> 就是降维后的数据。</p><p>2.重建：给定降维后的样本 <img src="./（概率）PCA和（变分）自编码器_files/equation(14)" alt="z_i" eeimg="1"> ，就得到了分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(56)" alt="p_\theta(x|z=z_i)" eeimg="1">，取这个分布的峰值点 <img src="./（概率）PCA和（变分）自编码器_files/equation(13)" alt="x_i" eeimg="1"> 就是重建后的数据。</p><p>3.生成：从分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(22)" alt="p(z)" eeimg="1"> 中采样一个 <img src="./（概率）PCA和（变分）自编码器_files/equation(23)" alt="z^{new}" eeimg="1"> ，就得到了分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(30)" alt="p_\theta(x|z=z^{new})" eeimg="1"> ，取这个分布的峰值点 <img src="./（概率）PCA和（变分）自编码器_files/equation(20)" alt="x^{new}" eeimg="1"> 就是新生成的数据。</p><p>4.密度估计：给定样本 <img src="./（概率）PCA和（变分）自编码器_files/equation(13)" alt="x_i" eeimg="1"> ，就得到了这一点的概率密度 <img src="./（概率）PCA和（变分）自编码器_files/equation(57)" alt="p_\theta(x=x_i)" eeimg="1"> 。</p><p>PCA只能做到1和2，对3和4无力，这一点我们已经分析过了。</p><p><br></p><p>Q：为什么隐变量要取单位高斯分布（标准正态分布）？</p><p>A：这是两个问题。</p><p>        subQ1：为什么要取高斯分布？</p><p>        subA1：为了求解方便，如果不取高斯分布，那么 <img src="./（概率）PCA和（变分）自编码器_files/equation(50)" alt="p_\theta(x)" eeimg="1"> 有很大的可能没有解析解，这会给求解带来很大的麻烦。还有一个原因，回想生成新样本的过程，要首先从 <img src="./（概率）PCA和（变分）自编码器_files/equation(22)" alt="p(z)" eeimg="1"> 中采样一个 <img src="./（概率）PCA和（变分）自编码器_files/equation(23)" alt="z^{new}" eeimg="1"> ，高斯分布采样简单。</p><p>        subQ2：为什么是零均值单位方差的？</p><p>        subA2：完全可以取任意均值和方差，但是我们要将 <img src="./（概率）PCA和（变分）自编码器_files/equation(22)" alt="p(z)" eeimg="1"> 和 <img src="./（概率）PCA和（变分）自编码器_files/equation(29)" alt="p_\theta(x|z)" eeimg="1"> 相乘，均值和方差部分可以挪到 <img src="./（概率）PCA和（变分）自编码器_files/equation(58)" alt="f(z;\theta)" eeimg="1"> 中，所以 <img src="./（概率）PCA和（变分）自编码器_files/equation(22)" alt="p(z)" eeimg="1"> 的均值和方差取多少都无所谓，方便起见就取单位均值方差了。</p><p><br></p><p>Q： <img src="./（概率）PCA和（变分）自编码器_files/equation(29)" alt="p_\theta(x|z)" eeimg="1"> 为什么选择了高斯分布呢？</p><p>A：因为简单，和上一个问题的一样。还有一个直觉的解释是 <img src="./（概率）PCA和（变分）自编码器_files/equation(29)" alt="p_\theta(x|z)" eeimg="1"> 认为 <img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1"> 是由 <img src="./（概率）PCA和（变分）自编码器_files/equation(59)" alt="f(z:\theta)" eeimg="1"> 和噪声 <img src="./（概率）PCA和（变分）自编码器_files/equation(37)" alt="\epsilon" eeimg="1"> 加和而成的，如果 <img src="./（概率）PCA和（变分）自编码器_files/equation(37)" alt="\epsilon" eeimg="1"> 是高斯分布的话，恰好和PCA的二范数重建误差相对应，这也算是一个佐证吧。</p><p><br></p><p>Q： <img src="./（概率）PCA和（变分）自编码器_files/equation(29)" alt="p_\theta(x|z)" eeimg="1"> 的方差为什么选择了各向同性的 <img src="./（概率）PCA和（变分）自编码器_files/equation(60)" alt="\sigma^2I" eeimg="1"> 而不是更一般的 <img src="./（概率）PCA和（变分）自编码器_files/equation(61)" alt="\Sigma" eeimg="1"> 呢？</p><p>A：方差可以选择一般的 <img src="./（概率）PCA和（变分）自编码器_files/equation(61)" alt="\Sigma" eeimg="1"> ，但是 <img src="./（概率）PCA和（变分）自编码器_files/equation(62)" alt="d^2" eeimg="1"> 个参数一定会给求解带来困难，所导出的方法虽然也是线性降维，但它已经不是PCA了，而是另外的方法（我也不知道是什么方法）。方差也可以选择成一个的各向异性的对角阵 <img src="./（概率）PCA和（变分）自编码器_files/equation(63)" alt="\Lambda" eeimg="1"> ，这样只有 <img src="./（概率）PCA和（变分）自编码器_files/equation(64)" alt="d" eeimg="1"> 个参数，事实上这就是因子分析，另一种线性降维方法。只有当方差选择成各向同性的对角阵 <img src="./（概率）PCA和（变分）自编码器_files/equation(60)" alt="\sigma^2I" eeimg="1"> 时，导出来的方法才叫主成分分析，这个地方PRML里有介绍。</p><h2>4.4 变分AutoEncoders</h2><p>隐变量边缘分布： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(45)" alt="p(z)=N(z|0,I)\\" eeimg="1"> </p><p>观测变量条件分布： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(65)" alt="p_\theta(x|z)=N(x|f(z;\theta), \sigma^2I)\\" eeimg="1"> </p><p>确定函数： </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(66)" alt="f(z;\theta)=\sigma(Wz+\mu) \\" eeimg="1"> </p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1"> 的生成过程：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(67)" alt="x= f(z;\theta) + \epsilon, 其中\epsilon \sim N(0,\sigma^2I)\\" eeimg="1"> </p><p><br></p><p>因为 <img src="./（概率）PCA和（变分）自编码器_files/equation(58)" alt="f(z;\theta)" eeimg="1"> 是 <img src="./（概率）PCA和（变分）自编码器_files/equation(68)" alt="z " eeimg="1"> 的非线性函数，所以这不再是一个线性高斯模型。观测变量的边缘分布：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(69)" alt="p_\theta(x) = \int_Zp(z)p_\theta(x|z)dz\\" eeimg="1"> </p><p>没有解析形式。这就意味着我们无法直接使用极大似然估计来求解参数 <img src="./（概率）PCA和（变分）自编码器_files/equation(12)" alt="\theta" eeimg="1"> 。更加绝望的是，隐变量的后验分布：</p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(70)" alt="p_\theta(z|x)=\frac{p_\theta(x|z)p_\theta(z)}{\int_Z p_\theta(x|z)p_\theta(z)dz}\\" eeimg="1"> </p><p>也没有解析形式（这是当然，因为分母没有解析形式了）。这就意味着我们也无法通过EM算法来估计参数和求解隐变量。</p><p>那么，建出来的模型该怎么求解呢？这就需要上变分推断(Variational Inference)，又称变分贝叶斯(Variational Bayes)了。本文不打算细讲变分推断，仅仅讲一下大致的流程。</p><p>变分推断会引入一个变分分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(53)" alt="q_\phi(z|x)" eeimg="1"> 来近似没有解析形式的后验概率 <img src="./（概率）PCA和（变分）自编码器_files/equation(51)" alt="p_\theta(z|x)" eeimg="1"> 。在变分AE的原文中，作者使用了SGD来同时优化参数 <img src="./（概率）PCA和（变分）自编码器_files/equation(12)" alt="\theta" eeimg="1"> 和 <img src="./（概率）PCA和（变分）自编码器_files/equation(71)" alt="\phi" eeimg="1"> 。一旦求出了这两个参数就可以得到这些概率：</p><p><br></p><p><img src="./（概率）PCA和（变分）自编码器_files/equation(72)" alt="p(z)、p_\theta(x|z)、q_\phi(z|x)\\" eeimg="1"> </p><p>注意因为 <img src="./（概率）PCA和（变分）自编码器_files/equation(50)" alt="p_\theta(x)" eeimg="1"> 和 <img src="./（概率）PCA和（变分）自编码器_files/equation(51)" alt="p_\theta(z|x)" eeimg="1"> 没有解析形式，所以即使求出了 <img src="./（概率）PCA和（变分）自编码器_files/equation(12)" alt="\theta" eeimg="1"> 我们也无法获得这两个概率。但是，正如上面说的， <img src="./（概率）PCA和（变分）自编码器_files/equation(53)" alt="q_\phi(z|x)" eeimg="1"> 就是 <img src="./（概率）PCA和（变分）自编码器_files/equation(51)" alt="p_\theta(z|x)" eeimg="1"> 的近似，所以需要用<img src="./（概率）PCA和（变分）自编码器_files/equation(51)" alt="p_\theta(z|x)" eeimg="1">的地方都可以用 <img src="./（概率）PCA和（变分）自编码器_files/equation(73)" alt="q_\phi(z|x) " eeimg="1"> 代替。</p><p><br></p><p>有了这三个概率，我们就可以做这些事情了：</p><p>1.降维：给定样本 <img src="./（概率）PCA和（变分）自编码器_files/equation(13)" alt="x_i" eeimg="1"> ，就得到了分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(74)" alt="q_\phi(z|x=x_i)" eeimg="1"> ，取这个分布的峰值点 <img src="./（概率）PCA和（变分）自编码器_files/equation(14)" alt="z_i" eeimg="1"> 就是降维后的数据。</p><p>2.重建：给定降维后的样本<img src="./（概率）PCA和（变分）自编码器_files/equation(14)" alt="z_i" eeimg="1"> ，就得到了分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(56)" alt="p_\theta(x|z=z_i)" eeimg="1">，取这个分布的峰值点 <img src="./（概率）PCA和（变分）自编码器_files/equation(13)" alt="x_i" eeimg="1"> 就是重建后的数据。</p><p>3.生成：从分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(22)" alt="p(z)" eeimg="1"> 中采样一个 <img src="./（概率）PCA和（变分）自编码器_files/equation(23)" alt="z^{new}" eeimg="1"> ，就得到了分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(30)" alt="p_\theta(x|z=z^{new})" eeimg="1"> ，取这个分布的峰值点 <img src="./（概率）PCA和（变分）自编码器_files/equation(20)" alt="x^{new}" eeimg="1"> 就是新生成的数据。</p><p>与概率PCA不同的是，这里无法解析地得到 <img src="./（概率）PCA和（变分）自编码器_files/equation(75)" alt="p_\theta(x_i)" eeimg="1"> ，进行密度估计需要进行另外的设计，通过采样得到，计算代价还是比较大的，具体步骤变分AE的原文中有介绍。</p><p>AutoEncoders只能做到1和2，对3无力。</p><h2>4.5 对比</h2><p>1. 从§4.1和§4.2可以看出，PCA实际上就是线性Autoencoders。两者无论是编码解码形式还是重建误差形式都完全一致，只有是否线性的区别。线性与否给优化求解带来了不同性质：PCA可以直接得到最优的解析解，而AutoEncoders只能通过反向传播得到局部最优的数值解。</p><p>2. 从§4.3和§4.4可以看出，概率PCA和变分AutoEncoders的唯一区别就是 <img src="./（概率）PCA和（变分）自编码器_files/equation(58)" alt="f(z;\theta)" eeimg="1"> 是否是 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的线性函数，但是这个区别给优化求解带来了巨大的影响。在概率PCA中， <img src="./（概率）PCA和（变分）自编码器_files/equation(58)" alt="f(z;\theta)" eeimg="1"> 是线性的，所以我们得到了一个线性高斯模型，线性高斯模型的优秀性质是牵扯到的4个概率都是高斯分布，所以我们可以直接给出边缘分布和编码分布的解析形式，极大似然估计和EM算法都可以使用，一切处理都非常方便。在变分AutoEncoders中， <img src="./（概率）PCA和（变分）自编码器_files/equation(58)" alt="f(z;\theta)" eeimg="1"> 是非线性的，所以边缘分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(76)" alt="p_\theta(x)=\int_Zp(z)p_\theta(x|z)dz" eeimg="1"> 不再有解析形式，极大似然估计无法使用；编码分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(77)" alt="p_\theta(z|x)=p_\theta(x,z)/ p_\theta(x)" eeimg="1"> 也不再有解析形式，EM算法无法使用，我们只能求助于变分推断，得到编码分布的近似 <img src="./（概率）PCA和（变分）自编码器_files/equation(53)" alt="q_\phi(z|x)" eeimg="1"> ，再利用别的技巧得到边缘分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(50)" alt="p_\theta(x)" eeimg="1"> 的估计。</p><p>3. 从§4.1和§4.3可以看出，PCA和概率PCA中 <img src="./（概率）PCA和（变分）自编码器_files/equation(6)" alt="x" eeimg="1"> 都是 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的线性函数，只不过概率PCA显式地把高斯噪声 <img src="./（概率）PCA和（变分）自编码器_files/equation(37)" alt="\epsilon" eeimg="1"> 写在了表达式中；PCA没有显式写出噪声，而是把高斯噪声隐含在了二范数重建误差中。</p><p>4. 从§4.2和§4.4可以看出，AE和VAE的最重要的区别在于VAE迫使隐变量 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 满足高斯分布 <img src="./（概率）PCA和（变分）自编码器_files/equation(78)" alt="p(z)=N(z|0,I)" eeimg="1"> ，而AE对 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的分布没有做任何假设。这个区别使得在生成新样本时，AE需要先数值拟合 <img src="./（概率）PCA和（变分）自编码器_files/equation(22)" alt="p(z)" eeimg="1"> ，才能生成符合数据集分布的隐变量，而VAE直接从 <img src="./（概率）PCA和（变分）自编码器_files/equation(79)" alt="N(z|0,I)" eeimg="1"> 中采样一个 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> ，它天然就符合数据集分布。事实上，这是因为在使用变分推断进行优化时，VAE迫使 <img src="./（概率）PCA和（变分）自编码器_files/equation(8)" alt="z" eeimg="1"> 的分布向<img src="./（概率）PCA和（变分）自编码器_files/equation(79)" alt="N(z|0,I)" eeimg="1">靠近，不过本文中没有讲优化细节，VAE的原文中有详细的解释。</p><p>5. PCA求解简单，但是它是线性降维，提取信息的能力有限；非线性的AE提取信息的能力强，但是求解复杂。要根据不同的场景选择不同的降维算法。</p><p>6. 要生成新样本时，不能选择PCA或AE，而是要选择概率PCA或VAE。</p><h2>5. 总结</h2><p>本文将降维按照是否线性、是否生成式划分，将PCA、概率PCA、AutoEncoders和变分AutoEncoders纳入了这个划分框架中，并分析了四种算法的内在联系。</p><p><br></p><p>注：这篇文章昨天在简书上写的，今天自己搬到知乎来了。</p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/3085936e8bc3" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage" data-za-detail-view-id="172"><span class="LinkCard-content"><span><span class="LinkCard-title" data-text="true">https://www.jianshu.com/p/3085936e8bc3</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>www.jianshu.com</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><p>更新：又写了一篇分析LDA的，欢迎大家阅读指正：</p><a target="_blank" href="https://zhuanlan.zhihu.com/p/42238953" data-draft-node="block" data-draft-type="link-card" data-image="https://pic1.zhimg.com/equation.jpg" data-image-width="0" data-image-height="0" class="LinkCard LinkCard--hasImage" data-za-detail-view-id="172"><span class="LinkCard-backdrop" style="background-image:url(https://pic1.zhimg.com/equation.jpg)"></span><span class="LinkCard-content"><span><span class="LinkCard-title" data-text="true">DeAlVe：教科书上的LDA为什么长这个样子？</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>zhuanlan.zhihu.com</span></span><span class="LinkCard-imageCell"><img class="LinkCard-image LinkCard-image--square" alt="图标" src="./（概率）PCA和（变分）自编码器_files/equation.jpg"></span></span></a><p></p></div></div><div class="ContentItem-time"><a target="_blank" href="http://zhuanlan.zhihu.com/p/42123894"><span data-tooltip="发布于 2018-08-15 18:24">编辑于 2018-08-17</span></a></div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19559450&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content">机器学习</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19564812&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19564812" target="_blank"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content">模式识别</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 575px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;42123894&quot;}}}"><span><button aria-label="赞同" type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 <!-- -->46</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>2 条评论</button><div class="Popover ShareMenu"><div class="" id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div data-za-detail-view-path-module="LeftTabBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;42123894&quot;}}}"><div><div class="Post-SideActions" style="opacity: 1;"><button class="like"><div class="Post-SideActions-icon"><svg class="Zi Zi--TriangleUp Post-SideActions-upIcon" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></div><div class="likeCount"><div class="likeCount-inner" data-previous="已赞同 47">赞同 46</div></div></button><div class="Popover ShareMenu"><div class="" id="Popover17-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover17-content"><button><div class="Post-SideActions-icon"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share" fill="currentColor" viewBox="0 0 24 24" width="20" height="20"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span></div>分享</button></div></div></div></div></div></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: 0px; left: 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div><div class="PostIndex-Contributes" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://zhuanlan.zhihu.com/paperweekly"><div class="Popover"><div id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="./（概率）PCA和（变分）自编码器_files/v2-8ae80aa47b735441469b01414fc5dcce_xs.jpg" srcset="https://pic3.zhimg.com/v2-8ae80aa47b735441469b01414fc5dcce_l.jpg 2x" alt="PaperWeekly"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://zhuanlan.zhihu.com/paperweekly"><div class="Popover"><div id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content">PaperWeekly</div></div></a></h2><div class="ContentItem-meta">PaperWeekly是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。</div></div><div class="ContentItem-extra"><button type="button" class="Button FollowButton Button--primary Button--blue">关注专栏</button></div></div></div></ul></div><div class="Recommendations-Main" style="width: 1840px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled=""><svg class="Zi Zi--ArrowLeft" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M14.782 16.78a.737.737 0 0 1-1.052 0L9.218 12.53a.758.758 0 0 1 0-1.063L13.73 7.22a.737.737 0 0 1 1.052 0c.29.294.29.77.001 1.063L11 12l3.782 3.716c.29.294.29.77 0 1.063z" fill-rule="evenodd"></path></svg></button><a href="http://zhuanlan.zhihu.com/p/27935339" class="PostItem"><div><img src="./（概率）PCA和（变分）自编码器_files/v2-dc136fd5cb6dae138073ad5d67f9241a_250x0.jpg" srcset="https://pic3.zhimg.com/v2-dc136fd5cb6dae138073ad5d67f9241a_qhd.jpg 2x" class="PostItem-TitleImage" alt="基于TensorFlow理解三大降维技术：PCA、t-SNE 和自编码器"><h1 class="PostItem-Title">基于TensorFlow理解三大降维技术：PCA、t-SNE 和自编码器</h1><div class="PostItem-Footer"><span>机器之心</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="http://zhuanlan.zhihu.com/p/25429082" class="PostItem"><div><h1 class="PostItem-Title">深度学习（五十一）变分贝叶斯自编码器(上)</h1><p class="PostItem-Summary"></p><div class="PostItem-Footer"><span>黄锦池</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="http://zhuanlan.zhihu.com/p/26655621" class="PostItem"><div><h1 class="PostItem-Title">Tensorflow下的自编码器</h1><p class="PostItem-Summary">在TensorFlow下的autoEncoder（自编码器）：预备条件：（1）Python2.7（2）预安装TensorFlow 由于存在格式的问题，下面的程序存在显示不正常，但是程序是可以正常执行的。import numpy as n…</p><div class="PostItem-Footer"><span>跨界工程师</span><span class="PostItem-FooterTitle">发表于技术部落联...</span></div></div></a><a href="http://zhuanlan.zhihu.com/p/26525819" class="PostItem"><div><img src="./（概率）PCA和（变分）自编码器_files/v2-ba1e98f17411b7eefac4d0a87e2b5bcd_250x0.jpg" srcset="https://pic2.zhimg.com/v2-ba1e98f17411b7eefac4d0a87e2b5bcd_qhd.jpg 2x" class="PostItem-TitleImage" alt="读懂编码器的工作原理及增量型编码器"><h1 class="PostItem-Title">读懂编码器的工作原理及增量型编码器</h1><div class="PostItem-Footer"><span>北岛李工</span><span class="PostItem-FooterTitle">发表于李工谈工控</span></div></div></a><button class="PagingButton PagingButton-Next"><svg class="Zi Zi--ArrowRight" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></button></ul></div><div class="Comments-container" data-za-detail-view-path-module="CommentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Comments Comments--withEditor Comments-withPagination"><div class="Topbar CommentTopbar"><div class="Topbar-title"><h2 class="CommentTopbar-title">2 条评论</h2></div><div class="Topbar-options"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Switch Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.004 7V4.232c0-.405.35-.733.781-.733.183 0 .36.06.501.17l6.437 5.033c.331.26.376.722.1 1.033a.803.803 0 0 1-.601.264H2.75a.75.75 0 0 1-.75-.75V7.75A.75.75 0 0 1 2.75 7h10.254zm-1.997 9.999v2.768c0 .405-.35.733-.782.733a.814.814 0 0 1-.5-.17l-6.437-5.034a.702.702 0 0 1-.1-1.032.803.803 0 0 1 .6-.264H21.25a.75.75 0 0 1 .75.75v1.499a.75.75 0 0 1-.75.75H11.007z" fill-rule="evenodd"></path></svg></span>切换为时间排序</button></div></div><div class="Comments-footer CommentEditor--normal"><div class="CommentEditor-input Input-wrapper Input-wrapper--spread Input-wrapper--large Input-wrapper--noPadding"><div class="Input Editable"><div class="Dropzone RichText ztext" style="min-height: 198px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-8g2o6" style="white-space: pre-wrap;">写下你的评论...</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-8g2o6" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; user-select: text; white-space: pre-wrap; word-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="8g2o6" data-offset-key="eog7j-0-0"><div data-offset-key="eog7j-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="eog7j-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input multiple="" type="file" accept="image/jpg,image/jpeg,image/png,image/gif" style="display: none;"><div></div></div></div><button disabled="" type="button" class="Button CommentEditor-singleButton Button--primary Button--blue">评论</button></div><div><div class="CommentList"><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover12-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover12-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/gchongm"><img class="Avatar UserLink-avatar" width="24" height="24" src="./（概率）PCA和（变分）自编码器_files/v2-9fb25b3404a794765f85e649d5fda9be_s.jpg" srcset="https://pic3.zhimg.com/v2-9fb25b3404a794765f85e649d5fda9be_xs.jpg 2x" alt="Eagerming"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/gchongm">Eagerming</a></span><span class="CommentItem-time">11 天前</span></div><div class="RichText ztext CommentItem-content"><p>醍醐灌顶</p></div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>赞</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div><div class="CommentItem"><div><div class="CommentItem-meta"><span class="UserLink CommentItem-avatar"><div class="Popover"><div id="Popover13-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover13-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/dealve"><img class="Avatar UserLink-avatar" width="24" height="24" src="./（概率）PCA和（变分）自编码器_files/da8e974dc_s.jpg" srcset="https://pic4.zhimg.com/da8e974dc_xs.jpg 2x" alt="DeAlVe"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/dealve">DeAlVe</a></span><span class="CommentItem-roleInfo"> (作者) </span><span><span class="CommentItem-reply">回复</span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/gchongm">Eagerming</a></span></span><span class="CommentItem-time">11 天前</span></div><div class="RichText ztext CommentItem-content">嘿嘿，多谢</div><div class="CommentItem-footer"><button type="button" class="Button CommentItem-likeBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>赞</button><button type="button" class="Button CommentItem-talkBtn Button--plain"><svg viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--comments Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><g>     <path d="M9 0C3.394 0 0 4.13 0 8c0 1.654.522 3.763 2.014 5.566.314.292.518.82.454 1.17-.165 1.488-.842 1.905-.842 1.905-.328.332.105.67.588.582 1.112-.2 2.07-.58 3.526-1.122.4-.202.464-.147.78-.078C11.524 17.764 18 14 18 8c0-3.665-3.43-8-9-8z"></path>     <path d="M19.14 9.628c.758.988.86 2.01.86 3.15 0 1.195-.62 3.11-1.368 3.938-.21.23-.354.467-.308.722.12 1.073.614 1.5.614 1.5.237.24-.188.563-.537.5-.802-.145-1.494-.42-2.545-.81-.29-.146-.336-.106-.563-.057-2.043.712-4.398.476-6.083-.926 5.964-.524 8.726-3.03 9.93-8.016z"></path>   </g></g></svg>查看对话</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 22 16" class="Icon Icon--reply Icon--left" width="13" height="16" aria-hidden="true" style="height: 16px; width: 13px;"><title></title><g><path d="M21.96 13.22c-1.687-3.552-5.13-8.062-11.637-8.65-.54-.053-1.376-.436-1.376-1.56V.677c0-.52-.635-.915-1.116-.52L.47 6.67C.18 6.947 0 7.334 0 7.763c0 .376.14.722.37.987 0 0 6.99 6.818 7.442 7.114.453.295 1.136.124 1.135-.5V13c.027-.814.703-1.466 1.532-1.466 1.185-.14 7.596-.077 10.33 2.396 0 0 .395.257.535.257.892 0 .614-.967.614-.967z"></path></g></svg>回复</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 20 18" xmlns="http://www.w3.org/2000/svg" class="Icon Icon--like Icon--left" width="13" height="16" aria-hidden="true" style="transform: rotate(180deg); height: 16px; width: 13px;"><title></title><g><path d="M.718 7.024c-.718 0-.718.63-.718.63l.996 9.693c0 .703.718.65.718.65h1.45c.916 0 .847-.65.847-.65V7.793c-.09-.88-.853-.79-.846-.79l-2.446.02zm11.727-.05S13.2 5.396 13.6 2.89C13.765.03 11.55-.6 10.565.53c-1.014 1.232 0 2.056-4.45 5.83C5.336 6.965 5 8.01 5 8.997v6.998c-.016 1.104.49 2 1.99 2h7.586c2.097 0 2.86-1.416 2.86-1.416s2.178-5.402 2.346-5.91c1.047-3.516-1.95-3.704-1.95-3.704l-5.387.007z"></path></g></svg>踩</button><button type="button" class="Button CommentItem-hoverBtn Button--plain"><svg viewBox="0 0 18 20" class="Icon Icon--report Icon--left" width="11" height="16" aria-hidden="true" style="height: 16px; width: 11px;"><title></title><g><path d="M16.947 1.13c-.633.135-3.927.638-5.697.384-3.133-.45-4.776-2.54-9.95-.888C.305 1.04.025 1.664.025 2.646L0 18.807c0 .3.1.54.304.718.195.202.438.304.73.304.275 0 .52-.102.73-.304.202-.18.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V1.964c0-.6-.42-.972-1.053-.835z"></path></g></svg>举报</button></div></div></div></div><span></span></div></div></div></article></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><div id="data" style="display:none" data-useragent="{&quot;os&quot;:{&quot;name&quot;:&quot;Linux&quot;,&quot;version&quot;:&quot;x86_64&quot;},&quot;browser&quot;:{&quot;name&quot;:&quot;Chrome&quot;,&quot;version&quot;:&quot;67.0.3396.99&quot;,&quot;major&quot;:&quot;67&quot;}}"></div><script src="./（概率）PCA和（变分）自编码器_files/vendor.995451a211dcf23e7059.js"></script><script src="./（概率）PCA和（变分）自编码器_files/column.raven.6cba4f4a0a338c575abc.js" defer=""></script><script src="./（概率）PCA和（变分）自编码器_files/column.app.48fbae75f76763cf82d1.js"></script><script></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><div class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete11-0" id="Popover10-toggle" aria-haspopup="true" aria-owns="Popover10-content" class="Input" placeholder="选择语言" value=""><div class="Input-after"><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></div></div></div></div></div></div></div></body></html>