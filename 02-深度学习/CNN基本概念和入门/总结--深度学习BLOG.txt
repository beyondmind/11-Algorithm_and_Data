BLOG（主要是CNN）
【1】简介
【2】训练
不能用传统的BP，因为梯度消失/爆炸。
如果对所有层同时训练，时间复杂度会太高；如果每次训练一层，偏差就会逐层传递，会严重欠拟合（因为深度网络的神经元和参数太多了）。

hinton提出的训练方法（不过现在已经不怎么使用）分为两步，
一 是每次训练一层网络（和BP类似，是逐层训练，即预训练。现在主要是降参），
二 是调优，使原始表示x向上生成的高级表示r和该高级表示r向下生成的x'尽可能一致。使用wake-sleep算法进行调优。将除最顶层的其它层间的权重变为双向的，即变为了图模型。
	wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”。
	sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。也就是“如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念”。

具体如下：	
1）使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）	

2）自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）：
由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以deep learning效果好很大程度上归功于第一步的feature learning过程。


【3】卷积神经网络的实现
重点是卷积层和子采样层的BP权值更新方法。
1.卷积层
2.子采样层
3.maxsoft回归,学习特征map的组合
就是让网络自己学习挑选哪些输入maps来计算得到输出map才是最好的。我们用αij表示在得到第j个输出map的其中第i个输入map的权值或者贡献。
另外，为了限制αi是稀疏的，也就是限制一个输出map只与某些而不是全部的输入maps相连，代价函数里增加稀疏约束项?(α)。

【4】用bp算法时该怎么训练
本文的主要目的是介绍CNN参数在使用bp算法时该怎么训练，毕竟CNN中有卷积层和下采样层，虽然和MLP的bp算法本质上相同

loss是系统的表达式。反向传播求误差敏感项

问题一：求输出层的误差敏感项。
这2个含义是不同的，一个是对输出层节点输入值的导数(softmax激发函数)，一个是对输出层节点输出值的导数(任意激发函数）。

问题二：当接在卷积层的下一层为pooling层时，求卷积层的误差敏感项。
其中的函数unsample()为上采样过程，其具体的操作得看是采用的什么pooling方法了。矩阵的点积操作，即对应元素的乘积。用特征核去得到卷积图的误差敏感值矩阵。当然，点乘卷积层激发函数对应位置的导数值

问题三：当接在pooling层的下一层为卷积层时，求该pooling层的误差敏感项。
相当于两矩阵卷积，用卷积图的误差敏感值矩阵和特征核。卷积过程中就包含了上采样补零。本质上还是bp算法，即第l层的误差敏感值等于第l+1层的误差敏感值乘以两者之间的权值，只不过这里由于是用了卷积，且是有重叠的

问题四：求与卷积层相连那层的权值、偏置值导数。
　　前面3个问题分别求得了误差敏感值。下面需要利用这些误差敏感值模型中参数的导数。这里没有考虑pooling层的非线性激发，因此pooling层前面是没有权值的，也就没有所谓的权值的导数了。现在将主要精力放在卷积层前面权值的求导上(也就是问题四)。




卷积层过后，可以先跟pooling层，再通过非线性传播函数。也可以是先通过非线性传播函数再经过pooling层。

当输入样本为多个时，bp算法中的误差敏感性也是一个矩阵。每一个样本都对应有自己每层的误差敏感性。


【5】Deep Learning模型之：CNN卷积神经网络（一）深度解析CNN
2.1 稀疏连接(Sparse Connectivity)
卷积网络通过在相邻两层之间强制使用局部连接模式来利用图像的空间局部特性，在第m层的隐层单元只与第m-1层的输入单元的局部区域有连接，第m-1层的这些局部 区域被称为空间连续的接受域。

2.2 权值共享(Shared Weights)
在卷积网络中，每个稀疏过滤器hi通过共享权值都会覆盖整个可视域，这些共享权值的单元构成一个特征映射。如果每个神经元用的是同一个卷积核去卷积图像，这样我们就只有100个参数啊，不管你隐层的神经元个数有多少，两层间的连接只有100个参数！这就是权值共享！

举例：
如果我们有1000x1000像素的图像，有1百万个隐层神经元，那么他们全连接的话（每个隐层神经元都连接图像的每一个像素点），就有1000x1000x1000000=10^12个连接，也就是10^12个权值参数。

但现在，首先，局部感受野：假如局部感受野是10x10，隐层每个感受野只需要和这10x10的局部图像相连接，所以1百万个隐层神经元就只有 10x10x1000000=10^8个参数。
	然后，权值共享，隐层的参数个数：隐层的参数个数和隐层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关。也就是说每一个神经元存在10x10=100个连接权值参数（卷积核）。不管你隐层的神经元个数有多少，两层间的连接只有100个参数！这就是权值共享！100种卷积核就有100个Feature Map。这100个Feature Map就组成了一层神经元。100种卷积核x每种卷积核共享100个参数=100x100=10^4。对偏置是所有滤波器共享的。
	最后，隐层的神经元个数：它和原图像，也就是输入的大小（神经元个数）、滤波器的大小和滤波器在图像中的滑动步长都有关！我的图像是1000x1000像素，而滤波器大小是10x10，假设滤波器没有重叠，也就是步长为10，这样隐层的神经元个数就是(1000x1000 )/ (10x10)=100x100个神经元了

图像越大，神经元个数和需要训练的权值参数个数的贫富差距就越大。
卷积网络的核心思想是将：局部感受野、权值共享（或者权值复制）以及时间或空间亚采样这三种结构思想

3、 CNN的训练
卷积网络在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。
在开始训练前，所有的权都应该用一些不同的小随机数进行初始化。“小随机数”用来保证网络不会因权值过大而进入饱和状态，从而导致训练失败；“不同”用来保证网络可以正常地学习。实际上，如果用相同的数去初始化权矩阵，则网络无能力学习。

训练算法与传统的BP算法差不多。主要包括4步，这4步被分为两个阶段：
第一阶段，向前传播阶段：实际上就是输入与每层的权值矩阵相点乘，得到最后的输出结果。
第二阶段，向后传播阶段：按极小化误差的方法反向传播调整权矩阵。


4.CNN的学习
网络的前后端是全连接。注意的是最后的卷积层的卷积核的尺寸要和上一层的输出相同，只有这样才能保证输出是一维向量。

卷积层的前馈运算是通过如下算法实现的：卷积层的输出= Sigmoid( Sum(卷积) +偏移量)；卷积核和偏移量都是可训练
子采样层的输出的计算式为：输出= Sigmoid( 采样*权重 +偏移量)；权值和偏置
以上两者反馈时，都使用了DSigmoid反传

 卷积网络较一般神经网络在图像处理方面有如下优点： a）输入图像和网络的拓扑结构能很好的吻合；b）特征提取和模式分类同时进行，并同时在训练中产生；c）权重共享可以减少网络的训练参数，使神经网络结构变得更简单，适应性更强。

【6】
卷积过程包括：用一个可训练的滤波器fx去卷积一个输入的图像（第一阶段是输入的图像，后面的阶段就是卷积特征map了），然后加一个偏置bx，得到卷积层Cx。
子采样过程包括：每邻域四个像素求和变为一个像素，然后通过标量Wx+1加权，再增加偏置bx+1，然后通过一个sigmoid激活函数，产生一个大概缩小四倍的特征映射图Sx+1。

C3这里需要注意的一点是：C3中的每个特征map是连接到S2中的所有6个或者几个特征map的，表示本层的特征map是上一层提取到的特征map的不同组合。这样做的原因有2点。第一，不完全的连接机制将连接的数量保持在合理的范围内。第二，也是最重要的，其破坏了网络的对称性。由于不同的特征图有不同的输入，所以迫使他们抽取不同的特征（希望是互补的）。

见笔记本最后面

【7】
一、常见问题
(1)解决梯度消失
减少层数；增大学习率（learning rate）；用ReLU代替sigmoid

(2）非线性映射的位置：

在前面提到的Bouvrie的文章中，非线性映射放在卷积层还是pooling层后面都可以，微博上几位大牛也是这个观点。

(3）权重衰减（weight decay）

因为我用的数据很少（数据多了跑起来太慢），我担心会出现过拟合，所以在cost function里加了一项正则项。但结果是不加正则项训练结果很好，一加就出现严重的under fitting，不管怎么调参数都没用。原来一直听说CNN的权重共享就相当于自带某种正则化，现在看起来确实是这样。

(4）随机梯度下降（SGD）的参数选择
最主要的就一个参数，minibatch的大小。在Deep Learning Toolbox的demo里，这个值取的是50。但是我的实验中，似乎minibatch取1的时候收敛最快。
我的感觉：训练样本顺序随机的话，每次产生的梯度也是随机的，那么50个样本平均一下梯度就很小了，也许这样比较稳健，但收敛会比较慢；相反，每个样本更新一次梯度就大得多，不过也许会比较盲目。

另外还有一个参数是learning rate。在实验中，增大minibatch会出现训不动的情况，这时适当增大learning rate能够让training cost继续下降。另外Yann LeCun似乎说过每一层应该采取不同的learning rate，不过我没试。

(5）参数的随机初始化
我试过将参数初始化到(0, 1)区间和(-1, 1)区间，感觉似乎差别不大？

(6）增加全连接层数后的性能
关于这点我不是很确定，但似乎除了增加训练时间外没什么实际作用…



二、网络的改进
（1）自动学习组合系数
为了打破对称性，从第一个卷积层到第二个卷积层并不是全链接，例如第二层第一个卷积核只卷积第一层的第123个feature map，第二层第二个卷积核只卷积第一层的第345个feature map。在LeNet里这个组合是人为规定的，Bouvrie的文章中提出这个组合系数也是可以学出来的。但是我的实验里人为规定和自动学习的效果好像差别不大，不知道更复杂的数据集上会怎么样。

（2）ReLU的位置
如果网络的最后一层是softmax分类器的话似乎其前一层就不能用ReLU，因为ReLU输出可能相差很大（比如0和几十），这时再经过softmax就会出现一个节点为1其它全0的情况。softmax的cost function里包含一项log(y)，如果y正好是0就没法算了。所以我在倒数第二层还是采用sigmoid。

（3）其他高级玩法
本来还打算玩一些其他更有趣的东西，比如dropout、maxout、max pooling等等。但是时间有限，老板已经嫌我不务正业了。