<!DOCTYPE html>
<!-- saved from url=(0037)https://zhuanlan.zhihu.com/p/37835894 -->
<html lang="zh" data-hairline="true" data-theme="light" data-focus-method="pointer"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>attention模型方法综述</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" property="description" content="本文基于几篇经典的论文，对attention模型的不同结构进行分析、拆解。先简单谈一谈attention模型的引入。以基于seq2seq模型的机器翻译为例，如果decoder只用encoder最后一个时刻输出的hidden state，可能会有两个…"><meta data-react-helmet="true" property="og:title" content="attention模型方法综述"><meta data-react-helmet="true" property="og:url" content="http://zhuanlan.zhihu.com/p/37835894"><meta data-react-helmet="true" property="og:description" content="本文基于几篇经典的论文，对attention模型的不同结构进行分析、拆解。先简单谈一谈attention模型的引入。以基于seq2seq模型的机器翻译为例，如果decoder只用encoder最后一个时刻输出的hidden state，可能会有两个…"><meta data-react-helmet="true" property="og:image" content=""><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><link href="./attention模型方法综述_files/column.app.0584d676aefe40328604.css" rel="stylesheet"><script type="text/javascript" charset="utf-8" async="" src="./attention模型方法综述_files/column.modals.8f7a90a3f13ca348daae.js"></script><script type="text/javascript" charset="utf-8" async="" src="./attention模型方法综述_files/column.richinput.72a5fb68a4bae54f8a08.js"></script></head><body class=""><p hidden="">有问题，上知乎。知乎作为中文互联网最大的知识分享平台，以「知识连接一切」为愿景，致力于构建一个人人都可以便捷接入的知识分享网络，让人们便捷地与世界分享知识、经验和见解，发现更大的世界。</p><div id="root"><div class="App" data-reactroot=""><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;PhilippZheng&quot;,&quot;itemId&quot;:37835894,&quot;title&quot;:&quot;attention模型方法综述&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;37835894&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader is-fixed" style="width: 1840px; top: 0px; left: 0px;"><div class="ColumnPageHeader-content"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" class="Icon ZhihuLogo ZhihuLogo--blue Icon--logo" style="height:30px;width:64px" width="64" height="30" aria-hidden="true"><title></title><g><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="https://zhuanlan.zhihu.com/qinlibo-ml"><img class="Avatar Avatar--round" width="30" height="30" src="./attention模型方法综述_files/v2-dade4f3465c87462e77ba24a253c0e20_is.jpg" srcset="https://pic1.zhimg.com/v2-dade4f3465c87462e77ba24a253c0e20_im.jpg 2x" alt="机器学习算法与自然语言处理"></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://zhuanlan.zhihu.com/qinlibo-ml">机器学习算法与自然语言处理</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div><div class="Sticky--holder" style="position: relative; top: 0px; right: 0px; bottom: 0px; left: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div></div><article class="Post-Main Post-NormalMain"><header class="Post-Header"><h1 class="Post-Title">attention模型方法综述</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="PhilippZheng"><meta itemprop="image" content="https://pic1.zhimg.com/v2-6cf45e398c7c654e2fb4eb0677931b8b_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/zheng-hao-99-70"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zheng-hao-99-70"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="./attention模型方法综述_files/v2-6cf45e398c7c654e2fb4eb0677931b8b_xs.jpg" srcset="https://pic1.zhimg.com/v2-6cf45e398c7c654e2fb4eb0677931b8b_l.jpg 2x" alt="PhilippZheng"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/zheng-hao-99-70">PhilippZheng</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="RichText ztext AuthorInfo-badgeText">机器学习、深度学习</div></div></div></div></div></div><div><span class="Voters"><button type="button" class="Button Button--plain">243 人<!-- -->赞了该文章</button></span></div></header><div><div class="RichText ztext Post-RichText"><p>本文基于几篇经典的论文，对attention模型的不同结构进行分析、拆解。</p><hr><p>先简单谈一谈attention模型的引入。以基于seq2seq模型的机器翻译为例，如果decoder只用encoder最后一个时刻输出的hidden state，可能会有两个问题（我个人的理解）</p><p>i）encoder最后一个hidden state，与句子末端词汇的关联较大，难以保留句子起始部分的信息；</p><p>ii）encoder按顺序依次接受输入，可以认为encoder产出的hidden state包含有词序信息。所以一定程度上decoder的翻译也基本上沿着原始句子的顺序依次进行，但实际中翻译却未必如此，以下是一个翻译的例子</p><p>英文原句：space and oceans are the new world which scientists are trying to explore</p><p>翻译结果：空间和海洋是科学家试图探索的新世界</p><p>词汇对照如下，</p><figure><noscript><img src="https://pic3.zhimg.com/v2-6f9794c2dfdc0051cbd5e5db8d9a6130_b.jpg" data-caption="" data-size="normal" data-rawwidth="600" data-rawheight="103" class="origin_image zh-lightbox-thumb" width="600" data-original="https://pic3.zhimg.com/v2-6f9794c2dfdc0051cbd5e5db8d9a6130_r.jpg"></noscript><img src="./attention模型方法综述_files/v2-6f9794c2dfdc0051cbd5e5db8d9a6130_hd.jpg" data-caption="" data-size="normal" data-rawwidth="600" data-rawheight="103" class="origin_image zh-lightbox-thumb lazy" width="600" data-original="https://pic3.zhimg.com/v2-6f9794c2dfdc0051cbd5e5db8d9a6130_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-6f9794c2dfdc0051cbd5e5db8d9a6130_b.jpg"></figure><p>可以看到，翻译的过程并不总是沿着原句从左至右依次进行翻译，例如上面例子的定语从句。</p><p>为了一定程度上解决以上的问题，14年的一篇文章 <i>sequence to sequence learning with neural networks </i>提出了一个有意思的trick，即在模型训练的过程中将原始句子进行反转，取得了一定的效果。</p><p>为了更好地解决问题，attention模型开始得到广泛重视和应用。</p><p>下面进入正题，进行对attention的介绍。</p><hr><ol><li>论文 <i>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</i></li></ol><a target="_blank" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1502.03044v1" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage" data-za-detail-view-id="172"><span class="LinkCard-content"><span><span class="LinkCard-title" data-text="true">Neural Image Caption Generation with Visual Attention</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>arxiv.org</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><p>文章讨论的场景是图像描述生成(Image Caption Generation)，对于这种场景，先放一张图，感受一下attention的框架</p><figure><noscript><img src="https://pic4.zhimg.com/v2-e3c5140cfaa5738b60e6fae1b06a1f1a_b.jpg" data-caption="" data-size="normal" data-rawwidth="593" data-rawheight="509" class="origin_image zh-lightbox-thumb" width="593" data-original="https://pic4.zhimg.com/v2-e3c5140cfaa5738b60e6fae1b06a1f1a_r.jpg"></noscript><img src="./attention模型方法综述_files/v2-e3c5140cfaa5738b60e6fae1b06a1f1a_hd.jpg" data-caption="" data-size="normal" data-rawwidth="593" data-rawheight="509" class="origin_image zh-lightbox-thumb lazy" width="593" data-original="https://pic4.zhimg.com/v2-e3c5140cfaa5738b60e6fae1b06a1f1a_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-e3c5140cfaa5738b60e6fae1b06a1f1a_b.jpg"></figure><p>文章提出了两种attention模式，即hard attention 和soft attention，来感受一下这两种attention，</p><figure><noscript><img src="https://pic2.zhimg.com/v2-1a7801d0340dec029f09a22c16b3e7ea_b.jpg" data-caption="" data-size="normal" data-rawwidth="924" data-rawheight="248" class="origin_image zh-lightbox-thumb" width="924" data-original="https://pic2.zhimg.com/v2-1a7801d0340dec029f09a22c16b3e7ea_r.jpg"></noscript><img src="./attention模型方法综述_files/v2-1a7801d0340dec029f09a22c16b3e7ea_hd.jpg" data-caption="" data-size="normal" data-rawwidth="924" data-rawheight="248" class="origin_image zh-lightbox-thumb lazy" width="924" data-original="https://pic2.zhimg.com/v2-1a7801d0340dec029f09a22c16b3e7ea_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-1a7801d0340dec029f09a22c16b3e7ea_b.jpg"></figure><p>可以看到，hard attention会专注于很小的区域，而soft attention的注意力相对发散。模型的encoder利用CNN(VGG net)，提取出图像的 <img src="./attention模型方法综述_files/equation" alt="L" eeimg="1"> 个 <img src="./attention模型方法综述_files/equation(1)" alt="D" eeimg="1"> 维的向量 <img src="./attention模型方法综述_files/equation(2)" alt="a_{i},i=1,2,...,L" eeimg="1"> ,每个向量表示图像的一部分信息。decoder是一个LSTM，每个timestep  <img src="./attention模型方法综述_files/equation(3)" alt="t " eeimg="1"> 的输入包含三个部分，即context vector   <img src="./attention模型方法综述_files/equation(4)" alt="z_{t}" eeimg="1"> 、前一个timestep的hidden state  <img src="./attention模型方法综述_files/equation(5)" alt="h_{t-1}" eeimg="1"> 、前一个timestep的output  <img src="./attention模型方法综述_files/equation(6)" alt="y_{t-1}" eeimg="1"> 。 <img src="./attention模型方法综述_files/equation(4)" alt="z_{t}" eeimg="1"> 由{ <img src="./attention模型方法综述_files/equation(7)" alt="a_{i}" eeimg="1"> }和权重{ <img src="./attention模型方法综述_files/equation(8)" alt="\alpha_{ti}" eeimg="1"> }通过加权得到。这里的权重 <img src="./attention模型方法综述_files/equation(8)" alt="\alpha_{ti}" eeimg="1"> 通过attention模型 <img src="./attention模型方法综述_files/equation(9)" alt="f_{att}" eeimg="1"> 来计算得到，而本文中的 <img src="./attention模型方法综述_files/equation(9)" alt="f_{att}" eeimg="1"> 是一个多层感知机(multilayer perceptron)。</p><figure><noscript><img src="https://pic3.zhimg.com/v2-b61109d6a2420e9007d703bd817bde37_b.jpg" data-caption="" data-size="normal" data-rawwidth="301" data-rawheight="90" class="content_image" width="301"></noscript><img src="./attention模型方法综述_files/v2-b61109d6a2420e9007d703bd817bde37_hd.jpg" data-caption="" data-size="normal" data-rawwidth="301" data-rawheight="90" class="content_image lazy" width="301" data-actualsrc="https://pic3.zhimg.com/v2-b61109d6a2420e9007d703bd817bde37_b.jpg"></figure><p>从而可以计算 <img src="./attention模型方法综述_files/equation(10)" alt="z_{t} = \sum_{i=1}^{L}{\alpha_{ti}*a_{i}}" eeimg="1"> 。接下来文章重点讨论hard(也叫stochastic attention)和soft(也叫deterministic)两种attention模式。</p><p>1) Stochastic “Hard” Attention</p><p>记 <img src="./attention模型方法综述_files/equation(11)" alt="s_{t}" eeimg="1"> 为decoder第 <img src="./attention模型方法综述_files/equation(12)" alt="t" eeimg="1"> 个时刻的attention所关注的位置编号， <img src="./attention模型方法综述_files/equation(13)" alt="s_{ti}" eeimg="1"> 表示第 <img src="./attention模型方法综述_files/equation(12)" alt="t" eeimg="1"> 时刻attention是否关注位置 <img src="./attention模型方法综述_files/equation(14)" alt="i" eeimg="1"> ， <img src="./attention模型方法综述_files/equation(13)" alt="s_{ti}" eeimg="1"> 服从多元伯努利分布(multinoulli distribution)， 对于任意的 <img src="./attention模型方法综述_files/equation(12)" alt="t" eeimg="1"> ， <img src="./attention模型方法综述_files/equation(15)" alt="s_{ti},i=1,2,...,L" eeimg="1"> 中有且只有取1，其余全部为0，所以 <img src="./attention模型方法综述_files/equation(16)" alt="[s_{t1},s_{t2},...,s_{tL}]" eeimg="1"> 是one-hot形式。这种attention每次只focus一个位置的做法，就是“hard”称谓的来源。 <img src="./attention模型方法综述_files/equation(4)" alt="z_{t}" eeimg="1"> 也就被视为一个变量，计算如下,</p><figure><noscript><img src="https://pic4.zhimg.com/v2-9b93677b9daf2aaaa0c033152141ef71_b.jpg" data-caption="" data-size="normal" data-rawwidth="327" data-rawheight="90" class="content_image" width="327"></noscript><img src="./attention模型方法综述_files/v2-9b93677b9daf2aaaa0c033152141ef71_hd.jpg" data-caption="" data-size="normal" data-rawwidth="327" data-rawheight="90" class="content_image lazy" width="327" data-actualsrc="https://pic4.zhimg.com/v2-9b93677b9daf2aaaa0c033152141ef71_b.jpg"></figure><p>问题是 <img src="./attention模型方法综述_files/equation(8)" alt="\alpha_{ti}" eeimg="1"> 怎么算呢？把 <img src="./attention模型方法综述_files/equation(8)" alt="\alpha_{ti}" eeimg="1"> 视为隐变量，研究模型的目标函数，进而研究目标函数对参数的梯度。直观理解，模型要根据 <img src="./attention模型方法综述_files/equation(17)" alt="a=(a_{1},...,a_{L})" eeimg="1"> 来生成序列 <img src="./attention模型方法综述_files/equation(18)" alt="y=(y_{1},...,y_{C})" eeimg="1"> ，所以目标可以是最大化log <img src="./attention模型方法综述_files/equation(19)" alt="p(y|a)" eeimg="1"> ，但这里没有显式的包含 <img src="./attention模型方法综述_files/equation(20)" alt="s" eeimg="1"> ，所以作者利用著名的Jensen不等式(<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Jensen%2527s_inequality" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Jensen's inequality</a>)对目标函数做了转化，得到了目标函数的一个lower bound，如下</p><figure><noscript><img src="https://pic4.zhimg.com/v2-4ce6ffa41d53049371c0ea2fd642f62e_b.jpg" data-caption="" data-size="normal" data-rawwidth="360" data-rawheight="147" class="content_image" width="360"></noscript><img src="./attention模型方法综述_files/v2-4ce6ffa41d53049371c0ea2fd642f62e_hd.jpg" data-caption="" data-size="normal" data-rawwidth="360" data-rawheight="147" class="content_image lazy" width="360" data-actualsrc="https://pic4.zhimg.com/v2-4ce6ffa41d53049371c0ea2fd642f62e_b.jpg"></figure><p>这里的 <img src="./attention模型方法综述_files/equation(20)" alt="s" eeimg="1"> ={ <img src="./attention模型方法综述_files/equation(21)" alt="s_{1},...,s_{C}" eeimg="1"> }，是时间轴上的重点focus的序列，理论上这种序列共有 <img src="./attention模型方法综述_files/equation(22)" alt="L^{C}" eeimg="1"> 个。</p><p>然后就用log <img src="./attention模型方法综述_files/equation(19)" alt="p(y|a)" eeimg="1"> 代替原始的目标函数，对模型的参数 <img src="./attention模型方法综述_files/equation(23)" alt="W" eeimg="1"> 算gradient。</p><figure><noscript><img src="https://pic3.zhimg.com/v2-5362b977935d4f299bd938bef886dd9c_b.jpg" data-caption="" data-size="normal" data-rawwidth="401" data-rawheight="153" class="content_image" width="401"></noscript><img src="./attention模型方法综述_files/v2-5362b977935d4f299bd938bef886dd9c_hd.jpg" data-caption="" data-size="normal" data-rawwidth="401" data-rawheight="153" class="content_image lazy" width="401" data-actualsrc="https://pic3.zhimg.com/v2-5362b977935d4f299bd938bef886dd9c_b.jpg"></figure><p>然后利用蒙特卡洛方法对 <img src="./attention模型方法综述_files/equation(20)" alt="s" eeimg="1"> 进行抽样，我们做 <img src="./attention模型方法综述_files/equation(24)" alt="N" eeimg="1"> 次这样的抽样实验，记每次取到的序列是 <img src="./attention模型方法综述_files/equation(25)" alt="\tilde{s}^{n}" eeimg="1">   ，易知 <img src="./attention模型方法综述_files/equation(25)" alt="\tilde{s}^{n}" eeimg="1"> 的概率为 <img src="./attention模型方法综述_files/equation(26)" alt="\frac{1}{N}" eeimg="1"> ，所以上面的求gradient的结果即为</p><figure><noscript><img src="https://pic1.zhimg.com/v2-a541e083d0f81bbad9c49bd69571532b_b.jpg" data-caption="" data-size="normal" data-rawwidth="407" data-rawheight="161" class="content_image" width="407"></noscript><img src="./attention模型方法综述_files/v2-a541e083d0f81bbad9c49bd69571532b_hd.jpg" data-caption="" data-size="normal" data-rawwidth="407" data-rawheight="161" class="content_image lazy" width="407" data-actualsrc="https://pic1.zhimg.com/v2-a541e083d0f81bbad9c49bd69571532b_b.jpg"></figure><p>接下来的一些细节涉及reinforcement learning，感兴趣的同学可以去看这篇paper。</p><p><br></p><p>2）Deterministic “Soft” Attention</p><p>说完"硬"的attention，再来说说“软”的attention。</p><p>相对来说soft attention很好理解，在hard attention里面，每个时刻 <img src="./attention模型方法综述_files/equation(12)" alt="t" eeimg="1"> 模型的序列 [ <img src="./attention模型方法综述_files/equation(27)" alt="s_{t1},...,s_{tL}" eeimg="1"> ] 只有一个取1，其余全部为0，也就是说每次只focus一个位置，而soft attention每次会照顾到全部的位置，只是不同位置的权重不同罢了。这时 <img src="./attention模型方法综述_files/equation(4)" alt="z_{t}" eeimg="1"> 即为 <img src="./attention模型方法综述_files/equation(7)" alt="a_{i}" eeimg="1"> 的加权求和 <img src="./attention模型方法综述_files/equation(28)" alt="z_{t} = \sum_{i=1}^{L}{\alpha_{ti}}*a_{i}" eeimg="1"> </p><p>这样soft attention是光滑的且可微的（即目标函数，也就是LSTM的目标函数对权重 <img src="./attention模型方法综述_files/equation(8)" alt="\alpha_{ti}" eeimg="1"> 是可微的，原因很简单，因为目标函数对 <img src="./attention模型方法综述_files/equation(4)" alt="z_{t}" eeimg="1"> 可微，而 <img src="./attention模型方法综述_files/equation(4)" alt="z_{t}" eeimg="1"> 对 <img src="./attention模型方法综述_files/equation(8)" alt="\alpha_{ti}" eeimg="1"> 可微，根据chain rule可得目标函数对 <img src="./attention模型方法综述_files/equation(8)" alt="\alpha_{ti}" eeimg="1"> 可微）。</p><p>文章还对这种soft attention做了微调， <img src="./attention模型方法综述_files/equation(29)" alt="z_{t} = \beta_{t}*\sum_{i=1}^{L}{\alpha_{ti}}*a_{i}" eeimg="1"> ，其中 <img src="./attention模型方法综述_files/equation(30)" alt="\beta_{t} = \sigma(f_{\beta}(h_{t-1}))" eeimg="1"> ，</p><p>用来调节context vector在LSTM中的比重（相对于 <img src="./attention模型方法综述_files/equation(5)" alt="h_{t-1}" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(6)" alt="y_{t-1}" eeimg="1"> 的比重）。</p><p>btw，模型的loss function加入了 <img src="./attention模型方法综述_files/equation(8)" alt="\alpha_{ti}" eeimg="1"> 的正则项，</p><figure><noscript><img src="https://pic4.zhimg.com/v2-311921af10ca22f34de4c02b97460109_b.jpg" data-caption="" data-size="normal" data-rawwidth="369" data-rawheight="82" class="content_image" width="369"></noscript><img src="./attention模型方法综述_files/v2-311921af10ca22f34de4c02b97460109_hd.jpg" data-caption="" data-size="normal" data-rawwidth="369" data-rawheight="82" class="content_image lazy" width="369" data-actualsrc="https://pic4.zhimg.com/v2-311921af10ca22f34de4c02b97460109_b.jpg"></figure><hr><p>2. 论文 <i>Effective Approaches to Attention-based Neural Machine Translation</i></p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.04025" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage" data-za-detail-view-id="172"><span class="LinkCard-content"><span><span class="LinkCard-title" data-text="true">[1508.04025] Effective Approaches to Attention-based Neural Machine Translation</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>arxiv.org</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><p>​文章提出了两种attention的改进版本，即global attention和local attention。</p><p>先感受一下global attention和local attention长什么样子</p><figure><noscript><img src="https://pic4.zhimg.com/v2-d4c44f741a904ca68eec3248a1a2a42a_b.jpg" data-size="normal" data-rawwidth="396" data-rawheight="351" class="content_image" width="396"></noscript><img src="./attention模型方法综述_files/v2-d4c44f741a904ca68eec3248a1a2a42a_hd.jpg" data-size="normal" data-rawwidth="396" data-rawheight="351" class="content_image lazy" width="396" data-actualsrc="https://pic4.zhimg.com/v2-d4c44f741a904ca68eec3248a1a2a42a_b.jpg"><figcaption>global attention</figcaption></figure><figure><noscript><img src="https://pic2.zhimg.com/v2-b4da8aab105d9b80de834cc853a80c83_b.jpg" data-size="normal" data-rawwidth="414" data-rawheight="381" class="content_image" width="414"></noscript><img src="./attention模型方法综述_files/v2-b4da8aab105d9b80de834cc853a80c83_hd.jpg" data-size="normal" data-rawwidth="414" data-rawheight="381" class="content_image lazy" width="414" data-actualsrc="https://pic2.zhimg.com/v2-b4da8aab105d9b80de834cc853a80c83_b.jpg"><figcaption>local attention</figcaption></figure><p>文章指出，local attention可以视为hard attention和soft attention的混合体（优势上的混合），因为它的计算复杂度要低于global attention、soft attention，而且与hard attention不同的是，local attention几乎处处可微，易与训练。</p><p>文章以机器翻译为场景， <img src="./attention模型方法综述_files/equation(31)" alt="x_{1},...,x_{n}" eeimg="1"> 为source sentence， <img src="./attention模型方法综述_files/equation(32)" alt="y_{1},...,y_{m}" eeimg="1"> 为target sentence，   <img src="./attention模型方法综述_files/equation(33)" alt="c_{1},...,c_{m}" eeimg="1"> 为encoder产生的context vector，objective function为</p><figure><noscript><img src="https://pic4.zhimg.com/v2-06e7e6b69be934f75ce9eb73087dab12_b.jpg" data-caption="" data-size="normal" data-rawwidth="291" data-rawheight="64" class="content_image" width="291"></noscript><img src="./attention模型方法综述_files/v2-06e7e6b69be934f75ce9eb73087dab12_hd.jpg" data-caption="" data-size="normal" data-rawwidth="291" data-rawheight="64" class="content_image lazy" width="291" data-actualsrc="https://pic4.zhimg.com/v2-06e7e6b69be934f75ce9eb73087dab12_b.jpg"></figure><p><img src="./attention模型方法综述_files/equation(34)" alt="c_{t}" eeimg="1"> 来源于encoder中多个source position所产生的hidden states，global attention和local attention的主要区别在于attention所forcus的source positions数目的不同：如果attention forcus全部的position，则是global attention，反之，若只forcus一部分position，则为local attention。</p><p>由此可见，这里的global attention、local attention和soft attention并无本质上的区别，两篇paper模型的差别只是在LSTM结构上有微小的差别。</p><p>在decoder的时刻 <img src="./attention模型方法综述_files/equation(12)" alt="t" eeimg="1"> ，在利用global attention或local attention得到context vector  <img src="./attention模型方法综述_files/equation(34)" alt="c_{t}" eeimg="1"> 之后，结合 <img src="./attention模型方法综述_files/equation(35)" alt="h_{t}" eeimg="1"> ，对二者做concatenate操作，得到attention hidden state，</p><figure><noscript><img src="https://pic4.zhimg.com/v2-c9306535611589cf3e4839cd7c1570ea_b.jpg" data-caption="" data-size="normal" data-rawwidth="261" data-rawheight="52" class="content_image" width="261"></noscript><img src="./attention模型方法综述_files/v2-c9306535611589cf3e4839cd7c1570ea_hd.jpg" data-caption="" data-size="normal" data-rawwidth="261" data-rawheight="52" class="content_image lazy" width="261" data-actualsrc="https://pic4.zhimg.com/v2-c9306535611589cf3e4839cd7c1570ea_b.jpg"></figure><p>最后利用softmax产出该时刻的输出</p><figure><noscript><img src="https://pic2.zhimg.com/v2-63b83402ac5eb3b20add851e61de6c97_b.jpg" data-caption="" data-size="normal" data-rawwidth="305" data-rawheight="58" class="content_image" width="305"></noscript><img src="./attention模型方法综述_files/v2-63b83402ac5eb3b20add851e61de6c97_hd.jpg" data-caption="" data-size="normal" data-rawwidth="305" data-rawheight="58" class="content_image lazy" width="305" data-actualsrc="https://pic2.zhimg.com/v2-63b83402ac5eb3b20add851e61de6c97_b.jpg"></figure><p>下面重点介绍global attention、local attention。</p><p>1）global attention</p><p>global attention 在计算context vector  <img src="./attention模型方法综述_files/equation(34)" alt="c_{t}" eeimg="1"> 的时候会考虑encoder所产生的全部hidden state。记decoder时刻 <img src="./attention模型方法综述_files/equation(12)" alt="t" eeimg="1"> 的target hidden为 <img src="./attention模型方法综述_files/equation(35)" alt="h_{t}" eeimg="1"> ，encoder的全部hidden state为 <img src="./attention模型方法综述_files/equation(36)" alt="\bar{h}_{s},s=1,2,...,n" eeimg="1"> ，对于其中任意 <img src="./attention模型方法综述_files/equation(37)" alt="\bar{h}_{s}" eeimg="1"> ，其权重 <img src="./attention模型方法综述_files/equation(38)" alt="\alpha_{t}(s)" eeimg="1"> 为</p><figure><noscript><img src="https://pic3.zhimg.com/v2-cfe940714b5d13fbbd00757055b9437a_b.jpg" data-caption="" data-size="normal" data-rawwidth="346" data-rawheight="123" class="content_image" width="346"></noscript><img src="./attention模型方法综述_files/v2-cfe940714b5d13fbbd00757055b9437a_hd.jpg" data-caption="" data-size="normal" data-rawwidth="346" data-rawheight="123" class="content_image lazy" width="346" data-actualsrc="https://pic3.zhimg.com/v2-cfe940714b5d13fbbd00757055b9437a_b.jpg"></figure><p>而其中的 <img src="./attention模型方法综述_files/equation(39)" alt="score(h_{t},\bar{h}_{s})" eeimg="1"> ，文章给出了四种种计算方法（文章称为alignment function)，</p><figure><noscript><img src="https://pic4.zhimg.com/v2-1eac404f6858e0a2f23011d804e1251d_b.jpg" data-caption="" data-size="normal" data-rawwidth="433" data-rawheight="127" class="origin_image zh-lightbox-thumb" width="433" data-original="https://pic4.zhimg.com/v2-1eac404f6858e0a2f23011d804e1251d_r.jpg"></noscript><img src="./attention模型方法综述_files/v2-1eac404f6858e0a2f23011d804e1251d_hd.jpg" data-caption="" data-size="normal" data-rawwidth="433" data-rawheight="127" class="origin_image zh-lightbox-thumb lazy" width="433" data-original="https://pic4.zhimg.com/v2-1eac404f6858e0a2f23011d804e1251d_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-1eac404f6858e0a2f23011d804e1251d_b.jpg"></figure><figure><noscript><img src="https://pic3.zhimg.com/v2-13ba58c1dd6676d908028aae6f47a84c_b.jpg" data-caption="" data-size="normal" data-rawwidth="377" data-rawheight="74" class="content_image" width="377"></noscript><img src="./attention模型方法综述_files/v2-13ba58c1dd6676d908028aae6f47a84c_hd.jpg" data-caption="" data-size="normal" data-rawwidth="377" data-rawheight="74" class="content_image lazy" width="377" data-actualsrc="https://pic3.zhimg.com/v2-13ba58c1dd6676d908028aae6f47a84c_b.jpg"></figure><p>四种方法都比较直观、简单。在得到这些权重后， <img src="./attention模型方法综述_files/equation(34)" alt="c_{t}" eeimg="1"> 的计算是很自然的，即为 <img src="./attention模型方法综述_files/equation(37)" alt="\bar{h}_{s}" eeimg="1"> 的weighted summation。</p><p>2) local attention</p><p>global attention可能的缺点在于每次都要扫描全部的source hidden state，计算开销较大，对于长句翻译不利，为了提升效率，提出local attention，每次只forcus一小部分的source position。</p><p>这里，context vector   <img src="./attention模型方法综述_files/equation(34)" alt="c_{t}" eeimg="1"> 的计算只forcus窗口 <img src="./attention模型方法综述_files/equation(40)" alt="[p_{t}-D,p_{t}+D]" eeimg="1"> 内的 <img src="./attention模型方法综述_files/equation(41)" alt="2D+1" eeimg="1"> 个source hidden states(若发生越界，则忽略界外的source hidden states)。其中 <img src="./attention模型方法综述_files/equation(42)" alt="p_{t}" eeimg="1"> 是一个source position index，可以理解为attention的“焦点”，作为模型的参数， <img src="./attention模型方法综述_files/equation(1)" alt="D" eeimg="1"> 根据经验来选择（文章选用10）。</p><p>关于 <img src="./attention模型方法综述_files/equation(42)" alt="p_{t}" eeimg="1"> 的计算，文章给出了两种计算方案，</p><p>i）Monotonic alignment (local-m)</p><p><img src="./attention模型方法综述_files/equation(43)" alt="p_{t}=t" eeimg="1"> </p><p>ii) Predictive alignment (local-p)</p><figure><noscript><img src="https://pic4.zhimg.com/v2-0bf49042025a0b00a1c3e043d75f5ad5_b.jpg" data-caption="" data-size="normal" data-rawwidth="317" data-rawheight="49" class="content_image" width="317"></noscript><img src="./attention模型方法综述_files/v2-0bf49042025a0b00a1c3e043d75f5ad5_hd.jpg" data-caption="" data-size="normal" data-rawwidth="317" data-rawheight="49" class="content_image lazy" width="317" data-actualsrc="https://pic4.zhimg.com/v2-0bf49042025a0b00a1c3e043d75f5ad5_b.jpg"></figure><p>其中 <img src="./attention模型方法综述_files/equation(44)" alt="W_{p}" eeimg="1"> 和 <img src="./attention模型方法综述_files/equation(45)" alt="v_{p}" eeimg="1"> 是模型的参数， <img src="./attention模型方法综述_files/equation(46)" alt="S" eeimg="1"> 是source sentence的长度，易知 <img src="./attention模型方法综述_files/equation(47)" alt="p_{t}\in[0,S]" eeimg="1"> 。</p><p>权重 <img src="./attention模型方法综述_files/equation(38)" alt="\alpha_{t}(s)" eeimg="1"> 的计算如下</p><figure><noscript><img src="https://pic4.zhimg.com/v2-1a1c8b21404b0fc6c5bbb84bcb66719d_b.jpg" data-caption="" data-size="normal" data-rawwidth="357" data-rawheight="68" class="content_image" width="357"></noscript><img src="./attention模型方法综述_files/v2-1a1c8b21404b0fc6c5bbb84bcb66719d_hd.jpg" data-caption="" data-size="normal" data-rawwidth="357" data-rawheight="68" class="content_image lazy" width="357" data-actualsrc="https://pic4.zhimg.com/v2-1a1c8b21404b0fc6c5bbb84bcb66719d_b.jpg"></figure><p>可以看出，距离中心 <img src="./attention模型方法综述_files/equation(42)" alt="p_{t}" eeimg="1"> 越远的位置，其位置上的source hidden state对应的权重就会被压缩地越厉害。</p><hr><p>3. 论文 <i>neural machine translation by jointly learning to align and translate</i></p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.0473v2" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage" data-za-detail-view-id="172"><span class="LinkCard-content"><span><span class="LinkCard-title" data-text="true">[1409.0473v2] Neural Machine Translation by Jointly Learning to Align and Translate</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>arxiv.org</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><p>这篇文章没有使用新的attention结构，其attention就是soft attention的形式。文章给出了一些attention的可视化效果图，</p><figure><noscript><img src="https://pic4.zhimg.com/v2-4890b32d36ffebe32951a2a935b8be18_b.jpg" data-caption="" data-size="normal" data-rawwidth="785" data-rawheight="815" class="origin_image zh-lightbox-thumb" width="785" data-original="https://pic4.zhimg.com/v2-4890b32d36ffebe32951a2a935b8be18_r.jpg"></noscript><img src="./attention模型方法综述_files/v2-4890b32d36ffebe32951a2a935b8be18_hd.jpg" data-caption="" data-size="normal" data-rawwidth="785" data-rawheight="815" class="origin_image zh-lightbox-thumb lazy" width="785" data-original="https://pic4.zhimg.com/v2-4890b32d36ffebe32951a2a935b8be18_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-4890b32d36ffebe32951a2a935b8be18_b.jpg"></figure><p>上面4幅图中，x轴代表原始英文句子，y轴代表翻译为法文的结果。每个像素代表的是纵轴的相应位置的target hidden state与横轴相应位置的source hidden state计算得到的权重 <img src="./attention模型方法综述_files/equation(48)" alt="\alpha_{ij}" eeimg="1"> ，权重越大，对应的像素点越亮。可以看到，亮斑基本处在对角线上，符合预期，毕竟翻译的过程基本是沿着原始句子从左至右依次进行翻译。</p><hr><p>4. 论文 <i>attention is all you need、weighted transformer network for machine translation</i></p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage" data-za-detail-view-id="172"><span class="LinkCard-content"><span><span class="LinkCard-title" data-text="true">[1706.03762] Attention Is All You Need</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>arxiv.org</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><a target="_blank" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.02132" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage" data-za-detail-view-id="172"><span class="LinkCard-content"><span><span class="LinkCard-title" data-text="true">[1711.02132] Weighted Transformer Network for Machine Translation</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>arxiv.org</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><p>作者首先指出，结合了RNN(及其变体)和注意力机制的模型在序列建模领域取得了不错的成绩，但由于RNN的循环特性导致其不利于并行计算，所以模型的训练时间往往较长，在GPU上一个大一点的seq2seq模型通常要跑上几天，所以作者对RNN深恶痛绝，遂决定舍弃RNN，只用注意力模型来进行序列的建模。</p><p>作者提出一种新型的网络结构，并起了个名字 Transformer，里面所包含的注意力机制称之为 self-attention。作者骄傲地宣称他这套Transformer是能够计算input和output的representation而不借助RNN的唯一的model，所以作者说有attention就够了～</p><p>模型同样包含encoder和decoder两个stage，encoder和decoder都是抛弃RNN，而是用堆叠起来的self-attention，和fully-connected layer来完成，模型的架构如下，</p><figure><noscript><img src="https://pic4.zhimg.com/v2-54b38f3ee05381c8fb9449cc755592b4_b.jpg" data-caption="" data-size="normal" data-rawwidth="678" data-rawheight="660" class="origin_image zh-lightbox-thumb" width="678" data-original="https://pic4.zhimg.com/v2-54b38f3ee05381c8fb9449cc755592b4_r.jpg"></noscript><img src="./attention模型方法综述_files/v2-54b38f3ee05381c8fb9449cc755592b4_hd.jpg" data-caption="" data-size="normal" data-rawwidth="678" data-rawheight="660" class="origin_image zh-lightbox-thumb lazy" width="678" data-original="https://pic4.zhimg.com/v2-54b38f3ee05381c8fb9449cc755592b4_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-54b38f3ee05381c8fb9449cc755592b4_b.jpg"></figure><p>从图中可以看出，模型共包含三个attention成分，分别是encoder的self-attention，decoder的self-attention，以及连接encoder和decoder的attention。</p><p>这三个attention block都是multi-head attention的形式，输入都是query   <img src="./attention模型方法综述_files/equation(49)" alt="Q" eeimg="1"> 、key   <img src="./attention模型方法综述_files/equation(50)" alt="K" eeimg="1"> 、value   <img src="./attention模型方法综述_files/equation(51)" alt="V" eeimg="1"> 三个元素，只是 <img src="./attention模型方法综述_files/equation(49)" alt="Q" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(50)" alt="K" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(51)" alt="V" eeimg="1"> 的取值不同罢了。接下来重点讨论最核心的模块 multi-head attention（多头注意力）。</p><p>multi-head attention由多个scaled dot-product attention这样的基础单元经过stack而成，</p><figure><noscript><img src="https://pic2.zhimg.com/v2-31f197ccd0304013539e031406b50ebd_b.jpg" data-caption="" data-size="normal" data-rawwidth="612" data-rawheight="328" class="origin_image zh-lightbox-thumb" width="612" data-original="https://pic2.zhimg.com/v2-31f197ccd0304013539e031406b50ebd_r.jpg"></noscript><img src="./attention模型方法综述_files/v2-31f197ccd0304013539e031406b50ebd_hd.jpg" data-caption="" data-size="normal" data-rawwidth="612" data-rawheight="328" class="origin_image zh-lightbox-thumb lazy" width="612" data-original="https://pic2.zhimg.com/v2-31f197ccd0304013539e031406b50ebd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-31f197ccd0304013539e031406b50ebd_b.jpg"></figure><p>那重点就变成scaled dot-product attention是什么鬼了。</p><p>按字面意思理解，scaled dot-product attention即 缩放了的点乘注意力，我们来对它进行研究。</p><p>在这之前，我们先回顾一下上文提到的传统的attention方法（例如global attention，score采用dot形式）</p><p>记decoder时刻 <img src="./attention模型方法综述_files/equation(12)" alt="t" eeimg="1"> 的target hidden state为 <img src="./attention模型方法综述_files/equation(35)" alt="h_{t}" eeimg="1"> ，encoder得到的全部source hidden state为 <img src="./attention模型方法综述_files/equation(52)" alt="\bar{h}_{s},s=1,...,n" eeimg="1"> ，则decoder的context vector  <img src="./attention模型方法综述_files/equation(34)" alt="c_{t}" eeimg="1"> 的计算过程如下，</p><p><img src="./attention模型方法综述_files/equation(53)" alt="e_{t}(s) = h_{t}^{T}*\bar{h}_{s}" eeimg="1"> </p><p><img src="./attention模型方法综述_files/equation(54)" alt="\alpha_{t}(s) = \frac{exp(e_{t}(s))}{\sum_{i=1}^{n}{exp(e_{t}(i))}}=softmax(e_{t}(s))" eeimg="1"> </p><p><img src="./attention模型方法综述_files/equation(55)" alt="c_{t}=\sum_{s=1}^{n}{\alpha_{t}(s)*\bar{h}_{s}}=\sum_{s=1}^{n}{softmax(h_{t}^{T}*\bar{h}_{s})*\bar{h}_{s}}-----------(*)" eeimg="1"> </p><p><br></p><p>作者先抛出三个名词 query   <img src="./attention模型方法综述_files/equation(49)" alt="Q" eeimg="1"> 、key   <img src="./attention模型方法综述_files/equation(50)" alt="K" eeimg="1"> 、value   <img src="./attention模型方法综述_files/equation(51)" alt="V" eeimg="1"> ，然后计算这三个元素的attention</p><p><img src="./attention模型方法综述_files/equation(56)" alt="Attention(Q,K,V)=softmax(QK^{T})V" eeimg="1"> </p><p>我的写法与论文有细微差别，但为了接下来说明的简便，我姑且简化成这样。这个Attention的计算跟上面的(*)式有几分相似。</p><p>那么 <img src="./attention模型方法综述_files/equation(49)" alt="Q" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(50)" alt="K" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(51)" alt="V" eeimg="1"> 到底是什么？论文里讲的比较晦涩，说说我的理解。encoder里的attention叫self-attention，顾名思义，就是自己和自己做attention。抛开这篇论文的做法，让我们激活自己的创造力，在传统的seq2seq中的encoder阶段，我们得到 <img src="./attention模型方法综述_files/equation(57)" alt="n" eeimg="1"> 个时刻的hidden states之后，可以用每一时刻的hidden state  <img src="./attention模型方法综述_files/equation(58)" alt="h_{i}" eeimg="1"> ，去分别和任意的hidden state <img src="./attention模型方法综述_files/equation(59)" alt="h_{j},j=1,2,...,n" eeimg="1"> 计算attention，这就有点self-attention的意思。回到当前的模型，由于抛弃了RNN，encoder过程就没了hidden states，那拿什么做self-attention来自嗨呢？可以想到，假如作为input的sequence共有 <img src="./attention模型方法综述_files/equation(57)" alt="n" eeimg="1"> 个word，那么我可以先对每一个word做embedding吧？就得到 <img src="./attention模型方法综述_files/equation(57)" alt="n" eeimg="1"> 个embedding，然后我就可以用embedding代替hidden state来做self-attention了！所以 <img src="./attention模型方法综述_files/equation(49)" alt="Q" eeimg="1"> 这个矩阵里面装的就是全部的word embedding，  <img src="./attention模型方法综述_files/equation(50)" alt="K" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(51)" alt="V" eeimg="1"> 也是一样。所以为什么管 <img src="./attention模型方法综述_files/equation(49)" alt="Q" eeimg="1"> 叫query？就是你每次拿一个word embedding，去“查询”其和任意的word embedding的match程度(也就是attention的大小)，你一共要做 <img src="./attention模型方法综述_files/equation(57)" alt="n" eeimg="1"> 轮这样的操作。 我们记word embedding的dimension为   <img src="./attention模型方法综述_files/equation(60)" alt="d_{model}" eeimg="1"> ，所以 <img src="./attention模型方法综述_files/equation(49)" alt="Q" eeimg="1"> 的shape就是 <img src="./attention模型方法综述_files/equation(61)" alt="n*d_{model}" eeimg="1"> ， <img src="./attention模型方法综述_files/equation(50)" alt="K" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(51)" alt="V" eeimg="1"> 也是一样，第 <img src="./attention模型方法综述_files/equation(14)" alt="i" eeimg="1"> 个word的embedding为 <img src="./attention模型方法综述_files/equation(62)" alt="v_{i}" eeimg="1"> ，所以该word的attention应为</p><p><img src="./attention模型方法综述_files/equation(63)" alt="Attention(Q_{i},K,V)=softmax(v_{i}*[v_{1}^{T},v_{2}^{T},...,v_{n}^{T}])*\begin{bmatrix} v_{1} \\ v_{2} \\ ... \\ v_{n} \end{bmatrix}=softmax(Q_{i}K^{T})V" eeimg="1"> </p><p>那同时做全部word的attention，则是</p><p><img src="./attention模型方法综述_files/equation(64)" alt="Attention(Q,K,V)=softmax(\begin{bmatrix} v_{1} \\ v_{2} \\ ... \\ v_{n} \end{bmatrix}*[v_{1}^{T},v_{2}^{T},...,v_{n}^{T}])*\begin{bmatrix} v_{1} \\ v_{2} \\ ... \\ v_{n} \end{bmatrix}=softmax(QK^{T})V" eeimg="1"> </p><p>scaled dot-product attention基本就是这样了。基于RNN的传统encoder在每个时刻会有输入和输出，而现在encoder由于抛弃了RNN序列模型，所以可以一下子把序列的全部内容输进去，来一次self-attention的自嗨～</p><p>理解了scaled dot-product attention之后，multi-head attention就好理解了，因为就是scaled dot-product attention的stacking。先把 <img src="./attention模型方法综述_files/equation(49)" alt="Q" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(50)" alt="K" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(51)" alt="V" eeimg="1"> 做linear transformation，然后对新生成的 <img src="./attention模型方法综述_files/equation(65)" alt="Q^{’}" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(66)" alt="K^{&#39;}" eeimg="1"> 、 <img src="./attention模型方法综述_files/equation(67)" alt="V^{&#39;}" eeimg="1"> 算attention，重复这样的操作 <img src="./attention模型方法综述_files/equation(68)" alt="h" eeimg="1"> 次，然后把 <img src="./attention模型方法综述_files/equation(68)" alt="h" eeimg="1"> 次的结果做concat，最后再做一次linear  transformation，就是multi-head attention这个小block的输出了。</p><figure><noscript><img src="https://pic2.zhimg.com/v2-5f6170dff4636fb8a52272cfa1c31db6_b.jpg" data-caption="" data-size="normal" data-rawwidth="553" data-rawheight="108" class="origin_image zh-lightbox-thumb" width="553" data-original="https://pic2.zhimg.com/v2-5f6170dff4636fb8a52272cfa1c31db6_r.jpg"></noscript><img src="./attention模型方法综述_files/v2-5f6170dff4636fb8a52272cfa1c31db6_hd.jpg" data-caption="" data-size="normal" data-rawwidth="553" data-rawheight="108" class="origin_image zh-lightbox-thumb lazy" width="553" data-original="https://pic2.zhimg.com/v2-5f6170dff4636fb8a52272cfa1c31db6_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-5f6170dff4636fb8a52272cfa1c31db6_b.jpg"></figure><p>以上介绍了encoder的self-attention。decoder中的encoder-decoder attention道理类似，可以理解为用decoder中的每个 <img src="./attention模型方法综述_files/equation(62)" alt="v_{i}" eeimg="1"> 对encoder中的 <img src="./attention模型方法综述_files/equation(69)" alt="v_{j}" eeimg="1"> 做一种交叉attention。decoder中的self-attention也一样的道理，只是要注意一点，decoder中你在用 <img src="./attention模型方法综述_files/equation(62)" alt="v_{i}" eeimg="1"> 对 <img src="./attention模型方法综述_files/equation(69)" alt="v_{j}" eeimg="1"> 做attention时，有一些pair是不合法的！原因在于，虽然encoder阶段你可以把序列的全部word一次全输入进去，但是decoder阶段却并不总是可以，想象一下你在做inference，decoder的产出还是按从左至右的顺序，所以你的 <img src="./attention模型方法综述_files/equation(62)" alt="v_{i}" eeimg="1"> 是没机会和 <img src="./attention模型方法综述_files/equation(69)" alt="v_{j}" eeimg="1"> ( <img src="./attention模型方法综述_files/equation(70)" alt="j&gt;i" eeimg="1"> )做attention的。那怎么将这一点体现在attention的计算中呢？文中说只需要令 <img src="./attention模型方法综述_files/equation(71)" alt="score(v_{i},v_{j})=-\infty" eeimg="1"> 即可，为何？因为这样的话，</p><p><img src="./attention模型方法综述_files/equation(72)" alt="\alpha_{ij}=softmax(score(v_{i},v_{j}))=softmax(-\infty)=0" eeimg="1"> </p><p>所以在计算 <img src="./attention模型方法综述_files/equation(62)" alt="v_{i}" eeimg="1"> 的self-attention的时候，就能够把 <img src="./attention模型方法综述_files/equation(69)" alt="v_{j}" eeimg="1"> 屏蔽掉～</p><p>所以这个问题也就解决了～～～</p><p>模型的其他模块，诸如position-wise feed-forward networks、position encoding、layer normalization、residual connection等，相对容易理解，感兴趣的同学可以去看paper，此处不再赘述。</p><p><br></p><p>总结</p><p>本文对 attention的五种结构，即hard attention、soft attention、global attention、local attention、self-attention进行了具体分析。五种attention在计算复杂度、部署难度、模型效果上会有一定差异，实际中还需根据业务实际合理选择模型。</p><p><br></p><p>参考文献：</p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//blog.heuritech.com/2016/01/20/attention-mechanism/" data-draft-node="block" data-draft-type="link-card" data-image="https://pic1.zhimg.com/v2-da66d8b68796b37ad8d1a25cb9fb12e8_180x120.jpg" data-image-width="709" data-image-height="273" class="LinkCard LinkCard--hasImage" data-za-detail-view-id="172"><span class="LinkCard-backdrop" style="background-image:url(https://pic1.zhimg.com/v2-da66d8b68796b37ad8d1a25cb9fb12e8_180x120.jpg)"></span><span class="LinkCard-content"><span><span class="LinkCard-title" data-text="true">Attention Mechanism</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>blog.heuritech.com</span></span><span class="LinkCard-imageCell"><img class="LinkCard-image LinkCard-image--horizontal" alt="图标" src="./attention模型方法综述_files/v2-da66d8b68796b37ad8d1a25cb9fb12e8_180x120.jpg"></span></span></a><p>2. <i>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</i></p><p>3. <i>Effective Approaches to Attention-based Neural Machine Translation</i></p><p>4.<i> neural machine translation by jointly learning to align and translate</i></p><p>5.<i> attention is all you need</i></p><p>6.<i> weighted transformer network for machine translation</i></p><p>7.  <i>sequence to sequence learning with neural networks</i></p></div></div><div class="ContentItem-time"><a target="_blank" href="http://zhuanlan.zhihu.com/p/37835894"><span data-tooltip="发布于 2018-06-08 09:59">编辑于 2018-06-09</span></a></div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19813032&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19813032" target="_blank"><div class="Popover"><div id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content">深度学习（Deep Learning）</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19559450&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content">机器学习</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19560026&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19560026" target="_blank"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content">自然语言处理</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 575px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;37835894&quot;}}}"><span><button aria-label="赞同" type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 <!-- -->243</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu"><div class="" id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div data-za-detail-view-path-module="LeftTabBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;37835894&quot;}}}"><div><div class="Post-SideActions" style="opacity: 1;"><button class="like"><div class="Post-SideActions-icon"><svg class="Zi Zi--TriangleUp Post-SideActions-upIcon" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></div><div class="likeCount"><div class="likeCount-inner" data-previous="已赞同 244">赞同 243</div></div></button><div class="Popover ShareMenu"><div class="" id="Popover21-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover21-content"><button><div class="Post-SideActions-icon"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share" fill="currentColor" viewBox="0 0 24 24" width="20" height="20"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span></div>分享</button></div></div></div></div></div></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: 0px; left: 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div><div class="PostIndex-Contributes" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://zhuanlan.zhihu.com/qinlibo-ml"><div class="Popover"><div id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="./attention模型方法综述_files/v2-dade4f3465c87462e77ba24a253c0e20_xs.jpg" srcset="https://pic1.zhimg.com/v2-dade4f3465c87462e77ba24a253c0e20_l.jpg 2x" alt="机器学习算法与自然语言处理"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://zhuanlan.zhihu.com/qinlibo-ml"><div class="Popover"><div id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content">机器学习算法与自然语言处理</div></div></a></h2><div class="ContentItem-meta">公众号[自然语言处理与机器学习] 微信号yizhennotes</div></div><div class="ContentItem-extra"><a href="https://zhuanlan.zhihu.com/qinlibo-ml" type="button" class="Button">进入专栏</a></div></div></div><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://zhuanlan.zhihu.com/paperweekly"><div class="Popover"><div id="Popover10-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover10-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="./attention模型方法综述_files/v2-8ae80aa47b735441469b01414fc5dcce_xs.jpg" srcset="https://pic3.zhimg.com/v2-8ae80aa47b735441469b01414fc5dcce_l.jpg 2x" alt="PaperWeekly"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://zhuanlan.zhihu.com/paperweekly"><div class="Popover"><div id="Popover11-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover11-content">PaperWeekly</div></div></a></h2><div class="ContentItem-meta">PaperWeekly是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。</div></div><div class="ContentItem-extra"><a href="https://zhuanlan.zhihu.com/paperweekly" type="button" class="Button">进入专栏</a></div></div></div></ul></div><div class="Recommendations-Main" style="width: 1840px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled=""><svg class="Zi Zi--ArrowLeft" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M14.782 16.78a.737.737 0 0 1-1.052 0L9.218 12.53a.758.758 0 0 1 0-1.063L13.73 7.22a.737.737 0 0 1 1.052 0c.29.294.29.77.001 1.063L11 12l3.782 3.716c.29.294.29.77 0 1.063z" fill-rule="evenodd"></path></svg></button><a href="http://zhuanlan.zhihu.com/p/35739040" class="PostItem"><div><img src="./attention模型方法综述_files/v2-a2db0687ce55dcd2324e00851dd0426c_250x0.jpg" srcset="https://pic4.zhimg.com/v2-a2db0687ce55dcd2324e00851dd0426c_qhd.jpg 2x" class="PostItem-TitleImage" alt="Attention用于NLP的一些小结"><h1 class="PostItem-Title">Attention用于NLP的一些小结</h1><div class="PostItem-Footer"><span>susht</span><span class="PostItem-FooterTitle">发表于NLP W...</span></div></div></a><a href="http://zhuanlan.zhihu.com/p/21336594" class="PostItem"><div><img src="./attention模型方法综述_files/d35308aae8f447ba0d8ab3aff604bd07_250x0.jpg" srcset="https://pic2.zhimg.com/d35308aae8f447ba0d8ab3aff604bd07_qhd.jpg 2x" class="PostItem-TitleImage" alt="Gated-Attention Readers for Text Comprehension"><h1 class="PostItem-Title">Gated-Attention Readers for Text Comprehension</h1><div class="PostItem-Footer"><span>张俊</span><span class="PostItem-FooterTitle">发表于Paper...</span></div></div></a><a href="http://zhuanlan.zhihu.com/p/31652512" class="PostItem"><div><img src="./attention模型方法综述_files/v2-144ea8825a90d526e1e48822a9140ea4_250x0.jpg" srcset="https://pic4.zhimg.com/v2-144ea8825a90d526e1e48822a9140ea4_qhd.jpg 2x" class="PostItem-TitleImage" alt="自然语言处理中的Attention Model：是什么以及为什么[二]"><h1 class="PostItem-Title">自然语言处理中的Attention Model：是什么以及为什么[二]</h1><div class="PostItem-Footer"><span>张江</span><span class="PostItem-FooterTitle">发表于人工智能学...</span></div></div></a><a href="http://zhuanlan.zhihu.com/p/35041012" class="PostItem"><div><img src="./attention模型方法综述_files/v2-82d4cfe0d155753b4396e629c4395447_250x0.jpg" srcset="https://pic1.zhimg.com/v2-82d4cfe0d155753b4396e629c4395447_qhd.jpg 2x" class="PostItem-TitleImage" alt="自然语言处理中的自注意力机制（Self-Attention Mechanism）"><h1 class="PostItem-Title">自然语言处理中的自注意力机制（Self-Attention Mechanism）</h1><div class="PostItem-Footer"><span>张俊</span><span class="PostItem-FooterTitle">发表于Paper...</span></div></div></a><button class="PagingButton PagingButton-Next"><svg class="Zi Zi--ArrowRight" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></button></ul></div><div class="Comments-container" data-za-detail-view-path-module="CommentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Comments Comments--withEditor Comments-withPagination"><div class="Topbar CommentTopbar"><div class="Topbar-title"><h2 class="CommentTopbar-title">还没有评论</h2></div><div class="Topbar-options"></div></div><div class="Comments-footer CommentEditor--normal"><div class="CommentEditor-input Input-wrapper Input-wrapper--spread Input-wrapper--large Input-wrapper--noPadding"><div class="Input Editable"><div class="Dropzone RichText ztext" style="min-height: 198px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-baimp">写下你的评论...</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-baimp" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; white-space: pre-wrap; word-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="baimp" data-offset-key="b2vi9-0-0"><div data-offset-key="b2vi9-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="b2vi9-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input multiple="" type="file" accept="image/jpg,image/jpeg,image/png,image/gif" style="display: none;"><div></div></div></div><button disabled="" type="button" class="Button CommentEditor-singleButton Button--primary Button--blue">评论</button></div><div><div class="CommentList"></div><span></span></div></div></div></article></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><div id="data" style="display:none" data-useragent="{&quot;os&quot;:{&quot;name&quot;:&quot;Linux&quot;,&quot;version&quot;:&quot;x86_64&quot;},&quot;browser&quot;:{&quot;name&quot;:&quot;Chrome&quot;,&quot;version&quot;:&quot;67.0.3396.99&quot;,&quot;major&quot;:&quot;67&quot;}}"></div><script src="./attention模型方法综述_files/vendor.10c9a7ff1d21e1c0c110.js"></script><script src="./attention模型方法综述_files/column.raven.8ceee5c509b4e518c638.js" defer=""></script><script src="./attention模型方法综述_files/column.app.c782616f0b8a072ba75f.js"></script><script></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><div class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete13-0" id="Popover12-toggle" aria-haspopup="true" aria-owns="Popover12-content" class="Input" placeholder="选择语言" value=""><div class="Input-after"><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></div></div></div></div></div></div></div></body></html>